{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Go Opik SDK \u00b6 Go SDK for Opik - an open-source LLM observability platform by Comet ML. Features \u00b6 Tracing : Create traces and spans to monitor LLM application execution Context Propagation : Automatic parent-child relationships using Go context Distributed Tracing : Propagate traces across service boundaries Datasets : Manage evaluation datasets with CRUD operations Experiments : Run and track evaluation experiments Prompts : Version-controlled prompt templates with variable substitution Evaluation Framework : Heuristic and LLM-based metrics for output evaluation LLM Integrations : Auto-tracing for OpenAI, Anthropic, and gollm CLI Tool : Command-line interface for management tasks Quick Start \u00b6 package main import ( \"context\" \"log\" opik \"github.com/agentplexus/go-comet-ml-opik\" ) func main() { // Create client (uses OPIK_API_KEY and OPIK_WORKSPACE env vars) client, err := opik.NewClient( opik.WithProjectName(\"My Project\"), ) if err != nil { log.Fatal(err) } ctx := context.Background() // Create a trace trace, _ := client.Trace(ctx, \"my-trace\", opik.WithTraceInput(map[string]any{\"prompt\": \"Hello\"}), ) // Create a span for an LLM call span, _ := trace.Span(ctx, \"llm-call\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), ) // Do your LLM call here... // End span with output span.End(ctx, opik.WithSpanOutput(map[string]any{\"response\": \"Hi!\"})) // End trace trace.End(ctx) } Installation \u00b6 go get github.com/agentplexus/go-comet-ml-opik Next Steps \u00b6 Installation & Configuration - Set up the SDK Traces and Spans - Learn the core concepts Evaluation Framework - Evaluate LLM outputs Integrations - Auto-trace OpenAI, Anthropic, and more Testing - Run the test suite (no API key required)","title":"Home"},{"location":"#go-opik-sdk","text":"Go SDK for Opik - an open-source LLM observability platform by Comet ML.","title":"Go Opik SDK"},{"location":"#features","text":"Tracing : Create traces and spans to monitor LLM application execution Context Propagation : Automatic parent-child relationships using Go context Distributed Tracing : Propagate traces across service boundaries Datasets : Manage evaluation datasets with CRUD operations Experiments : Run and track evaluation experiments Prompts : Version-controlled prompt templates with variable substitution Evaluation Framework : Heuristic and LLM-based metrics for output evaluation LLM Integrations : Auto-tracing for OpenAI, Anthropic, and gollm CLI Tool : Command-line interface for management tasks","title":"Features"},{"location":"#quick-start","text":"package main import ( \"context\" \"log\" opik \"github.com/agentplexus/go-comet-ml-opik\" ) func main() { // Create client (uses OPIK_API_KEY and OPIK_WORKSPACE env vars) client, err := opik.NewClient( opik.WithProjectName(\"My Project\"), ) if err != nil { log.Fatal(err) } ctx := context.Background() // Create a trace trace, _ := client.Trace(ctx, \"my-trace\", opik.WithTraceInput(map[string]any{\"prompt\": \"Hello\"}), ) // Create a span for an LLM call span, _ := trace.Span(ctx, \"llm-call\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), ) // Do your LLM call here... // End span with output span.End(ctx, opik.WithSpanOutput(map[string]any{\"response\": \"Hi!\"})) // End trace trace.End(ctx) }","title":"Quick Start"},{"location":"#installation","text":"go get github.com/agentplexus/go-comet-ml-opik","title":"Installation"},{"location":"#next-steps","text":"Installation & Configuration - Set up the SDK Traces and Spans - Learn the core concepts Evaluation Framework - Evaluate LLM outputs Integrations - Auto-trace OpenAI, Anthropic, and more Testing - Run the test suite (no API key required)","title":"Next Steps"},{"location":"api-reference/","text":"API Reference \u00b6 Client \u00b6 Creating a Client \u00b6 // Default client (uses env vars and config file) client, err := opik.NewClient() // With options client, err := opik.NewClient( opik.WithURL(\"https://www.comet.com/opik/api\"), opik.WithAPIKey(\"your-api-key\"), opik.WithWorkspace(\"your-workspace\"), opik.WithProjectName(\"My Project\"), opik.WithHTTPClient(customHTTPClient), ) Client Options \u00b6 Option Description WithURL(url) API endpoint URL WithAPIKey(key) API key WithWorkspace(name) Workspace name WithProjectName(name) Default project WithHTTPClient(client) Custom HTTP client Accessing the Generated API \u00b6 For advanced usage, access the underlying ogen-generated API client: api := client.API() // Use generated methods directly resp, err := api.GetProjects(ctx, api.GetProjectsParams{ Page: api.NewOptInt32(1), Size: api.NewOptInt32(100), }) Error Handling \u00b6 Error Types \u00b6 // Check for specific errors if opik.IsNotFound(err) { // Resource doesn't exist } if opik.IsUnauthorized(err) { // Invalid credentials } if opik.IsRateLimited(err) { // Too many requests } if opik.IsBadRequest(err) { // Invalid request parameters } Error Details \u00b6 if apiErr, ok := err.(*opik.APIError); ok { fmt.Printf(\"Status: %d\\n\", apiErr.StatusCode) fmt.Printf(\"Message: %s\\n\", apiErr.Message) } Types \u00b6 Trace \u00b6 type Trace struct { // Methods ID() string Name() string End(ctx context.Context, opts ...TraceOption) error Update(ctx context.Context, opts ...TraceOption) error Span(ctx context.Context, name string, opts ...SpanOption) (*Span, error) AddFeedbackScore(ctx context.Context, name string, value float64, reason string) error } Span \u00b6 type Span struct { // Methods ID() string TraceID() string Name() string End(ctx context.Context, opts ...SpanOption) error Update(ctx context.Context, opts ...SpanOption) error Span(ctx context.Context, name string, opts ...SpanOption) (*Span, error) AddFeedbackScore(ctx context.Context, name string, value float64, reason string) error } Dataset \u00b6 type Dataset struct { // Fields Name string Description string ID string // Methods ID() string InsertItem(ctx context.Context, data map[string]any) error InsertItems(ctx context.Context, items []map[string]any) error GetItems(ctx context.Context, page, size int) ([]*DatasetItem, error) Delete(ctx context.Context) error } Experiment \u00b6 type Experiment struct { // Fields Name string ID string // Methods LogItem(ctx context.Context, itemID, traceID string, opts ...ExperimentItemOption) error Complete(ctx context.Context) error Cancel(ctx context.Context) error Delete(ctx context.Context) error } Prompt \u00b6 type Prompt struct { // Fields Name string Description string // Methods CreateVersion(ctx context.Context, template string, opts ...VersionOption) (*PromptVersion, error) GetVersions(ctx context.Context, page, size int) ([]*PromptVersion, error) } type PromptVersion struct { // Fields Commit string Template string // Methods Render(vars map[string]string) string ExtractVariables() []string } Span Types \u00b6 const ( SpanTypeLLM = \"llm\" SpanTypeTool = \"tool\" SpanTypeGeneral = \"general\" ) Context Functions \u00b6 // Start trace with context ctx, trace, err := opik.StartTrace(ctx, client, \"name\", opts...) // Start span with context ctx, span, err := opik.StartSpan(ctx, \"name\", opts...) // Get from context trace := opik.TraceFromContext(ctx) span := opik.SpanFromContext(ctx) // Add to context ctx = opik.ContextWithTrace(ctx, trace) ctx = opik.ContextWithSpan(ctx, span) Distributed Tracing \u00b6 // Get headers for propagation headers := opik.GetDistributedTraceHeaders(ctx) // Inject into HTTP request opik.InjectDistributedTraceHeaders(ctx, req) // Extract from HTTP request headers := opik.ExtractDistributedTraceHeaders(req) // Continue a distributed trace ctx, span, err := client.ContinueTrace(ctx, headers, \"name\", opts...) Configuration \u00b6 // Load configuration cfg := opik.LoadConfig() // Save configuration err := opik.SaveConfig(cfg) // Config struct type Config struct { URL string APIKey string Workspace string ProjectName string } Testing Utilities \u00b6 // Record traces locally (no server) client := opik.RecordTracesLocally(\"project-name\") // Access recorded data recording := client.Recording() traces := recording.Traces() spans := recording.Spans()","title":"API Reference"},{"location":"api-reference/#api-reference","text":"","title":"API Reference"},{"location":"api-reference/#client","text":"","title":"Client"},{"location":"api-reference/#creating-a-client","text":"// Default client (uses env vars and config file) client, err := opik.NewClient() // With options client, err := opik.NewClient( opik.WithURL(\"https://www.comet.com/opik/api\"), opik.WithAPIKey(\"your-api-key\"), opik.WithWorkspace(\"your-workspace\"), opik.WithProjectName(\"My Project\"), opik.WithHTTPClient(customHTTPClient), )","title":"Creating a Client"},{"location":"api-reference/#client-options","text":"Option Description WithURL(url) API endpoint URL WithAPIKey(key) API key WithWorkspace(name) Workspace name WithProjectName(name) Default project WithHTTPClient(client) Custom HTTP client","title":"Client Options"},{"location":"api-reference/#accessing-the-generated-api","text":"For advanced usage, access the underlying ogen-generated API client: api := client.API() // Use generated methods directly resp, err := api.GetProjects(ctx, api.GetProjectsParams{ Page: api.NewOptInt32(1), Size: api.NewOptInt32(100), })","title":"Accessing the Generated API"},{"location":"api-reference/#error-handling","text":"","title":"Error Handling"},{"location":"api-reference/#error-types","text":"// Check for specific errors if opik.IsNotFound(err) { // Resource doesn't exist } if opik.IsUnauthorized(err) { // Invalid credentials } if opik.IsRateLimited(err) { // Too many requests } if opik.IsBadRequest(err) { // Invalid request parameters }","title":"Error Types"},{"location":"api-reference/#error-details","text":"if apiErr, ok := err.(*opik.APIError); ok { fmt.Printf(\"Status: %d\\n\", apiErr.StatusCode) fmt.Printf(\"Message: %s\\n\", apiErr.Message) }","title":"Error Details"},{"location":"api-reference/#types","text":"","title":"Types"},{"location":"api-reference/#trace","text":"type Trace struct { // Methods ID() string Name() string End(ctx context.Context, opts ...TraceOption) error Update(ctx context.Context, opts ...TraceOption) error Span(ctx context.Context, name string, opts ...SpanOption) (*Span, error) AddFeedbackScore(ctx context.Context, name string, value float64, reason string) error }","title":"Trace"},{"location":"api-reference/#span","text":"type Span struct { // Methods ID() string TraceID() string Name() string End(ctx context.Context, opts ...SpanOption) error Update(ctx context.Context, opts ...SpanOption) error Span(ctx context.Context, name string, opts ...SpanOption) (*Span, error) AddFeedbackScore(ctx context.Context, name string, value float64, reason string) error }","title":"Span"},{"location":"api-reference/#dataset","text":"type Dataset struct { // Fields Name string Description string ID string // Methods ID() string InsertItem(ctx context.Context, data map[string]any) error InsertItems(ctx context.Context, items []map[string]any) error GetItems(ctx context.Context, page, size int) ([]*DatasetItem, error) Delete(ctx context.Context) error }","title":"Dataset"},{"location":"api-reference/#experiment","text":"type Experiment struct { // Fields Name string ID string // Methods LogItem(ctx context.Context, itemID, traceID string, opts ...ExperimentItemOption) error Complete(ctx context.Context) error Cancel(ctx context.Context) error Delete(ctx context.Context) error }","title":"Experiment"},{"location":"api-reference/#prompt","text":"type Prompt struct { // Fields Name string Description string // Methods CreateVersion(ctx context.Context, template string, opts ...VersionOption) (*PromptVersion, error) GetVersions(ctx context.Context, page, size int) ([]*PromptVersion, error) } type PromptVersion struct { // Fields Commit string Template string // Methods Render(vars map[string]string) string ExtractVariables() []string }","title":"Prompt"},{"location":"api-reference/#span-types","text":"const ( SpanTypeLLM = \"llm\" SpanTypeTool = \"tool\" SpanTypeGeneral = \"general\" )","title":"Span Types"},{"location":"api-reference/#context-functions","text":"// Start trace with context ctx, trace, err := opik.StartTrace(ctx, client, \"name\", opts...) // Start span with context ctx, span, err := opik.StartSpan(ctx, \"name\", opts...) // Get from context trace := opik.TraceFromContext(ctx) span := opik.SpanFromContext(ctx) // Add to context ctx = opik.ContextWithTrace(ctx, trace) ctx = opik.ContextWithSpan(ctx, span)","title":"Context Functions"},{"location":"api-reference/#distributed-tracing","text":"// Get headers for propagation headers := opik.GetDistributedTraceHeaders(ctx) // Inject into HTTP request opik.InjectDistributedTraceHeaders(ctx, req) // Extract from HTTP request headers := opik.ExtractDistributedTraceHeaders(req) // Continue a distributed trace ctx, span, err := client.ContinueTrace(ctx, headers, \"name\", opts...)","title":"Distributed Tracing"},{"location":"api-reference/#configuration","text":"// Load configuration cfg := opik.LoadConfig() // Save configuration err := opik.SaveConfig(cfg) // Config struct type Config struct { URL string APIKey string Workspace string ProjectName string }","title":"Configuration"},{"location":"api-reference/#testing-utilities","text":"// Record traces locally (no server) client := opik.RecordTracesLocally(\"project-name\") // Access recorded data recording := client.Recording() traces := recording.Traces() spans := recording.Spans()","title":"Testing Utilities"},{"location":"cli/","text":"CLI Reference \u00b6 The Opik CLI provides command-line access to manage projects, traces, datasets, and experiments. Installation \u00b6 go install github.com/agentplexus/go-comet-ml-opik/cmd/opik@latest Configuration \u00b6 Configure Credentials \u00b6 opik configure -api-key=your-key -workspace=your-workspace Options \u00b6 Flag Description -api-key API key for Opik Cloud -workspace Workspace name -url Custom API endpoint URL Configuration is saved to ~/.opik.config . Commands \u00b6 Projects \u00b6 List and manage projects. # List all projects opik projects -list # Create a new project opik projects -create=\"New Project\" # Output as JSON opik projects -list -format=json Flag Description -list List all projects -create Create a project with the given name -format Output format: text (default) or json Traces \u00b6 View recent traces. # List recent traces opik traces -list # Filter by project opik traces -list -project=\"My Project\" # Limit results opik traces -list -limit=20 # Output as JSON opik traces -list -format=json Flag Description -list List recent traces -project Filter by project name -limit Maximum traces to show (default: 10) -format Output format: text (default) or json Datasets \u00b6 Manage evaluation datasets. # List all datasets opik datasets -list # Create a new dataset opik datasets -create=\"evaluation-data\" # Get dataset by name opik datasets -get=\"my-dataset\" # Delete a dataset opik datasets -delete=\"old-dataset\" # Output as JSON opik datasets -list -format=json Flag Description -list List all datasets -create Create a dataset with the given name -get Get a dataset by name -delete Delete a dataset by name -format Output format: text (default) or json Experiments \u00b6 View experiments. # List experiments for a dataset opik experiments -list -dataset=\"my-dataset\" # Output as JSON opik experiments -list -dataset=\"my-dataset\" -format=json Flag Description -list List experiments -dataset Dataset name (required for listing) -format Output format: text (default) or json Help \u00b6 # Show general help opik help # Show command-specific help opik projects -h opik traces -h opik datasets -h opik experiments -h Environment Variables \u00b6 The CLI respects these environment variables: Variable Description OPIK_API_KEY API key for Opik Cloud OPIK_WORKSPACE Workspace name OPIK_URL_OVERRIDE Custom API endpoint OPIK_PROJECT_NAME Default project name Examples \u00b6 Quick Setup \u00b6 # Configure credentials opik configure -api-key=your-key -workspace=your-workspace # Verify by listing projects opik projects -list Daily Workflow \u00b6 # Check recent traces opik traces -list -project=\"production\" -limit=20 # Review datasets opik datasets -list # Check experiment results opik experiments -list -dataset=\"qa-eval\" Scripting \u00b6 # Export traces as JSON for analysis opik traces -list -format=json > traces.json # Create dataset from script opik datasets -create=\"$(date +%Y%m%d)-eval\"","title":"CLI Reference"},{"location":"cli/#cli-reference","text":"The Opik CLI provides command-line access to manage projects, traces, datasets, and experiments.","title":"CLI Reference"},{"location":"cli/#installation","text":"go install github.com/agentplexus/go-comet-ml-opik/cmd/opik@latest","title":"Installation"},{"location":"cli/#configuration","text":"","title":"Configuration"},{"location":"cli/#configure-credentials","text":"opik configure -api-key=your-key -workspace=your-workspace","title":"Configure Credentials"},{"location":"cli/#options","text":"Flag Description -api-key API key for Opik Cloud -workspace Workspace name -url Custom API endpoint URL Configuration is saved to ~/.opik.config .","title":"Options"},{"location":"cli/#commands","text":"","title":"Commands"},{"location":"cli/#projects","text":"List and manage projects. # List all projects opik projects -list # Create a new project opik projects -create=\"New Project\" # Output as JSON opik projects -list -format=json Flag Description -list List all projects -create Create a project with the given name -format Output format: text (default) or json","title":"Projects"},{"location":"cli/#traces","text":"View recent traces. # List recent traces opik traces -list # Filter by project opik traces -list -project=\"My Project\" # Limit results opik traces -list -limit=20 # Output as JSON opik traces -list -format=json Flag Description -list List recent traces -project Filter by project name -limit Maximum traces to show (default: 10) -format Output format: text (default) or json","title":"Traces"},{"location":"cli/#datasets","text":"Manage evaluation datasets. # List all datasets opik datasets -list # Create a new dataset opik datasets -create=\"evaluation-data\" # Get dataset by name opik datasets -get=\"my-dataset\" # Delete a dataset opik datasets -delete=\"old-dataset\" # Output as JSON opik datasets -list -format=json Flag Description -list List all datasets -create Create a dataset with the given name -get Get a dataset by name -delete Delete a dataset by name -format Output format: text (default) or json","title":"Datasets"},{"location":"cli/#experiments","text":"View experiments. # List experiments for a dataset opik experiments -list -dataset=\"my-dataset\" # Output as JSON opik experiments -list -dataset=\"my-dataset\" -format=json Flag Description -list List experiments -dataset Dataset name (required for listing) -format Output format: text (default) or json","title":"Experiments"},{"location":"cli/#help","text":"# Show general help opik help # Show command-specific help opik projects -h opik traces -h opik datasets -h opik experiments -h","title":"Help"},{"location":"cli/#environment-variables","text":"The CLI respects these environment variables: Variable Description OPIK_API_KEY API key for Opik Cloud OPIK_WORKSPACE Workspace name OPIK_URL_OVERRIDE Custom API endpoint OPIK_PROJECT_NAME Default project name","title":"Environment Variables"},{"location":"cli/#examples","text":"","title":"Examples"},{"location":"cli/#quick-setup","text":"# Configure credentials opik configure -api-key=your-key -workspace=your-workspace # Verify by listing projects opik projects -list","title":"Quick Setup"},{"location":"cli/#daily-workflow","text":"# Check recent traces opik traces -list -project=\"production\" -limit=20 # Review datasets opik datasets -list # Check experiment results opik experiments -list -dataset=\"qa-eval\"","title":"Daily Workflow"},{"location":"cli/#scripting","text":"# Export traces as JSON for analysis opik traces -list -format=json > traces.json # Create dataset from script opik datasets -create=\"$(date +%Y%m%d)-eval\"","title":"Scripting"},{"location":"core-concepts/context-propagation/","text":"Context Propagation \u00b6 The SDK uses Go's context.Context for automatic trace and span propagation. This enables: Automatic parent-child span relationships Easy access to current trace/span from anywhere Clean API without passing trace objects everywhere Starting Traces with Context \u00b6 // Start a trace and get updated context ctx, trace, _ := opik.StartTrace(ctx, client, \"my-trace\") // The trace is now in the context currentTrace := opik.TraceFromContext(ctx) Starting Spans with Context \u00b6 // Start a span - automatically nested under current span/trace ctx, span, _ := opik.StartSpan(ctx, \"my-span\") // Start another span - automatically nested under the previous span ctx, childSpan, _ := opik.StartSpan(ctx, \"child-span\") Retrieving from Context \u00b6 // Get current trace trace := opik.TraceFromContext(ctx) if trace != nil { fmt.Printf(\"Current trace: %s\\n\", trace.ID()) } // Get current span span := opik.SpanFromContext(ctx) if span != nil { fmt.Printf(\"Current span: %s\\n\", span.ID()) } Practical Example \u00b6 Context propagation makes it easy to add tracing to existing code: func HandleRequest(ctx context.Context, req *Request) (*Response, error) { // Start trace at entry point ctx, trace, _ := opik.StartTrace(ctx, client, \"handle-request\", opik.WithTraceInput(req), ) defer trace.End(ctx) // Call nested functions - they can access trace via context result, err := processRequest(ctx, req) if err != nil { return nil, err } trace.End(ctx, opik.WithTraceOutput(result)) return result, nil } func processRequest(ctx context.Context, req *Request) (*Response, error) { // Create span - automatically nested under trace ctx, span, _ := opik.StartSpan(ctx, \"process-request\") defer span.End(ctx) // Call LLM return callLLM(ctx, req.Query) } func callLLM(ctx context.Context, query string) (*Response, error) { // Create LLM span - automatically nested under process-request span ctx, span, _ := opik.StartSpan(ctx, \"llm-call\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), ) defer span.End(ctx) // Make the actual LLM call response, err := llmClient.Complete(ctx, query) span.End(ctx, opik.WithSpanOutput(map[string]any{\"response\": response})) return response, err } Distributed Tracing \u00b6 For microservices, propagate trace context across HTTP boundaries: Injecting Headers (Client Side) \u00b6 // Inject trace headers into outgoing request req, _ := http.NewRequestWithContext(ctx, \"POST\", url, body) opik.InjectDistributedTraceHeaders(ctx, req) Extracting Headers (Server Side) \u00b6 func handler(w http.ResponseWriter, r *http.Request) { // Extract trace headers headers := opik.ExtractDistributedTraceHeaders(r) // Continue the distributed trace ctx, span, _ := client.ContinueTrace(r.Context(), headers, \"handle-request\") defer span.End(ctx) // Process request... } Propagating HTTP Client \u00b6 Use the built-in propagating client: // Create client that auto-injects trace headers httpClient := opik.PropagatingHTTPClient() // All requests will include trace headers resp, _ := httpClient.Do(req.WithContext(ctx)) Header Format \u00b6 The SDK uses these headers for distributed tracing: Header Description X-Opik-Trace-ID The trace ID X-Opik-Parent-Span-ID The parent span ID","title":"Context Propagation"},{"location":"core-concepts/context-propagation/#context-propagation","text":"The SDK uses Go's context.Context for automatic trace and span propagation. This enables: Automatic parent-child span relationships Easy access to current trace/span from anywhere Clean API without passing trace objects everywhere","title":"Context Propagation"},{"location":"core-concepts/context-propagation/#starting-traces-with-context","text":"// Start a trace and get updated context ctx, trace, _ := opik.StartTrace(ctx, client, \"my-trace\") // The trace is now in the context currentTrace := opik.TraceFromContext(ctx)","title":"Starting Traces with Context"},{"location":"core-concepts/context-propagation/#starting-spans-with-context","text":"// Start a span - automatically nested under current span/trace ctx, span, _ := opik.StartSpan(ctx, \"my-span\") // Start another span - automatically nested under the previous span ctx, childSpan, _ := opik.StartSpan(ctx, \"child-span\")","title":"Starting Spans with Context"},{"location":"core-concepts/context-propagation/#retrieving-from-context","text":"// Get current trace trace := opik.TraceFromContext(ctx) if trace != nil { fmt.Printf(\"Current trace: %s\\n\", trace.ID()) } // Get current span span := opik.SpanFromContext(ctx) if span != nil { fmt.Printf(\"Current span: %s\\n\", span.ID()) }","title":"Retrieving from Context"},{"location":"core-concepts/context-propagation/#practical-example","text":"Context propagation makes it easy to add tracing to existing code: func HandleRequest(ctx context.Context, req *Request) (*Response, error) { // Start trace at entry point ctx, trace, _ := opik.StartTrace(ctx, client, \"handle-request\", opik.WithTraceInput(req), ) defer trace.End(ctx) // Call nested functions - they can access trace via context result, err := processRequest(ctx, req) if err != nil { return nil, err } trace.End(ctx, opik.WithTraceOutput(result)) return result, nil } func processRequest(ctx context.Context, req *Request) (*Response, error) { // Create span - automatically nested under trace ctx, span, _ := opik.StartSpan(ctx, \"process-request\") defer span.End(ctx) // Call LLM return callLLM(ctx, req.Query) } func callLLM(ctx context.Context, query string) (*Response, error) { // Create LLM span - automatically nested under process-request span ctx, span, _ := opik.StartSpan(ctx, \"llm-call\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), ) defer span.End(ctx) // Make the actual LLM call response, err := llmClient.Complete(ctx, query) span.End(ctx, opik.WithSpanOutput(map[string]any{\"response\": response})) return response, err }","title":"Practical Example"},{"location":"core-concepts/context-propagation/#distributed-tracing","text":"For microservices, propagate trace context across HTTP boundaries:","title":"Distributed Tracing"},{"location":"core-concepts/context-propagation/#injecting-headers-client-side","text":"// Inject trace headers into outgoing request req, _ := http.NewRequestWithContext(ctx, \"POST\", url, body) opik.InjectDistributedTraceHeaders(ctx, req)","title":"Injecting Headers (Client Side)"},{"location":"core-concepts/context-propagation/#extracting-headers-server-side","text":"func handler(w http.ResponseWriter, r *http.Request) { // Extract trace headers headers := opik.ExtractDistributedTraceHeaders(r) // Continue the distributed trace ctx, span, _ := client.ContinueTrace(r.Context(), headers, \"handle-request\") defer span.End(ctx) // Process request... }","title":"Extracting Headers (Server Side)"},{"location":"core-concepts/context-propagation/#propagating-http-client","text":"Use the built-in propagating client: // Create client that auto-injects trace headers httpClient := opik.PropagatingHTTPClient() // All requests will include trace headers resp, _ := httpClient.Do(req.WithContext(ctx))","title":"Propagating HTTP Client"},{"location":"core-concepts/context-propagation/#header-format","text":"The SDK uses these headers for distributed tracing: Header Description X-Opik-Trace-ID The trace ID X-Opik-Parent-Span-ID The parent span ID","title":"Header Format"},{"location":"core-concepts/feedback-scores/","text":"Feedback Scores \u00b6 Feedback scores allow you to attach quality metrics to traces and spans. Use them to: Record user feedback (thumbs up/down, ratings) Store evaluation results from automated metrics Track quality metrics over time Adding Feedback to Traces \u00b6 // Add a numeric score trace.AddFeedbackScore(ctx, \"accuracy\", 0.95, \"High accuracy response\") // Add multiple scores trace.AddFeedbackScore(ctx, \"relevance\", 0.87, \"Mostly relevant\") trace.AddFeedbackScore(ctx, \"helpfulness\", 0.92, \"Very helpful\") Adding Feedback to Spans \u00b6 // Add feedback to specific spans span.AddFeedbackScore(ctx, \"latency_score\", 0.75, \"Response time acceptable\") span.AddFeedbackScore(ctx, \"quality\", 0.90, \"Good quality output\") Score Parameters \u00b6 Parameter Type Description name string Name of the score (e.g., \"accuracy\", \"relevance\") value float64 Score value (typically 0.0 to 1.0) reason string Optional explanation for the score Use Cases \u00b6 User Feedback \u00b6 func handleFeedback(ctx context.Context, traceID string, rating int) { // Convert 1-5 rating to 0-1 score score := float64(rating-1) / 4.0 trace := getTrace(traceID) trace.AddFeedbackScore(ctx, \"user_rating\", score, fmt.Sprintf(\"User rated %d/5\", rating)) } Automated Evaluation \u00b6 func evaluateResponse(ctx context.Context, trace *opik.Trace, response string) { // Run evaluation metrics relevanceScore := evaluateRelevance(response) factualScore := evaluateFactuality(response) // Record as feedback scores trace.AddFeedbackScore(ctx, \"relevance\", relevanceScore, \"Automated relevance check\") trace.AddFeedbackScore(ctx, \"factuality\", factualScore, \"Automated fact check\") } A/B Testing \u00b6 func recordABResult(ctx context.Context, trace *opik.Trace, variant string, converted bool) { score := 0.0 if converted { score = 1.0 } trace.AddFeedbackScore(ctx, \"conversion\", score, fmt.Sprintf(\"Variant %s, converted: %v\", variant, converted)) } Viewing Feedback Scores \u00b6 Feedback scores are visible in the Opik UI: On the trace detail page In trace list summaries In experiment comparisons In analytics dashboards Best Practices \u00b6 Use consistent names : Standardize score names across your application Normalize values : Use 0.0-1.0 range for easy comparison Include reasons : Add explanations for debugging and analysis Score at appropriate level : Use trace-level for overall quality, span-level for specific operations","title":"Feedback Scores"},{"location":"core-concepts/feedback-scores/#feedback-scores","text":"Feedback scores allow you to attach quality metrics to traces and spans. Use them to: Record user feedback (thumbs up/down, ratings) Store evaluation results from automated metrics Track quality metrics over time","title":"Feedback Scores"},{"location":"core-concepts/feedback-scores/#adding-feedback-to-traces","text":"// Add a numeric score trace.AddFeedbackScore(ctx, \"accuracy\", 0.95, \"High accuracy response\") // Add multiple scores trace.AddFeedbackScore(ctx, \"relevance\", 0.87, \"Mostly relevant\") trace.AddFeedbackScore(ctx, \"helpfulness\", 0.92, \"Very helpful\")","title":"Adding Feedback to Traces"},{"location":"core-concepts/feedback-scores/#adding-feedback-to-spans","text":"// Add feedback to specific spans span.AddFeedbackScore(ctx, \"latency_score\", 0.75, \"Response time acceptable\") span.AddFeedbackScore(ctx, \"quality\", 0.90, \"Good quality output\")","title":"Adding Feedback to Spans"},{"location":"core-concepts/feedback-scores/#score-parameters","text":"Parameter Type Description name string Name of the score (e.g., \"accuracy\", \"relevance\") value float64 Score value (typically 0.0 to 1.0) reason string Optional explanation for the score","title":"Score Parameters"},{"location":"core-concepts/feedback-scores/#use-cases","text":"","title":"Use Cases"},{"location":"core-concepts/feedback-scores/#user-feedback","text":"func handleFeedback(ctx context.Context, traceID string, rating int) { // Convert 1-5 rating to 0-1 score score := float64(rating-1) / 4.0 trace := getTrace(traceID) trace.AddFeedbackScore(ctx, \"user_rating\", score, fmt.Sprintf(\"User rated %d/5\", rating)) }","title":"User Feedback"},{"location":"core-concepts/feedback-scores/#automated-evaluation","text":"func evaluateResponse(ctx context.Context, trace *opik.Trace, response string) { // Run evaluation metrics relevanceScore := evaluateRelevance(response) factualScore := evaluateFactuality(response) // Record as feedback scores trace.AddFeedbackScore(ctx, \"relevance\", relevanceScore, \"Automated relevance check\") trace.AddFeedbackScore(ctx, \"factuality\", factualScore, \"Automated fact check\") }","title":"Automated Evaluation"},{"location":"core-concepts/feedback-scores/#ab-testing","text":"func recordABResult(ctx context.Context, trace *opik.Trace, variant string, converted bool) { score := 0.0 if converted { score = 1.0 } trace.AddFeedbackScore(ctx, \"conversion\", score, fmt.Sprintf(\"Variant %s, converted: %v\", variant, converted)) }","title":"A/B Testing"},{"location":"core-concepts/feedback-scores/#viewing-feedback-scores","text":"Feedback scores are visible in the Opik UI: On the trace detail page In trace list summaries In experiment comparisons In analytics dashboards","title":"Viewing Feedback Scores"},{"location":"core-concepts/feedback-scores/#best-practices","text":"Use consistent names : Standardize score names across your application Normalize values : Use 0.0-1.0 range for easy comparison Include reasons : Add explanations for debugging and analysis Score at appropriate level : Use trace-level for overall quality, span-level for specific operations","title":"Best Practices"},{"location":"core-concepts/traces-and-spans/","text":"Traces and Spans \u00b6 Traces and spans are the core building blocks for observability in Opik. Concepts \u00b6 Trace : Represents a complete request or workflow through your application Span : Represents a single operation within a trace (e.g., an LLM call, tool execution) Spans can be nested to show parent-child relationships, creating a tree structure that represents your application's execution flow. Creating Traces \u00b6 client, _ := opik.NewClient() // Create a simple trace trace, _ := client.Trace(ctx, \"my-trace\") // Create a trace with options trace, _ := client.Trace(ctx, \"my-trace\", opik.WithTraceInput(map[string]any{\"prompt\": \"Hello\"}), opik.WithTraceMetadata(map[string]any{\"user_id\": \"123\"}), opik.WithTraceTags(\"production\", \"v2\"), ) // End the trace trace.End(ctx) // End with output trace.End(ctx, opik.WithTraceOutput(map[string]any{\"response\": \"Hi!\"})) Creating Spans \u00b6 Spans are created from traces or other spans: // Create a span from a trace span, _ := trace.Span(ctx, \"process-input\") // Create a nested span from another span childSpan, _ := span.Span(ctx, \"validate-input\") // End spans (in reverse order) childSpan.End(ctx) span.End(ctx) Span Types \u00b6 Use span types to categorize operations: // LLM call span span, _ := trace.Span(ctx, \"llm-call\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), opik.WithSpanProvider(\"openai\"), ) // Tool execution span span, _ := trace.Span(ctx, \"search-tool\", opik.WithSpanType(opik.SpanTypeTool), ) // General processing span (default) span, _ := trace.Span(ctx, \"process-data\", opik.WithSpanType(opik.SpanTypeGeneral), ) Span Options \u00b6 Option Description WithSpanType(type) Set span type (LLM, Tool, General) WithSpanModel(model) Set the model name for LLM spans WithSpanProvider(provider) Set the provider name (openai, anthropic) WithSpanInput(data) Set input data WithSpanOutput(data) Set output data WithSpanMetadata(data) Set metadata WithSpanTags(tags...) Add tags Complete Example \u00b6 func processQuery(ctx context.Context, client *opik.Client, query string) (string, error) { // Create trace for the entire request trace, _ := client.Trace(ctx, \"process-query\", opik.WithTraceInput(map[string]any{\"query\": query}), ) defer trace.End(ctx) // Span for preprocessing preprocessSpan, _ := trace.Span(ctx, \"preprocess\") processedQuery := preprocess(query) preprocessSpan.End(ctx, opik.WithSpanOutput(map[string]any{ \"processed\": processedQuery, })) // Span for LLM call llmSpan, _ := trace.Span(ctx, \"llm-call\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), opik.WithSpanInput(map[string]any{\"prompt\": processedQuery}), ) response, err := callLLM(processedQuery) if err != nil { llmSpan.End(ctx, opik.WithSpanMetadata(map[string]any{\"error\": err.Error()})) return \"\", err } llmSpan.End(ctx, opik.WithSpanOutput(map[string]any{\"response\": response})) // Update trace with final output trace.End(ctx, opik.WithTraceOutput(map[string]any{\"result\": response})) return response, nil } Updating Traces and Spans \u00b6 Update metadata after creation: // Update trace trace.Update(ctx, opik.WithTraceMetadata(map[string]any{\"status\": \"completed\"}), opik.WithTraceTags(\"success\"), ) // Update span span.Update(ctx, opik.WithSpanMetadata(map[string]any{\"tokens\": 150}), )","title":"Traces and Spans"},{"location":"core-concepts/traces-and-spans/#traces-and-spans","text":"Traces and spans are the core building blocks for observability in Opik.","title":"Traces and Spans"},{"location":"core-concepts/traces-and-spans/#concepts","text":"Trace : Represents a complete request or workflow through your application Span : Represents a single operation within a trace (e.g., an LLM call, tool execution) Spans can be nested to show parent-child relationships, creating a tree structure that represents your application's execution flow.","title":"Concepts"},{"location":"core-concepts/traces-and-spans/#creating-traces","text":"client, _ := opik.NewClient() // Create a simple trace trace, _ := client.Trace(ctx, \"my-trace\") // Create a trace with options trace, _ := client.Trace(ctx, \"my-trace\", opik.WithTraceInput(map[string]any{\"prompt\": \"Hello\"}), opik.WithTraceMetadata(map[string]any{\"user_id\": \"123\"}), opik.WithTraceTags(\"production\", \"v2\"), ) // End the trace trace.End(ctx) // End with output trace.End(ctx, opik.WithTraceOutput(map[string]any{\"response\": \"Hi!\"}))","title":"Creating Traces"},{"location":"core-concepts/traces-and-spans/#creating-spans","text":"Spans are created from traces or other spans: // Create a span from a trace span, _ := trace.Span(ctx, \"process-input\") // Create a nested span from another span childSpan, _ := span.Span(ctx, \"validate-input\") // End spans (in reverse order) childSpan.End(ctx) span.End(ctx)","title":"Creating Spans"},{"location":"core-concepts/traces-and-spans/#span-types","text":"Use span types to categorize operations: // LLM call span span, _ := trace.Span(ctx, \"llm-call\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), opik.WithSpanProvider(\"openai\"), ) // Tool execution span span, _ := trace.Span(ctx, \"search-tool\", opik.WithSpanType(opik.SpanTypeTool), ) // General processing span (default) span, _ := trace.Span(ctx, \"process-data\", opik.WithSpanType(opik.SpanTypeGeneral), )","title":"Span Types"},{"location":"core-concepts/traces-and-spans/#span-options","text":"Option Description WithSpanType(type) Set span type (LLM, Tool, General) WithSpanModel(model) Set the model name for LLM spans WithSpanProvider(provider) Set the provider name (openai, anthropic) WithSpanInput(data) Set input data WithSpanOutput(data) Set output data WithSpanMetadata(data) Set metadata WithSpanTags(tags...) Add tags","title":"Span Options"},{"location":"core-concepts/traces-and-spans/#complete-example","text":"func processQuery(ctx context.Context, client *opik.Client, query string) (string, error) { // Create trace for the entire request trace, _ := client.Trace(ctx, \"process-query\", opik.WithTraceInput(map[string]any{\"query\": query}), ) defer trace.End(ctx) // Span for preprocessing preprocessSpan, _ := trace.Span(ctx, \"preprocess\") processedQuery := preprocess(query) preprocessSpan.End(ctx, opik.WithSpanOutput(map[string]any{ \"processed\": processedQuery, })) // Span for LLM call llmSpan, _ := trace.Span(ctx, \"llm-call\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), opik.WithSpanInput(map[string]any{\"prompt\": processedQuery}), ) response, err := callLLM(processedQuery) if err != nil { llmSpan.End(ctx, opik.WithSpanMetadata(map[string]any{\"error\": err.Error()})) return \"\", err } llmSpan.End(ctx, opik.WithSpanOutput(map[string]any{\"response\": response})) // Update trace with final output trace.End(ctx, opik.WithTraceOutput(map[string]any{\"result\": response})) return response, nil }","title":"Complete Example"},{"location":"core-concepts/traces-and-spans/#updating-traces-and-spans","text":"Update metadata after creation: // Update trace trace.Update(ctx, opik.WithTraceMetadata(map[string]any{\"status\": \"completed\"}), opik.WithTraceTags(\"success\"), ) // Update span span.Update(ctx, opik.WithSpanMetadata(map[string]any{\"tokens\": 150}), )","title":"Updating Traces and Spans"},{"location":"evaluation/heuristic-metrics/","text":"Heuristic Metrics \u00b6 Rule-based metrics that don't require an LLM. Fast, deterministic, and free. import \"github.com/agentplexus/go-comet-ml-opik/evaluation/heuristic\" String Matching \u00b6 Equals \u00b6 Check if output matches expected exactly. // Case-insensitive metric := heuristic.NewEquals(false) // Case-sensitive metric := heuristic.NewEquals(true) Contains \u00b6 Check if output contains expected as substring. metric := heuristic.NewContains(false) // case-insensitive StartsWith / EndsWith \u00b6 metric := heuristic.NewStartsWith(false) metric := heuristic.NewEndsWith(false) ContainsAny / ContainsAll \u00b6 Check for multiple substrings. // Match if ANY substring is found metric := heuristic.NewContainsAny([]string{\"yes\", \"correct\", \"right\"}, false) // Match if ALL substrings are found metric := heuristic.NewContainsAll([]string{\"name\", \"email\", \"phone\"}, false) NotEmpty \u00b6 Check that output is not empty. metric := heuristic.NewNotEmpty() LengthBetween \u00b6 Check output length is within range. metric := heuristic.NewLengthBetween(10, 1000) // 10-1000 characters WordCount \u00b6 Check word count is within range. metric := heuristic.NewWordCount(5, 100) // 5-100 words Parsing/Format Validation \u00b6 JSON Validation \u00b6 // Check if valid JSON metric := heuristic.NewIsJSON() // Check if JSON object (not array) metric := heuristic.NewIsJSONObject() // Check if JSON array metric := heuristic.NewIsJSONArray() // Check if JSON has specific keys metric := heuristic.NewJSONHasKeys([]string{\"name\", \"email\", \"status\"}) // Validate against JSON schema schema := `{ \"type\": \"object\", \"properties\": { \"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"number\"} }, \"required\": [\"name\"] }` metric := heuristic.NewJSONSchemaValid(schema) XML Validation \u00b6 metric := heuristic.NewIsXML() Type Validation \u00b6 metric := heuristic.NewIsNumber() // Valid number metric := heuristic.NewIsBoolean() // \"true\"/\"false\" Pattern Matching \u00b6 Regex Match \u00b6 // Must match pattern metric := heuristic.MustRegexMatch(`^\\d{3}-\\d{4}$`) // phone format // Must NOT match pattern metric := heuristic.MustRegexNotMatch(`(?i)error|fail`) Common Formats \u00b6 // Email format metric := heuristic.NewEmailFormat() // URL format metric := heuristic.NewURLFormat() // Phone format (flexible) metric := heuristic.NewPhoneFormat() // Date format (ISO 8601) metric := heuristic.NewDateFormat() // UUID format metric := heuristic.NewUUIDFormat() Text Similarity \u00b6 Levenshtein Similarity \u00b6 Edit distance based similarity (0-1 scale). metric := heuristic.NewLevenshteinSimilarity(false) // case-insensitive Jaccard Similarity \u00b6 Word overlap similarity. metric := heuristic.NewJaccardSimilarity(false) Cosine Similarity \u00b6 Word vector cosine similarity. metric := heuristic.NewCosineSimilarity(false) BLEU Score \u00b6 Machine translation evaluation metric. metric := heuristic.NewBLEU(4) // n-gram size up to 4 ROUGE Score \u00b6 Recall-oriented similarity metric. metric := heuristic.NewROUGE(1.0) // beta parameter Fuzzy Match \u00b6 Flexible string matching with threshold. metric := heuristic.NewFuzzyMatch(0.8, false) // 80% threshold Using Multiple Metrics \u00b6 metrics := []evaluation.Metric{ heuristic.NewEquals(false), heuristic.NewContains(false), heuristic.NewIsJSON(), heuristic.NewLevenshteinSimilarity(false), heuristic.NewBLEU(4), } engine := evaluation.NewEngine(metrics) input := evaluation.NewMetricInput(\"prompt\", \"output\").WithExpected(\"expected\") result := engine.EvaluateOne(ctx, input) for name, score := range result.Scores { fmt.Printf(\"%s: %.2f\\n\", name, score.Value) } Creating Custom Heuristics \u00b6 Implement the Metric interface: type MyMetric struct { evaluation.BaseMetric } func NewMyMetric() *MyMetric { return &MyMetric{ BaseMetric: evaluation.NewBaseMetric(\"my_metric\"), } } func (m *MyMetric) Score(ctx context.Context, input evaluation.MetricInput) *evaluation.ScoreResult { // Your custom logic score := calculateScore(input.Output, input.Expected) return evaluation.NewScoreResult(m.Name(), score) }","title":"Heuristic Metrics"},{"location":"evaluation/heuristic-metrics/#heuristic-metrics","text":"Rule-based metrics that don't require an LLM. Fast, deterministic, and free. import \"github.com/agentplexus/go-comet-ml-opik/evaluation/heuristic\"","title":"Heuristic Metrics"},{"location":"evaluation/heuristic-metrics/#string-matching","text":"","title":"String Matching"},{"location":"evaluation/heuristic-metrics/#equals","text":"Check if output matches expected exactly. // Case-insensitive metric := heuristic.NewEquals(false) // Case-sensitive metric := heuristic.NewEquals(true)","title":"Equals"},{"location":"evaluation/heuristic-metrics/#contains","text":"Check if output contains expected as substring. metric := heuristic.NewContains(false) // case-insensitive","title":"Contains"},{"location":"evaluation/heuristic-metrics/#startswith-endswith","text":"metric := heuristic.NewStartsWith(false) metric := heuristic.NewEndsWith(false)","title":"StartsWith / EndsWith"},{"location":"evaluation/heuristic-metrics/#containsany-containsall","text":"Check for multiple substrings. // Match if ANY substring is found metric := heuristic.NewContainsAny([]string{\"yes\", \"correct\", \"right\"}, false) // Match if ALL substrings are found metric := heuristic.NewContainsAll([]string{\"name\", \"email\", \"phone\"}, false)","title":"ContainsAny / ContainsAll"},{"location":"evaluation/heuristic-metrics/#notempty","text":"Check that output is not empty. metric := heuristic.NewNotEmpty()","title":"NotEmpty"},{"location":"evaluation/heuristic-metrics/#lengthbetween","text":"Check output length is within range. metric := heuristic.NewLengthBetween(10, 1000) // 10-1000 characters","title":"LengthBetween"},{"location":"evaluation/heuristic-metrics/#wordcount","text":"Check word count is within range. metric := heuristic.NewWordCount(5, 100) // 5-100 words","title":"WordCount"},{"location":"evaluation/heuristic-metrics/#parsingformat-validation","text":"","title":"Parsing/Format Validation"},{"location":"evaluation/heuristic-metrics/#json-validation","text":"// Check if valid JSON metric := heuristic.NewIsJSON() // Check if JSON object (not array) metric := heuristic.NewIsJSONObject() // Check if JSON array metric := heuristic.NewIsJSONArray() // Check if JSON has specific keys metric := heuristic.NewJSONHasKeys([]string{\"name\", \"email\", \"status\"}) // Validate against JSON schema schema := `{ \"type\": \"object\", \"properties\": { \"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"number\"} }, \"required\": [\"name\"] }` metric := heuristic.NewJSONSchemaValid(schema)","title":"JSON Validation"},{"location":"evaluation/heuristic-metrics/#xml-validation","text":"metric := heuristic.NewIsXML()","title":"XML Validation"},{"location":"evaluation/heuristic-metrics/#type-validation","text":"metric := heuristic.NewIsNumber() // Valid number metric := heuristic.NewIsBoolean() // \"true\"/\"false\"","title":"Type Validation"},{"location":"evaluation/heuristic-metrics/#pattern-matching","text":"","title":"Pattern Matching"},{"location":"evaluation/heuristic-metrics/#regex-match","text":"// Must match pattern metric := heuristic.MustRegexMatch(`^\\d{3}-\\d{4}$`) // phone format // Must NOT match pattern metric := heuristic.MustRegexNotMatch(`(?i)error|fail`)","title":"Regex Match"},{"location":"evaluation/heuristic-metrics/#common-formats","text":"// Email format metric := heuristic.NewEmailFormat() // URL format metric := heuristic.NewURLFormat() // Phone format (flexible) metric := heuristic.NewPhoneFormat() // Date format (ISO 8601) metric := heuristic.NewDateFormat() // UUID format metric := heuristic.NewUUIDFormat()","title":"Common Formats"},{"location":"evaluation/heuristic-metrics/#text-similarity","text":"","title":"Text Similarity"},{"location":"evaluation/heuristic-metrics/#levenshtein-similarity","text":"Edit distance based similarity (0-1 scale). metric := heuristic.NewLevenshteinSimilarity(false) // case-insensitive","title":"Levenshtein Similarity"},{"location":"evaluation/heuristic-metrics/#jaccard-similarity","text":"Word overlap similarity. metric := heuristic.NewJaccardSimilarity(false)","title":"Jaccard Similarity"},{"location":"evaluation/heuristic-metrics/#cosine-similarity","text":"Word vector cosine similarity. metric := heuristic.NewCosineSimilarity(false)","title":"Cosine Similarity"},{"location":"evaluation/heuristic-metrics/#bleu-score","text":"Machine translation evaluation metric. metric := heuristic.NewBLEU(4) // n-gram size up to 4","title":"BLEU Score"},{"location":"evaluation/heuristic-metrics/#rouge-score","text":"Recall-oriented similarity metric. metric := heuristic.NewROUGE(1.0) // beta parameter","title":"ROUGE Score"},{"location":"evaluation/heuristic-metrics/#fuzzy-match","text":"Flexible string matching with threshold. metric := heuristic.NewFuzzyMatch(0.8, false) // 80% threshold","title":"Fuzzy Match"},{"location":"evaluation/heuristic-metrics/#using-multiple-metrics","text":"metrics := []evaluation.Metric{ heuristic.NewEquals(false), heuristic.NewContains(false), heuristic.NewIsJSON(), heuristic.NewLevenshteinSimilarity(false), heuristic.NewBLEU(4), } engine := evaluation.NewEngine(metrics) input := evaluation.NewMetricInput(\"prompt\", \"output\").WithExpected(\"expected\") result := engine.EvaluateOne(ctx, input) for name, score := range result.Scores { fmt.Printf(\"%s: %.2f\\n\", name, score.Value) }","title":"Using Multiple Metrics"},{"location":"evaluation/heuristic-metrics/#creating-custom-heuristics","text":"Implement the Metric interface: type MyMetric struct { evaluation.BaseMetric } func NewMyMetric() *MyMetric { return &MyMetric{ BaseMetric: evaluation.NewBaseMetric(\"my_metric\"), } } func (m *MyMetric) Score(ctx context.Context, input evaluation.MetricInput) *evaluation.ScoreResult { // Your custom logic score := calculateScore(input.Output, input.Expected) return evaluation.NewScoreResult(m.Name(), score) }","title":"Creating Custom Heuristics"},{"location":"evaluation/llm-judges/","text":"LLM Judge Metrics \u00b6 Use an LLM to evaluate outputs that can't be measured with simple rules. import ( \"github.com/agentplexus/go-comet-ml-opik/evaluation/llm\" \"github.com/agentplexus/go-comet-ml-opik/integrations/openai\" ) Setting Up a Provider \u00b6 First, create an LLM provider: // OpenAI provider := openai.NewProvider( openai.WithAPIKey(\"your-api-key\"), openai.WithModel(\"gpt-4o\"), ) // Anthropic provider := anthropic.NewProvider( anthropic.WithAPIKey(\"your-api-key\"), anthropic.WithModel(\"claude-sonnet-4-20250514\"), ) // gollm (any provider) provider := gollm.NewProvider(gollmClient, gollm.WithModel(\"gpt-4o\"), ) Built-in Judge Metrics \u00b6 Answer Relevance \u00b6 Evaluates how relevant the answer is to the question. metric := llm.NewAnswerRelevance(provider) Hallucination \u00b6 Detects factual claims not supported by the context. metric := llm.NewHallucination(provider) Requires context in the input: input := evaluation.NewMetricInput(question, answer). WithContext(relevantDocuments) Factuality \u00b6 Checks if the response is factually accurate. metric := llm.NewFactuality(provider) Context Recall \u00b6 Measures how much of the expected information is captured. metric := llm.NewContextRecall(provider) Context Precision \u00b6 Measures precision of information retrieval. metric := llm.NewContextPrecision(provider) Moderation \u00b6 Checks for harmful, inappropriate, or policy-violating content. metric := llm.NewModeration(provider) Coherence \u00b6 Evaluates logical flow and consistency. metric := llm.NewCoherence(provider) Helpfulness \u00b6 Measures how helpful the response is to the user. metric := llm.NewHelpfulness(provider) G-EVAL \u00b6 Flexible evaluation with custom criteria and evaluation steps. geval := llm.NewGEval(provider, \"fluency and coherence\") // With custom evaluation steps geval = geval.WithEvaluationSteps([]string{ \"Check if the response is grammatically correct\", \"Evaluate the logical flow of ideas\", \"Assess clarity and readability\", \"Check for appropriate vocabulary usage\", }) score := geval.Score(ctx, input) Custom Judge \u00b6 Create a judge with a custom prompt template: prompt := ` Evaluate whether the response maintains a professional tone. User message: {{input}} AI response: {{output}} Provide a score from 0.0 to 1.0 where: - 1.0: Completely professional - 0.5: Somewhat professional with minor issues - 0.0: Unprofessional Return JSON: {\"score\": <float>, \"reason\": \"<explanation>\"} ` judge := llm.NewCustomJudge(\"tone_check\", prompt, provider) Template Variables \u00b6 Variable Description {{input}} The original input/prompt {{output}} The LLM's response {{expected}} Expected/ground truth output {{context}} Additional context Using Multiple Judges \u00b6 metrics := []evaluation.Metric{ llm.NewAnswerRelevance(provider), llm.NewHallucination(provider), llm.NewCoherence(provider), llm.NewHelpfulness(provider), } engine := evaluation.NewEngine(metrics, evaluation.WithConcurrency(2), // Limit concurrent LLM calls ) input := evaluation.NewMetricInput(question, answer). WithExpected(expectedAnswer). WithContext(documents) result := engine.EvaluateOne(ctx, input) Caching Responses \u00b6 Reduce costs by caching identical evaluations: // Wrap provider with caching cachedProvider := llm.NewCachingProvider(provider) // Use cached provider for metrics metric := llm.NewAnswerRelevance(cachedProvider) Best Practices \u00b6 Choose appropriate models : GPT-4 or Claude 3 for nuanced evaluation Limit concurrency : Respect rate limits Use caching : For repeated evaluations Combine with heuristics : Use LLM judges only when needed Monitor costs : LLM evaluations add up Cost Considerations \u00b6 Each LLM judge metric makes an API call. For large datasets: Pre-filter with heuristic metrics Use caching for duplicate inputs Batch evaluations during off-peak hours Consider smaller models for simple judgments","title":"LLM Judges"},{"location":"evaluation/llm-judges/#llm-judge-metrics","text":"Use an LLM to evaluate outputs that can't be measured with simple rules. import ( \"github.com/agentplexus/go-comet-ml-opik/evaluation/llm\" \"github.com/agentplexus/go-comet-ml-opik/integrations/openai\" )","title":"LLM Judge Metrics"},{"location":"evaluation/llm-judges/#setting-up-a-provider","text":"First, create an LLM provider: // OpenAI provider := openai.NewProvider( openai.WithAPIKey(\"your-api-key\"), openai.WithModel(\"gpt-4o\"), ) // Anthropic provider := anthropic.NewProvider( anthropic.WithAPIKey(\"your-api-key\"), anthropic.WithModel(\"claude-sonnet-4-20250514\"), ) // gollm (any provider) provider := gollm.NewProvider(gollmClient, gollm.WithModel(\"gpt-4o\"), )","title":"Setting Up a Provider"},{"location":"evaluation/llm-judges/#built-in-judge-metrics","text":"","title":"Built-in Judge Metrics"},{"location":"evaluation/llm-judges/#answer-relevance","text":"Evaluates how relevant the answer is to the question. metric := llm.NewAnswerRelevance(provider)","title":"Answer Relevance"},{"location":"evaluation/llm-judges/#hallucination","text":"Detects factual claims not supported by the context. metric := llm.NewHallucination(provider) Requires context in the input: input := evaluation.NewMetricInput(question, answer). WithContext(relevantDocuments)","title":"Hallucination"},{"location":"evaluation/llm-judges/#factuality","text":"Checks if the response is factually accurate. metric := llm.NewFactuality(provider)","title":"Factuality"},{"location":"evaluation/llm-judges/#context-recall","text":"Measures how much of the expected information is captured. metric := llm.NewContextRecall(provider)","title":"Context Recall"},{"location":"evaluation/llm-judges/#context-precision","text":"Measures precision of information retrieval. metric := llm.NewContextPrecision(provider)","title":"Context Precision"},{"location":"evaluation/llm-judges/#moderation","text":"Checks for harmful, inappropriate, or policy-violating content. metric := llm.NewModeration(provider)","title":"Moderation"},{"location":"evaluation/llm-judges/#coherence","text":"Evaluates logical flow and consistency. metric := llm.NewCoherence(provider)","title":"Coherence"},{"location":"evaluation/llm-judges/#helpfulness","text":"Measures how helpful the response is to the user. metric := llm.NewHelpfulness(provider)","title":"Helpfulness"},{"location":"evaluation/llm-judges/#g-eval","text":"Flexible evaluation with custom criteria and evaluation steps. geval := llm.NewGEval(provider, \"fluency and coherence\") // With custom evaluation steps geval = geval.WithEvaluationSteps([]string{ \"Check if the response is grammatically correct\", \"Evaluate the logical flow of ideas\", \"Assess clarity and readability\", \"Check for appropriate vocabulary usage\", }) score := geval.Score(ctx, input)","title":"G-EVAL"},{"location":"evaluation/llm-judges/#custom-judge","text":"Create a judge with a custom prompt template: prompt := ` Evaluate whether the response maintains a professional tone. User message: {{input}} AI response: {{output}} Provide a score from 0.0 to 1.0 where: - 1.0: Completely professional - 0.5: Somewhat professional with minor issues - 0.0: Unprofessional Return JSON: {\"score\": <float>, \"reason\": \"<explanation>\"} ` judge := llm.NewCustomJudge(\"tone_check\", prompt, provider)","title":"Custom Judge"},{"location":"evaluation/llm-judges/#template-variables","text":"Variable Description {{input}} The original input/prompt {{output}} The LLM's response {{expected}} Expected/ground truth output {{context}} Additional context","title":"Template Variables"},{"location":"evaluation/llm-judges/#using-multiple-judges","text":"metrics := []evaluation.Metric{ llm.NewAnswerRelevance(provider), llm.NewHallucination(provider), llm.NewCoherence(provider), llm.NewHelpfulness(provider), } engine := evaluation.NewEngine(metrics, evaluation.WithConcurrency(2), // Limit concurrent LLM calls ) input := evaluation.NewMetricInput(question, answer). WithExpected(expectedAnswer). WithContext(documents) result := engine.EvaluateOne(ctx, input)","title":"Using Multiple Judges"},{"location":"evaluation/llm-judges/#caching-responses","text":"Reduce costs by caching identical evaluations: // Wrap provider with caching cachedProvider := llm.NewCachingProvider(provider) // Use cached provider for metrics metric := llm.NewAnswerRelevance(cachedProvider)","title":"Caching Responses"},{"location":"evaluation/llm-judges/#best-practices","text":"Choose appropriate models : GPT-4 or Claude 3 for nuanced evaluation Limit concurrency : Respect rate limits Use caching : For repeated evaluations Combine with heuristics : Use LLM judges only when needed Monitor costs : LLM evaluations add up","title":"Best Practices"},{"location":"evaluation/llm-judges/#cost-considerations","text":"Each LLM judge metric makes an API call. For large datasets: Pre-filter with heuristic metrics Use caching for duplicate inputs Batch evaluations during off-peak hours Consider smaller models for simple judgments","title":"Cost Considerations"},{"location":"evaluation/overview/","text":"Evaluation Framework \u00b6 The evaluation framework provides tools for measuring LLM output quality using both rule-based heuristics and LLM-as-judge approaches. Architecture \u00b6 evaluation/ \u251c\u2500\u2500 Metric # Interface for all metrics \u251c\u2500\u2500 MetricInput # Input data for evaluation \u251c\u2500\u2500 ScoreResult # Result of a metric evaluation \u251c\u2500\u2500 Engine # Runs multiple metrics concurrently \u251c\u2500\u2500 heuristic/ # Rule-based metrics \u2502 \u251c\u2500\u2500 string.go # String matching (equals, contains) \u2502 \u251c\u2500\u2500 parsing.go # Format validation (JSON, XML) \u2502 \u251c\u2500\u2500 pattern.go # Regex and format patterns \u2502 \u2514\u2500\u2500 similarity.go # Text similarity (BLEU, ROUGE) \u2514\u2500\u2500 llm/ # LLM-based judge metrics \u251c\u2500\u2500 provider.go # LLM provider interface \u2514\u2500\u2500 metrics.go # Judge metrics (relevance, hallucination) Quick Example \u00b6 import ( \"github.com/agentplexus/go-comet-ml-opik/evaluation\" \"github.com/agentplexus/go-comet-ml-opik/evaluation/heuristic\" ) // Create metrics metrics := []evaluation.Metric{ heuristic.NewEquals(false), // Case-insensitive equality heuristic.NewContains(false), // Substring check heuristic.NewIsJSON(), // JSON validation } // Create engine engine := evaluation.NewEngine(metrics, evaluation.WithConcurrency(4), ) // Create input input := evaluation.NewMetricInput(\"What is 2+2?\", \"The answer is 4.\") input = input.WithExpected(\"4\") // Evaluate result := engine.EvaluateOne(ctx, input) fmt.Printf(\"Average score: %.2f\\n\", result.AverageScore()) for name, score := range result.Scores { fmt.Printf(\" %s: %.2f\\n\", name, score.Value) } Metric Interface \u00b6 All metrics implement this interface: type Metric interface { Name() string Score(ctx context.Context, input MetricInput) *ScoreResult } MetricInput \u00b6 Contains all data needed for evaluation: type MetricInput struct { Input string // The original input/prompt Output string // The LLM's output to evaluate Expected string // Expected/ground truth output Context string // Additional context Metadata map[string]any // Any extra data } // Create input input := evaluation.NewMetricInput(prompt, llmOutput) input = input.WithExpected(expectedOutput) input = input.WithContext(additionalContext) ScoreResult \u00b6 Contains the evaluation result: type ScoreResult struct { Name string // Metric name Value float64 // Score (typically 0.0 to 1.0) Reason string // Explanation for the score Metadata map[string]any // Additional data Error error // Error if evaluation failed } // Helper constructors score := evaluation.NewScoreResult(\"accuracy\", 0.95) score := evaluation.NewScoreResultWithReason(\"accuracy\", 0.95, \"Exact match found\") score := evaluation.BooleanScore(\"is_valid\", true) // 1.0 for true, 0.0 for false Evaluation Engine \u00b6 Run multiple metrics concurrently: // Create engine with options engine := evaluation.NewEngine(metrics, evaluation.WithConcurrency(4), // Run 4 metrics in parallel ) // Evaluate single input result := engine.EvaluateOne(ctx, input) // Evaluate multiple inputs inputs := []evaluation.MetricInput{input1, input2, input3} results := engine.EvaluateMany(ctx, inputs) // Evaluate with item IDs (for datasets) itemResults := engine.EvaluateWithIDs(ctx, map[string]evaluation.MetricInput{ \"item-1\": input1, \"item-2\": input2, }) Dataset Evaluator \u00b6 Evaluate entire datasets: evaluator := evaluation.NewDatasetEvaluator(engine, client) results, err := evaluator.Evaluate(ctx, dataset, func(item map[string]any) string { // Generate output for each dataset item return llmClient.Complete(item[\"input\"].(string)) }, ) Metric Categories \u00b6 Category Description Examples Heuristic Rule-based, deterministic Equals, Contains, IsJSON, BLEU LLM Judge Uses LLM to evaluate Relevance, Hallucination, Factuality Best Practices \u00b6 Combine metrics : Use multiple metrics for comprehensive evaluation Use heuristics first : They're faster and cheaper than LLM judges Set appropriate concurrency : Balance speed vs. rate limits Handle errors : Check ScoreResult.Error for failed evaluations Log to traces : Add scores as feedback to traces for tracking","title":"Overview"},{"location":"evaluation/overview/#evaluation-framework","text":"The evaluation framework provides tools for measuring LLM output quality using both rule-based heuristics and LLM-as-judge approaches.","title":"Evaluation Framework"},{"location":"evaluation/overview/#architecture","text":"evaluation/ \u251c\u2500\u2500 Metric # Interface for all metrics \u251c\u2500\u2500 MetricInput # Input data for evaluation \u251c\u2500\u2500 ScoreResult # Result of a metric evaluation \u251c\u2500\u2500 Engine # Runs multiple metrics concurrently \u251c\u2500\u2500 heuristic/ # Rule-based metrics \u2502 \u251c\u2500\u2500 string.go # String matching (equals, contains) \u2502 \u251c\u2500\u2500 parsing.go # Format validation (JSON, XML) \u2502 \u251c\u2500\u2500 pattern.go # Regex and format patterns \u2502 \u2514\u2500\u2500 similarity.go # Text similarity (BLEU, ROUGE) \u2514\u2500\u2500 llm/ # LLM-based judge metrics \u251c\u2500\u2500 provider.go # LLM provider interface \u2514\u2500\u2500 metrics.go # Judge metrics (relevance, hallucination)","title":"Architecture"},{"location":"evaluation/overview/#quick-example","text":"import ( \"github.com/agentplexus/go-comet-ml-opik/evaluation\" \"github.com/agentplexus/go-comet-ml-opik/evaluation/heuristic\" ) // Create metrics metrics := []evaluation.Metric{ heuristic.NewEquals(false), // Case-insensitive equality heuristic.NewContains(false), // Substring check heuristic.NewIsJSON(), // JSON validation } // Create engine engine := evaluation.NewEngine(metrics, evaluation.WithConcurrency(4), ) // Create input input := evaluation.NewMetricInput(\"What is 2+2?\", \"The answer is 4.\") input = input.WithExpected(\"4\") // Evaluate result := engine.EvaluateOne(ctx, input) fmt.Printf(\"Average score: %.2f\\n\", result.AverageScore()) for name, score := range result.Scores { fmt.Printf(\" %s: %.2f\\n\", name, score.Value) }","title":"Quick Example"},{"location":"evaluation/overview/#metric-interface","text":"All metrics implement this interface: type Metric interface { Name() string Score(ctx context.Context, input MetricInput) *ScoreResult }","title":"Metric Interface"},{"location":"evaluation/overview/#metricinput","text":"Contains all data needed for evaluation: type MetricInput struct { Input string // The original input/prompt Output string // The LLM's output to evaluate Expected string // Expected/ground truth output Context string // Additional context Metadata map[string]any // Any extra data } // Create input input := evaluation.NewMetricInput(prompt, llmOutput) input = input.WithExpected(expectedOutput) input = input.WithContext(additionalContext)","title":"MetricInput"},{"location":"evaluation/overview/#scoreresult","text":"Contains the evaluation result: type ScoreResult struct { Name string // Metric name Value float64 // Score (typically 0.0 to 1.0) Reason string // Explanation for the score Metadata map[string]any // Additional data Error error // Error if evaluation failed } // Helper constructors score := evaluation.NewScoreResult(\"accuracy\", 0.95) score := evaluation.NewScoreResultWithReason(\"accuracy\", 0.95, \"Exact match found\") score := evaluation.BooleanScore(\"is_valid\", true) // 1.0 for true, 0.0 for false","title":"ScoreResult"},{"location":"evaluation/overview/#evaluation-engine","text":"Run multiple metrics concurrently: // Create engine with options engine := evaluation.NewEngine(metrics, evaluation.WithConcurrency(4), // Run 4 metrics in parallel ) // Evaluate single input result := engine.EvaluateOne(ctx, input) // Evaluate multiple inputs inputs := []evaluation.MetricInput{input1, input2, input3} results := engine.EvaluateMany(ctx, inputs) // Evaluate with item IDs (for datasets) itemResults := engine.EvaluateWithIDs(ctx, map[string]evaluation.MetricInput{ \"item-1\": input1, \"item-2\": input2, })","title":"Evaluation Engine"},{"location":"evaluation/overview/#dataset-evaluator","text":"Evaluate entire datasets: evaluator := evaluation.NewDatasetEvaluator(engine, client) results, err := evaluator.Evaluate(ctx, dataset, func(item map[string]any) string { // Generate output for each dataset item return llmClient.Complete(item[\"input\"].(string)) }, )","title":"Dataset Evaluator"},{"location":"evaluation/overview/#metric-categories","text":"Category Description Examples Heuristic Rule-based, deterministic Equals, Contains, IsJSON, BLEU LLM Judge Uses LLM to evaluate Relevance, Hallucination, Factuality","title":"Metric Categories"},{"location":"evaluation/overview/#best-practices","text":"Combine metrics : Use multiple metrics for comprehensive evaluation Use heuristics first : They're faster and cheaper than LLM judges Set appropriate concurrency : Balance speed vs. rate limits Handle errors : Check ScoreResult.Error for failed evaluations Log to traces : Add scores as feedback to traces for tracking","title":"Best Practices"},{"location":"features/batching/","text":"Batching \u00b6 Batch API calls for improved performance and efficiency. Batching Client \u00b6 // Create a batching client client, _ := opik.NewBatchingClient( opik.WithProjectName(\"My Project\"), ) // Operations are batched automatically client.AddFeedbackAsync(\"trace\", traceID, \"score\", 0.95, \"reason\") // Flush pending operations client.Flush(5 * time.Second) // Close and flush on shutdown defer client.Close(10 * time.Second) Local Recording (Testing) \u00b6 For testing without sending data to the server: // Record traces locally client := opik.RecordTracesLocally(\"my-project\") // Use normally trace, _ := client.Trace(ctx, \"test-trace\") span, _ := trace.Span(ctx, \"test-span\") span.End(ctx) trace.End(ctx) // Access recorded data recording := client.Recording() traces := recording.Traces() spans := recording.Spans() // Inspect recorded data for _, t := range traces { fmt.Printf(\"Trace: %s - %s\\n\", t.ID, t.Name) } Attachments \u00b6 Create and manage file attachments: From File \u00b6 attachment, err := opik.NewAttachmentFromFile(\"/path/to/image.png\") if err != nil { log.Fatal(err) } fmt.Printf(\"Name: %s\\n\", attachment.Name) fmt.Printf(\"Type: %s\\n\", attachment.ContentType) fmt.Printf(\"Size: %d bytes\\n\", len(attachment.Data)) From Bytes \u00b6 jsonData := []byte(`{\"key\": \"value\"}`) attachment := opik.NewAttachmentFromBytes(\"data.json\", jsonData, \"application/json\") Text Attachment \u00b6 attachment := opik.NewTextAttachment(\"notes.txt\", \"Some text content\") Using Attachments \u00b6 // Get data URL for embedding in HTML/markdown dataURL := attachment.ToDataURL() // Result: \"data:image/png;base64,...\" // Include in span metadata span.End(ctx, opik.WithSpanMetadata(map[string]any{ \"attachment\": dataURL, })) Batch Operations \u00b6 Async Feedback \u00b6 // Add feedback without waiting client.AddFeedbackAsync(\"trace\", traceID, \"accuracy\", 0.95, \"High accuracy\") client.AddFeedbackAsync(\"span\", spanID, \"quality\", 0.87, \"Good quality\") // Flush when ready client.Flush(5 * time.Second) Graceful Shutdown \u00b6 func main() { client, _ := opik.NewBatchingClient() // Ensure clean shutdown defer func() { if err := client.Close(10 * time.Second); err != nil { log.Printf(\"Error closing client: %v\", err) } }() // Your application code... } Configuration Options \u00b6 Batch Size \u00b6 client, _ := opik.NewBatchingClient( opik.WithBatchSize(100), // Send when 100 items accumulated ) Flush Interval \u00b6 client, _ := opik.NewBatchingClient( opik.WithFlushInterval(5 * time.Second), // Auto-flush every 5 seconds ) Best Practices \u00b6 Use batching for high-volume : Reduce API calls in production Always close clients : Ensure pending data is flushed Set appropriate timeouts : Allow enough time for flushing Use local recording for tests : Avoid external dependencies in tests Monitor batch sizes : Tune for your workload","title":"Batching"},{"location":"features/batching/#batching","text":"Batch API calls for improved performance and efficiency.","title":"Batching"},{"location":"features/batching/#batching-client","text":"// Create a batching client client, _ := opik.NewBatchingClient( opik.WithProjectName(\"My Project\"), ) // Operations are batched automatically client.AddFeedbackAsync(\"trace\", traceID, \"score\", 0.95, \"reason\") // Flush pending operations client.Flush(5 * time.Second) // Close and flush on shutdown defer client.Close(10 * time.Second)","title":"Batching Client"},{"location":"features/batching/#local-recording-testing","text":"For testing without sending data to the server: // Record traces locally client := opik.RecordTracesLocally(\"my-project\") // Use normally trace, _ := client.Trace(ctx, \"test-trace\") span, _ := trace.Span(ctx, \"test-span\") span.End(ctx) trace.End(ctx) // Access recorded data recording := client.Recording() traces := recording.Traces() spans := recording.Spans() // Inspect recorded data for _, t := range traces { fmt.Printf(\"Trace: %s - %s\\n\", t.ID, t.Name) }","title":"Local Recording (Testing)"},{"location":"features/batching/#attachments","text":"Create and manage file attachments:","title":"Attachments"},{"location":"features/batching/#from-file","text":"attachment, err := opik.NewAttachmentFromFile(\"/path/to/image.png\") if err != nil { log.Fatal(err) } fmt.Printf(\"Name: %s\\n\", attachment.Name) fmt.Printf(\"Type: %s\\n\", attachment.ContentType) fmt.Printf(\"Size: %d bytes\\n\", len(attachment.Data))","title":"From File"},{"location":"features/batching/#from-bytes","text":"jsonData := []byte(`{\"key\": \"value\"}`) attachment := opik.NewAttachmentFromBytes(\"data.json\", jsonData, \"application/json\")","title":"From Bytes"},{"location":"features/batching/#text-attachment","text":"attachment := opik.NewTextAttachment(\"notes.txt\", \"Some text content\")","title":"Text Attachment"},{"location":"features/batching/#using-attachments","text":"// Get data URL for embedding in HTML/markdown dataURL := attachment.ToDataURL() // Result: \"data:image/png;base64,...\" // Include in span metadata span.End(ctx, opik.WithSpanMetadata(map[string]any{ \"attachment\": dataURL, }))","title":"Using Attachments"},{"location":"features/batching/#batch-operations","text":"","title":"Batch Operations"},{"location":"features/batching/#async-feedback","text":"// Add feedback without waiting client.AddFeedbackAsync(\"trace\", traceID, \"accuracy\", 0.95, \"High accuracy\") client.AddFeedbackAsync(\"span\", spanID, \"quality\", 0.87, \"Good quality\") // Flush when ready client.Flush(5 * time.Second)","title":"Async Feedback"},{"location":"features/batching/#graceful-shutdown","text":"func main() { client, _ := opik.NewBatchingClient() // Ensure clean shutdown defer func() { if err := client.Close(10 * time.Second); err != nil { log.Printf(\"Error closing client: %v\", err) } }() // Your application code... }","title":"Graceful Shutdown"},{"location":"features/batching/#configuration-options","text":"","title":"Configuration Options"},{"location":"features/batching/#batch-size","text":"client, _ := opik.NewBatchingClient( opik.WithBatchSize(100), // Send when 100 items accumulated )","title":"Batch Size"},{"location":"features/batching/#flush-interval","text":"client, _ := opik.NewBatchingClient( opik.WithFlushInterval(5 * time.Second), // Auto-flush every 5 seconds )","title":"Flush Interval"},{"location":"features/batching/#best-practices","text":"Use batching for high-volume : Reduce API calls in production Always close clients : Ensure pending data is flushed Set appropriate timeouts : Allow enough time for flushing Use local recording for tests : Avoid external dependencies in tests Monitor batch sizes : Tune for your workload","title":"Best Practices"},{"location":"features/datasets/","text":"Datasets \u00b6 Datasets store evaluation data for testing and benchmarking your LLM applications. Creating Datasets \u00b6 // Create a simple dataset dataset, _ := client.CreateDataset(ctx, \"my-dataset\") // Create with options dataset, _ := client.CreateDataset(ctx, \"evaluation-data\", opik.WithDatasetDescription(\"Test data for Q&A evaluation\"), opik.WithDatasetTags(\"qa\", \"evaluation\", \"v1\"), ) Inserting Items \u00b6 Single Item \u00b6 dataset.InsertItem(ctx, map[string]any{ \"input\": \"What is the capital of France?\", \"expected\": \"Paris\", \"category\": \"geography\", }) Multiple Items \u00b6 items := []map[string]any{ { \"input\": \"What is 2+2?\", \"expected\": \"4\", \"category\": \"math\", }, { \"input\": \"What is 3+3?\", \"expected\": \"6\", \"category\": \"math\", }, { \"input\": \"What color is the sky?\", \"expected\": \"Blue\", \"category\": \"general\", }, } dataset.InsertItems(ctx, items) Retrieving Items \u00b6 // Get items with pagination items, _ := dataset.GetItems(ctx, 1, 100) // page 1, 100 items per page for _, item := range items { fmt.Printf(\"Input: %v\\n\", item.Data[\"input\"]) fmt.Printf(\"Expected: %v\\n\", item.Data[\"expected\"]) } Listing Datasets \u00b6 // List all datasets datasets, _ := client.ListDatasets(ctx, 1, 100) for _, ds := range datasets { fmt.Printf(\"Dataset: %s (ID: %s)\\n\", ds.Name, ds.ID) } Getting a Dataset by Name \u00b6 dataset, _ := client.GetDatasetByName(ctx, \"my-dataset\") fmt.Printf(\"Name: %s\\n\", dataset.Name) fmt.Printf(\"Description: %s\\n\", dataset.Description) Deleting Datasets \u00b6 // Delete by dataset object dataset.Delete(ctx) Dataset Item Structure \u00b6 Each dataset item can contain any fields you need: Common Field Description input The input/prompt to evaluate expected Expected/ground truth output context Additional context for evaluation metadata Any additional metadata Example: Building an Evaluation Dataset \u00b6 func buildEvaluationDataset(ctx context.Context, client *opik.Client) (*opik.Dataset, error) { // Create dataset dataset, err := client.CreateDataset(ctx, \"qa-evaluation-v1\", opik.WithDatasetDescription(\"Q&A evaluation dataset\"), opik.WithDatasetTags(\"qa\", \"production\"), ) if err != nil { return nil, err } // Load test cases from file or database testCases := loadTestCases() // Convert to dataset items items := make([]map[string]any, len(testCases)) for i, tc := range testCases { items[i] = map[string]any{ \"input\": tc.Question, \"expected\": tc.Answer, \"context\": tc.Context, \"category\": tc.Category, } } // Bulk insert err = dataset.InsertItems(ctx, items) if err != nil { return nil, err } return dataset, nil } Using Datasets with Experiments \u00b6 See Experiments for how to run evaluations against datasets.","title":"Datasets"},{"location":"features/datasets/#datasets","text":"Datasets store evaluation data for testing and benchmarking your LLM applications.","title":"Datasets"},{"location":"features/datasets/#creating-datasets","text":"// Create a simple dataset dataset, _ := client.CreateDataset(ctx, \"my-dataset\") // Create with options dataset, _ := client.CreateDataset(ctx, \"evaluation-data\", opik.WithDatasetDescription(\"Test data for Q&A evaluation\"), opik.WithDatasetTags(\"qa\", \"evaluation\", \"v1\"), )","title":"Creating Datasets"},{"location":"features/datasets/#inserting-items","text":"","title":"Inserting Items"},{"location":"features/datasets/#single-item","text":"dataset.InsertItem(ctx, map[string]any{ \"input\": \"What is the capital of France?\", \"expected\": \"Paris\", \"category\": \"geography\", })","title":"Single Item"},{"location":"features/datasets/#multiple-items","text":"items := []map[string]any{ { \"input\": \"What is 2+2?\", \"expected\": \"4\", \"category\": \"math\", }, { \"input\": \"What is 3+3?\", \"expected\": \"6\", \"category\": \"math\", }, { \"input\": \"What color is the sky?\", \"expected\": \"Blue\", \"category\": \"general\", }, } dataset.InsertItems(ctx, items)","title":"Multiple Items"},{"location":"features/datasets/#retrieving-items","text":"// Get items with pagination items, _ := dataset.GetItems(ctx, 1, 100) // page 1, 100 items per page for _, item := range items { fmt.Printf(\"Input: %v\\n\", item.Data[\"input\"]) fmt.Printf(\"Expected: %v\\n\", item.Data[\"expected\"]) }","title":"Retrieving Items"},{"location":"features/datasets/#listing-datasets","text":"// List all datasets datasets, _ := client.ListDatasets(ctx, 1, 100) for _, ds := range datasets { fmt.Printf(\"Dataset: %s (ID: %s)\\n\", ds.Name, ds.ID) }","title":"Listing Datasets"},{"location":"features/datasets/#getting-a-dataset-by-name","text":"dataset, _ := client.GetDatasetByName(ctx, \"my-dataset\") fmt.Printf(\"Name: %s\\n\", dataset.Name) fmt.Printf(\"Description: %s\\n\", dataset.Description)","title":"Getting a Dataset by Name"},{"location":"features/datasets/#deleting-datasets","text":"// Delete by dataset object dataset.Delete(ctx)","title":"Deleting Datasets"},{"location":"features/datasets/#dataset-item-structure","text":"Each dataset item can contain any fields you need: Common Field Description input The input/prompt to evaluate expected Expected/ground truth output context Additional context for evaluation metadata Any additional metadata","title":"Dataset Item Structure"},{"location":"features/datasets/#example-building-an-evaluation-dataset","text":"func buildEvaluationDataset(ctx context.Context, client *opik.Client) (*opik.Dataset, error) { // Create dataset dataset, err := client.CreateDataset(ctx, \"qa-evaluation-v1\", opik.WithDatasetDescription(\"Q&A evaluation dataset\"), opik.WithDatasetTags(\"qa\", \"production\"), ) if err != nil { return nil, err } // Load test cases from file or database testCases := loadTestCases() // Convert to dataset items items := make([]map[string]any, len(testCases)) for i, tc := range testCases { items[i] = map[string]any{ \"input\": tc.Question, \"expected\": tc.Answer, \"context\": tc.Context, \"category\": tc.Category, } } // Bulk insert err = dataset.InsertItems(ctx, items) if err != nil { return nil, err } return dataset, nil }","title":"Example: Building an Evaluation Dataset"},{"location":"features/datasets/#using-datasets-with-experiments","text":"See Experiments for how to run evaluations against datasets.","title":"Using Datasets with Experiments"},{"location":"features/experiments/","text":"Experiments \u00b6 Experiments track evaluation runs against datasets, allowing you to compare different models, prompts, or configurations. Creating Experiments \u00b6 // Create an experiment for a dataset experiment, _ := client.CreateExperiment(ctx, \"my-dataset\", opik.WithExperimentName(\"gpt-4-evaluation-v1\"), opik.WithExperimentMetadata(map[string]any{ \"model\": \"gpt-4\", \"temperature\": 0.7, \"prompt_version\": \"v2\", }), ) Logging Experiment Items \u00b6 For each dataset item you evaluate, log the result: // Log an experiment item experiment.LogItem(ctx, datasetItemID, traceID, opik.WithExperimentItemInput(map[string]any{\"question\": \"What is 2+2?\"}), opik.WithExperimentItemOutput(map[string]any{\"answer\": \"4\"}), ) Complete Evaluation Workflow \u00b6 func runExperiment(ctx context.Context, client *opik.Client, datasetName string) error { // Get dataset dataset, err := client.GetDatasetByName(ctx, datasetName) if err != nil { return err } // Create experiment experiment, err := client.CreateExperiment(ctx, datasetName, opik.WithExperimentName(fmt.Sprintf(\"eval-%s\", time.Now().Format(\"20060102-150405\"))), opik.WithExperimentMetadata(map[string]any{ \"model\": \"gpt-4\", }), ) if err != nil { return err } // Get dataset items items, err := dataset.GetItems(ctx, 1, 1000) if err != nil { return err } // Evaluate each item for _, item := range items { // Create trace for this evaluation trace, _ := client.Trace(ctx, \"evaluate-item\", opik.WithTraceInput(item.Data), ) // Run your LLM input := item.Data[\"input\"].(string) output, err := runLLM(ctx, input) // Log result experiment.LogItem(ctx, item.ID, trace.ID(), opik.WithExperimentItemInput(item.Data), opik.WithExperimentItemOutput(map[string]any{\"response\": output}), ) // Add evaluation scores if expected, ok := item.Data[\"expected\"].(string); ok { score := evaluateMatch(output, expected) trace.AddFeedbackScore(ctx, \"accuracy\", score, \"\") } trace.End(ctx) } // Mark experiment complete experiment.Complete(ctx) return nil } Experiment States \u00b6 // Mark as complete when done experiment.Complete(ctx) // Cancel if something went wrong experiment.Cancel(ctx) Listing Experiments \u00b6 // List experiments for a dataset experiments, _ := client.ListExperiments(ctx, datasetID, 1, 100) for _, exp := range experiments { fmt.Printf(\"Experiment: %s (ID: %s)\\n\", exp.Name, exp.ID) } Deleting Experiments \u00b6 experiment.Delete(ctx) Experiment Metadata \u00b6 Use metadata to track configuration and make experiments comparable: experiment, _ := client.CreateExperiment(ctx, \"my-dataset\", opik.WithExperimentName(\"comparison-test\"), opik.WithExperimentMetadata(map[string]any{ // Model configuration \"model\": \"gpt-4\", \"temperature\": 0.7, \"max_tokens\": 1000, // Prompt information \"prompt_version\": \"v2.1\", \"system_prompt\": \"You are a helpful assistant...\", // Environment \"environment\": \"staging\", \"run_by\": \"automated-pipeline\", }), ) Best Practices \u00b6 Name experiments descriptively : Include model, date, or version info Use consistent metadata : Define standard fields for comparison Link traces to items : Always associate traces with experiment items Add feedback scores : Include evaluation metrics for analysis Complete or cancel : Always finalize experiment state","title":"Experiments"},{"location":"features/experiments/#experiments","text":"Experiments track evaluation runs against datasets, allowing you to compare different models, prompts, or configurations.","title":"Experiments"},{"location":"features/experiments/#creating-experiments","text":"// Create an experiment for a dataset experiment, _ := client.CreateExperiment(ctx, \"my-dataset\", opik.WithExperimentName(\"gpt-4-evaluation-v1\"), opik.WithExperimentMetadata(map[string]any{ \"model\": \"gpt-4\", \"temperature\": 0.7, \"prompt_version\": \"v2\", }), )","title":"Creating Experiments"},{"location":"features/experiments/#logging-experiment-items","text":"For each dataset item you evaluate, log the result: // Log an experiment item experiment.LogItem(ctx, datasetItemID, traceID, opik.WithExperimentItemInput(map[string]any{\"question\": \"What is 2+2?\"}), opik.WithExperimentItemOutput(map[string]any{\"answer\": \"4\"}), )","title":"Logging Experiment Items"},{"location":"features/experiments/#complete-evaluation-workflow","text":"func runExperiment(ctx context.Context, client *opik.Client, datasetName string) error { // Get dataset dataset, err := client.GetDatasetByName(ctx, datasetName) if err != nil { return err } // Create experiment experiment, err := client.CreateExperiment(ctx, datasetName, opik.WithExperimentName(fmt.Sprintf(\"eval-%s\", time.Now().Format(\"20060102-150405\"))), opik.WithExperimentMetadata(map[string]any{ \"model\": \"gpt-4\", }), ) if err != nil { return err } // Get dataset items items, err := dataset.GetItems(ctx, 1, 1000) if err != nil { return err } // Evaluate each item for _, item := range items { // Create trace for this evaluation trace, _ := client.Trace(ctx, \"evaluate-item\", opik.WithTraceInput(item.Data), ) // Run your LLM input := item.Data[\"input\"].(string) output, err := runLLM(ctx, input) // Log result experiment.LogItem(ctx, item.ID, trace.ID(), opik.WithExperimentItemInput(item.Data), opik.WithExperimentItemOutput(map[string]any{\"response\": output}), ) // Add evaluation scores if expected, ok := item.Data[\"expected\"].(string); ok { score := evaluateMatch(output, expected) trace.AddFeedbackScore(ctx, \"accuracy\", score, \"\") } trace.End(ctx) } // Mark experiment complete experiment.Complete(ctx) return nil }","title":"Complete Evaluation Workflow"},{"location":"features/experiments/#experiment-states","text":"// Mark as complete when done experiment.Complete(ctx) // Cancel if something went wrong experiment.Cancel(ctx)","title":"Experiment States"},{"location":"features/experiments/#listing-experiments","text":"// List experiments for a dataset experiments, _ := client.ListExperiments(ctx, datasetID, 1, 100) for _, exp := range experiments { fmt.Printf(\"Experiment: %s (ID: %s)\\n\", exp.Name, exp.ID) }","title":"Listing Experiments"},{"location":"features/experiments/#deleting-experiments","text":"experiment.Delete(ctx)","title":"Deleting Experiments"},{"location":"features/experiments/#experiment-metadata","text":"Use metadata to track configuration and make experiments comparable: experiment, _ := client.CreateExperiment(ctx, \"my-dataset\", opik.WithExperimentName(\"comparison-test\"), opik.WithExperimentMetadata(map[string]any{ // Model configuration \"model\": \"gpt-4\", \"temperature\": 0.7, \"max_tokens\": 1000, // Prompt information \"prompt_version\": \"v2.1\", \"system_prompt\": \"You are a helpful assistant...\", // Environment \"environment\": \"staging\", \"run_by\": \"automated-pipeline\", }), )","title":"Experiment Metadata"},{"location":"features/experiments/#best-practices","text":"Name experiments descriptively : Include model, date, or version info Use consistent metadata : Define standard fields for comparison Link traces to items : Always associate traces with experiment items Add feedback scores : Include evaluation metrics for analysis Complete or cancel : Always finalize experiment state","title":"Best Practices"},{"location":"features/prompts/","text":"Prompts \u00b6 Manage version-controlled prompt templates with variable substitution. Creating Prompts \u00b6 // Create a prompt with template prompt, _ := client.CreatePrompt(ctx, \"greeting-prompt\", opik.WithPromptDescription(\"Greeting template for users\"), opik.WithPromptTemplate(\"Hello, {{name}}! Welcome to {{place}}.\"), opik.WithPromptTags(\"greeting\", \"template\"), ) Template Syntax \u00b6 Use {{variable}} syntax for placeholders: template := `You are a {{role}} assistant. User query: {{query}} Please provide a {{style}} response.` Getting Prompts \u00b6 // Get latest version by name version, _ := client.GetPromptByName(ctx, \"greeting-prompt\", \"\") // Get specific version (if you have the commit hash) version, _ := client.GetPromptByName(ctx, \"greeting-prompt\", \"abc123\") Rendering Templates \u00b6 // Render with variables rendered := version.Render(map[string]string{ \"name\": \"Alice\", \"place\": \"Wonderland\", }) // Result: \"Hello, Alice! Welcome to Wonderland.\" // Extract variables from template vars := version.ExtractVariables() // Result: [\"name\", \"place\"] Creating New Versions \u00b6 // Create a new version of an existing prompt newVersion, _ := prompt.CreateVersion(ctx, \"Hi, {{name}}! Great to see you!\", opik.WithVersionChangeDescription(\"Simplified greeting\"), ) Listing Versions \u00b6 // Get all versions of a prompt versions, _ := prompt.GetVersions(ctx, 1, 100) for _, v := range versions { fmt.Printf(\"Version: %s\\n\", v.Commit) fmt.Printf(\"Template: %s\\n\", v.Template) fmt.Printf(\"Created: %s\\n\", v.CreatedAt) } Listing All Prompts \u00b6 prompts, _ := client.ListPrompts(ctx, 1, 100) for _, p := range prompts { fmt.Printf(\"Prompt: %s\\n\", p.Name) fmt.Printf(\"Description: %s\\n\", p.Description) } Using Prompts in LLM Calls \u00b6 func generateResponse(ctx context.Context, client *opik.Client, query string) (string, error) { // Get the prompt template version, err := client.GetPromptByName(ctx, \"assistant-prompt\", \"\") if err != nil { return \"\", err } // Render with variables prompt := version.Render(map[string]string{ \"role\": \"helpful\", \"query\": query, \"style\": \"concise\", }) // Create trace trace, _ := client.Trace(ctx, \"generate-response\", opik.WithTraceInput(map[string]any{ \"query\": query, \"prompt_name\": \"assistant-prompt\", \"prompt_version\": version.Commit, }), ) defer trace.End(ctx) // Call LLM with rendered prompt response, err := llmClient.Complete(ctx, prompt) if err != nil { return \"\", err } trace.End(ctx, opik.WithTraceOutput(map[string]any{\"response\": response})) return response, nil } Prompt Versioning Best Practices \u00b6 Use descriptive names : Make prompts easy to find Add change descriptions : Document what changed in each version Tag prompts : Use tags for organization (e.g., \"production\", \"experimental\") Track versions in traces : Include prompt version in trace metadata Test before promoting : Use experiments to compare prompt versions","title":"Prompts"},{"location":"features/prompts/#prompts","text":"Manage version-controlled prompt templates with variable substitution.","title":"Prompts"},{"location":"features/prompts/#creating-prompts","text":"// Create a prompt with template prompt, _ := client.CreatePrompt(ctx, \"greeting-prompt\", opik.WithPromptDescription(\"Greeting template for users\"), opik.WithPromptTemplate(\"Hello, {{name}}! Welcome to {{place}}.\"), opik.WithPromptTags(\"greeting\", \"template\"), )","title":"Creating Prompts"},{"location":"features/prompts/#template-syntax","text":"Use {{variable}} syntax for placeholders: template := `You are a {{role}} assistant. User query: {{query}} Please provide a {{style}} response.`","title":"Template Syntax"},{"location":"features/prompts/#getting-prompts","text":"// Get latest version by name version, _ := client.GetPromptByName(ctx, \"greeting-prompt\", \"\") // Get specific version (if you have the commit hash) version, _ := client.GetPromptByName(ctx, \"greeting-prompt\", \"abc123\")","title":"Getting Prompts"},{"location":"features/prompts/#rendering-templates","text":"// Render with variables rendered := version.Render(map[string]string{ \"name\": \"Alice\", \"place\": \"Wonderland\", }) // Result: \"Hello, Alice! Welcome to Wonderland.\" // Extract variables from template vars := version.ExtractVariables() // Result: [\"name\", \"place\"]","title":"Rendering Templates"},{"location":"features/prompts/#creating-new-versions","text":"// Create a new version of an existing prompt newVersion, _ := prompt.CreateVersion(ctx, \"Hi, {{name}}! Great to see you!\", opik.WithVersionChangeDescription(\"Simplified greeting\"), )","title":"Creating New Versions"},{"location":"features/prompts/#listing-versions","text":"// Get all versions of a prompt versions, _ := prompt.GetVersions(ctx, 1, 100) for _, v := range versions { fmt.Printf(\"Version: %s\\n\", v.Commit) fmt.Printf(\"Template: %s\\n\", v.Template) fmt.Printf(\"Created: %s\\n\", v.CreatedAt) }","title":"Listing Versions"},{"location":"features/prompts/#listing-all-prompts","text":"prompts, _ := client.ListPrompts(ctx, 1, 100) for _, p := range prompts { fmt.Printf(\"Prompt: %s\\n\", p.Name) fmt.Printf(\"Description: %s\\n\", p.Description) }","title":"Listing All Prompts"},{"location":"features/prompts/#using-prompts-in-llm-calls","text":"func generateResponse(ctx context.Context, client *opik.Client, query string) (string, error) { // Get the prompt template version, err := client.GetPromptByName(ctx, \"assistant-prompt\", \"\") if err != nil { return \"\", err } // Render with variables prompt := version.Render(map[string]string{ \"role\": \"helpful\", \"query\": query, \"style\": \"concise\", }) // Create trace trace, _ := client.Trace(ctx, \"generate-response\", opik.WithTraceInput(map[string]any{ \"query\": query, \"prompt_name\": \"assistant-prompt\", \"prompt_version\": version.Commit, }), ) defer trace.End(ctx) // Call LLM with rendered prompt response, err := llmClient.Complete(ctx, prompt) if err != nil { return \"\", err } trace.End(ctx, opik.WithTraceOutput(map[string]any{\"response\": response})) return response, nil }","title":"Using Prompts in LLM Calls"},{"location":"features/prompts/#prompt-versioning-best-practices","text":"Use descriptive names : Make prompts easy to find Add change descriptions : Document what changed in each version Tag prompts : Use tags for organization (e.g., \"production\", \"experimental\") Track versions in traces : Include prompt version in trace metadata Test before promoting : Use experiments to compare prompt versions","title":"Prompt Versioning Best Practices"},{"location":"features/streaming/","text":"Streaming \u00b6 Track streaming LLM responses with automatic chunk accumulation. Starting a Streaming Span \u00b6 // Start a streaming span ctx, streamSpan, _ := opik.StartStreamingSpan(ctx, \"stream-response\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), ) Adding Chunks \u00b6 As chunks arrive from your LLM, add them to the span: for chunk := range streamChannel { // Add chunk content streamSpan.AddChunk(chunk.Content) } With Chunk Options \u00b6 // Add chunk with token count streamSpan.AddChunk(chunk.Content, opik.WithChunkTokenCount(chunk.TokenCount), ) // Mark final chunk with finish reason streamSpan.AddChunk(lastChunk.Content, opik.WithChunkTokenCount(lastChunk.TokenCount), opik.WithChunkFinishReason(\"stop\"), ) Ending the Streaming Span \u00b6 // End span - accumulated content is automatically captured streamSpan.End(ctx) Complete Example \u00b6 func streamCompletion(ctx context.Context, prompt string) (string, error) { // Start streaming span ctx, streamSpan, err := opik.StartStreamingSpan(ctx, \"openai-stream\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), opik.WithSpanProvider(\"openai\"), opik.WithSpanInput(map[string]any{\"prompt\": prompt}), ) if err != nil { return \"\", err } // Create streaming request to OpenAI stream, err := openaiClient.CreateChatCompletionStream(ctx, openai.ChatCompletionRequest{ Model: \"gpt-4\", Messages: []openai.ChatCompletionMessage{{Role: \"user\", Content: prompt}}, Stream: true, }) if err != nil { streamSpan.End(ctx) return \"\", err } defer stream.Close() // Collect chunks var fullResponse strings.Builder for { response, err := stream.Recv() if err == io.EOF { // Mark final chunk streamSpan.AddChunk(\"\", opik.WithChunkFinishReason(\"stop\")) break } if err != nil { streamSpan.End(ctx) return \"\", err } content := response.Choices[0].Delta.Content fullResponse.WriteString(content) // Add chunk to span streamSpan.AddChunk(content) // Optionally output to user in real-time fmt.Print(content) } // End span with accumulated output streamSpan.End(ctx) return fullResponse.String(), nil } Chunk Options \u00b6 Option Description WithChunkTokenCount(n) Number of tokens in this chunk WithChunkFinishReason(reason) Finish reason (e.g., \"stop\", \"length\") Stream Accumulator \u00b6 The streaming span automatically accumulates: Content : All chunk content concatenated Token count : Sum of all chunk token counts Duration : Time from first chunk to last Finish reason : From the final chunk Accessing Accumulated Data \u00b6 // Get the accumulated content before ending accumulated := streamSpan.AccumulatedContent() // Get chunk count count := streamSpan.ChunkCount() // Get total tokens tokens := streamSpan.TotalTokens() Best Practices \u00b6 Start span before streaming : Create the span before initiating the stream Add chunks as they arrive : Don't buffer - add immediately for accurate timing Include token counts : If available, include for cost tracking Mark finish reason : Always mark the final chunk with a finish reason Handle errors : End the span even if streaming fails","title":"Streaming"},{"location":"features/streaming/#streaming","text":"Track streaming LLM responses with automatic chunk accumulation.","title":"Streaming"},{"location":"features/streaming/#starting-a-streaming-span","text":"// Start a streaming span ctx, streamSpan, _ := opik.StartStreamingSpan(ctx, \"stream-response\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), )","title":"Starting a Streaming Span"},{"location":"features/streaming/#adding-chunks","text":"As chunks arrive from your LLM, add them to the span: for chunk := range streamChannel { // Add chunk content streamSpan.AddChunk(chunk.Content) }","title":"Adding Chunks"},{"location":"features/streaming/#with-chunk-options","text":"// Add chunk with token count streamSpan.AddChunk(chunk.Content, opik.WithChunkTokenCount(chunk.TokenCount), ) // Mark final chunk with finish reason streamSpan.AddChunk(lastChunk.Content, opik.WithChunkTokenCount(lastChunk.TokenCount), opik.WithChunkFinishReason(\"stop\"), )","title":"With Chunk Options"},{"location":"features/streaming/#ending-the-streaming-span","text":"// End span - accumulated content is automatically captured streamSpan.End(ctx)","title":"Ending the Streaming Span"},{"location":"features/streaming/#complete-example","text":"func streamCompletion(ctx context.Context, prompt string) (string, error) { // Start streaming span ctx, streamSpan, err := opik.StartStreamingSpan(ctx, \"openai-stream\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gpt-4\"), opik.WithSpanProvider(\"openai\"), opik.WithSpanInput(map[string]any{\"prompt\": prompt}), ) if err != nil { return \"\", err } // Create streaming request to OpenAI stream, err := openaiClient.CreateChatCompletionStream(ctx, openai.ChatCompletionRequest{ Model: \"gpt-4\", Messages: []openai.ChatCompletionMessage{{Role: \"user\", Content: prompt}}, Stream: true, }) if err != nil { streamSpan.End(ctx) return \"\", err } defer stream.Close() // Collect chunks var fullResponse strings.Builder for { response, err := stream.Recv() if err == io.EOF { // Mark final chunk streamSpan.AddChunk(\"\", opik.WithChunkFinishReason(\"stop\")) break } if err != nil { streamSpan.End(ctx) return \"\", err } content := response.Choices[0].Delta.Content fullResponse.WriteString(content) // Add chunk to span streamSpan.AddChunk(content) // Optionally output to user in real-time fmt.Print(content) } // End span with accumulated output streamSpan.End(ctx) return fullResponse.String(), nil }","title":"Complete Example"},{"location":"features/streaming/#chunk-options","text":"Option Description WithChunkTokenCount(n) Number of tokens in this chunk WithChunkFinishReason(reason) Finish reason (e.g., \"stop\", \"length\")","title":"Chunk Options"},{"location":"features/streaming/#stream-accumulator","text":"The streaming span automatically accumulates: Content : All chunk content concatenated Token count : Sum of all chunk token counts Duration : Time from first chunk to last Finish reason : From the final chunk","title":"Stream Accumulator"},{"location":"features/streaming/#accessing-accumulated-data","text":"// Get the accumulated content before ending accumulated := streamSpan.AccumulatedContent() // Get chunk count count := streamSpan.ChunkCount() // Get total tokens tokens := streamSpan.TotalTokens()","title":"Accessing Accumulated Data"},{"location":"features/streaming/#best-practices","text":"Start span before streaming : Create the span before initiating the stream Add chunks as they arrive : Don't buffer - add immediately for accurate timing Include token counts : If available, include for cost tracking Mark finish reason : Always mark the final chunk with a finish reason Handle errors : End the span even if streaming fails","title":"Best Practices"},{"location":"getting-started/configuration/","text":"Configuration \u00b6 The SDK supports multiple configuration methods. They are applied in this order (later overrides earlier): Config file ( ~/.opik.config ) Environment variables Programmatic options Environment Variables \u00b6 Variable Description OPIK_URL_OVERRIDE API endpoint URL OPIK_API_KEY API key for Opik Cloud OPIK_WORKSPACE Workspace name for Opik Cloud OPIK_PROJECT_NAME Default project name OPIK_TRACK_DISABLE Set to true to disable tracing Example \u00b6 export OPIK_API_KEY=\"your-api-key\" export OPIK_WORKSPACE=\"your-workspace\" export OPIK_PROJECT_NAME=\"my-project\" Config File \u00b6 Create ~/.opik.config with INI format: [opik] url_override = https://www.comet.com/opik/api api_key = your-api-key workspace = your-workspace project_name = My Project Programmatic Configuration \u00b6 Override any configuration using functional options: client, err := opik.NewClient( opik.WithURL(\"https://www.comet.com/opik/api\"), opik.WithAPIKey(\"your-api-key\"), opik.WithWorkspace(\"your-workspace\"), opik.WithProjectName(\"My Project\"), ) Available Options \u00b6 Option Description WithURL(url) Set the API endpoint URL WithAPIKey(key) Set the API key WithWorkspace(name) Set the workspace name WithProjectName(name) Set the default project name WithHTTPClient(client) Use a custom HTTP client Configure via CLI \u00b6 Use the CLI to save configuration: opik configure -api-key=your-key -workspace=your-workspace This saves to ~/.opik.config . Disable Tracing \u00b6 To disable tracing (useful in tests or local development): export OPIK_TRACK_DISABLE=true Or programmatically: client := opik.RecordTracesLocally(\"my-project\") This records traces in memory without sending to the server. Load and Check Configuration \u00b6 // Load current configuration cfg := opik.LoadConfig() fmt.Printf(\"URL: %s\\n\", cfg.URL) fmt.Printf(\"Workspace: %s\\n\", cfg.Workspace) fmt.Printf(\"Project: %s\\n\", cfg.ProjectName)","title":"Configuration"},{"location":"getting-started/configuration/#configuration","text":"The SDK supports multiple configuration methods. They are applied in this order (later overrides earlier): Config file ( ~/.opik.config ) Environment variables Programmatic options","title":"Configuration"},{"location":"getting-started/configuration/#environment-variables","text":"Variable Description OPIK_URL_OVERRIDE API endpoint URL OPIK_API_KEY API key for Opik Cloud OPIK_WORKSPACE Workspace name for Opik Cloud OPIK_PROJECT_NAME Default project name OPIK_TRACK_DISABLE Set to true to disable tracing","title":"Environment Variables"},{"location":"getting-started/configuration/#example","text":"export OPIK_API_KEY=\"your-api-key\" export OPIK_WORKSPACE=\"your-workspace\" export OPIK_PROJECT_NAME=\"my-project\"","title":"Example"},{"location":"getting-started/configuration/#config-file","text":"Create ~/.opik.config with INI format: [opik] url_override = https://www.comet.com/opik/api api_key = your-api-key workspace = your-workspace project_name = My Project","title":"Config File"},{"location":"getting-started/configuration/#programmatic-configuration","text":"Override any configuration using functional options: client, err := opik.NewClient( opik.WithURL(\"https://www.comet.com/opik/api\"), opik.WithAPIKey(\"your-api-key\"), opik.WithWorkspace(\"your-workspace\"), opik.WithProjectName(\"My Project\"), )","title":"Programmatic Configuration"},{"location":"getting-started/configuration/#available-options","text":"Option Description WithURL(url) Set the API endpoint URL WithAPIKey(key) Set the API key WithWorkspace(name) Set the workspace name WithProjectName(name) Set the default project name WithHTTPClient(client) Use a custom HTTP client","title":"Available Options"},{"location":"getting-started/configuration/#configure-via-cli","text":"Use the CLI to save configuration: opik configure -api-key=your-key -workspace=your-workspace This saves to ~/.opik.config .","title":"Configure via CLI"},{"location":"getting-started/configuration/#disable-tracing","text":"To disable tracing (useful in tests or local development): export OPIK_TRACK_DISABLE=true Or programmatically: client := opik.RecordTracesLocally(\"my-project\") This records traces in memory without sending to the server.","title":"Disable Tracing"},{"location":"getting-started/configuration/#load-and-check-configuration","text":"// Load current configuration cfg := opik.LoadConfig() fmt.Printf(\"URL: %s\\n\", cfg.URL) fmt.Printf(\"Workspace: %s\\n\", cfg.Workspace) fmt.Printf(\"Project: %s\\n\", cfg.ProjectName)","title":"Load and Check Configuration"},{"location":"getting-started/development/","text":"Development Guide \u00b6 This guide covers development setup, code generation, and contribution guidelines for the Go Opik SDK. Prerequisites \u00b6 Before contributing to the SDK, ensure you have the following installed: Go 1.21+ - Download Go golangci-lint - For linting: go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest ogen - For API client generation: go install github.com/ogen-go/ogen/cmd/ogen@latest Project Structure \u00b6 go-comet-ml-opik/ \u251c\u2500\u2500 internal/api/ # ogen-generated API client (DO NOT EDIT) \u251c\u2500\u2500 integrations/ # LLM provider integrations \u2502 \u251c\u2500\u2500 openai/ \u2502 \u251c\u2500\u2500 anthropic/ \u2502 \u2514\u2500\u2500 gollm/ \u251c\u2500\u2500 evaluation/ # Evaluation framework \u2502 \u251c\u2500\u2500 heuristic/ \u2502 \u2514\u2500\u2500 llm/ \u251c\u2500\u2500 middleware/ # HTTP tracing middleware \u251c\u2500\u2500 testutil/ # Test utilities \u251c\u2500\u2500 cmd/opik/ # CLI tool \u251c\u2500\u2500 examples/ # Usage examples \u251c\u2500\u2500 openapi/ # OpenAPI specification \u2502 \u2514\u2500\u2500 openapi.yaml \u2514\u2500\u2500 docsrc/ # Documentation source Regenerating the API Client \u00b6 The SDK uses ogen to generate a type-safe API client from the Opik OpenAPI specification. When the upstream API changes, you'll need to regenerate the client code. Step 1: Update the OpenAPI Specification \u00b6 The OpenAPI spec is located at openapi/openapi.yaml . To update it: # Option A: Copy from your local Opik clone cp /path/to/opik/sdks/code_generation/fern/openapi/openapi.yaml openapi/ # Option B: Download from the Opik repository curl -o openapi/openapi.yaml \\ https://raw.githubusercontent.com/comet-ml/opik/main/sdks/code_generation/fern/openapi/openapi.yaml Step 2: Install ogen (if not already installed) \u00b6 go install github.com/ogen-go/ogen/cmd/ogen@latest Verify installation: ogen --version Step 3: Run the Generation Script \u00b6 ./generate.sh This script: Runs ogen to generate Go code from the OpenAPI spec Applies fixes for known issues (e.g., jx.Raw comparison) Runs go mod tidy to update dependencies Verifies the build compiles Manual Generation \u00b6 If you need to run the generation manually: # Generate the API client ogen --package api --target internal/api --clean openapi/openapi.yaml # Update dependencies go mod tidy # Verify build go build ./... Known Issues and Fixes \u00b6 The generated code may have issues that require post-processing: jx.Raw Comparison \u00b6 The ogen generator creates equality functions that compare jx.Raw values directly, which doesn't work in Go. The generate.sh script automatically fixes this by replacing direct comparisons with bytes.Equal() . Before (generated): if a.Input != b.Input { return false } After (fixed): if !bytes.Equal([]byte(a.Input), []byte(b.Input)) { return false } Troubleshooting \u00b6 \"ogen: command not found\" \u00b6 Ensure $GOPATH/bin is in your PATH: export PATH=$PATH:$(go env GOPATH)/bin Build Errors After Generation \u00b6 If you encounter build errors after regenerating: Check that the OpenAPI spec is valid Ensure you're using a compatible ogen version Review the generated code for any new fields that need handling Run go mod tidy to resolve dependency issues API Changes Breaking Existing Code \u00b6 When the API changes, you may need to update the SDK wrapper code (files outside internal/api/ ): Check for new required fields in request/response types Update any OptXxx type handling for new optional fields Add support for new endpoints in the appropriate files Running Tests \u00b6 # Run all tests go test ./... # Run tests with race detection go test -race ./... # Run tests with coverage go test -coverprofile=coverage.out ./... go tool cover -html=coverage.out Running Linter \u00b6 # Run all linters golangci-lint run # Run with auto-fix where possible golangci-lint run --fix Building the CLI \u00b6 # Build for current platform make build-cli # Or manually go build -o bin/opik ./cmd/opik Building Documentation \u00b6 # Generate presentation HTML make presentation # Build MkDocs site make docs # Serve locally for development make docs-serve Code Style Guidelines \u00b6 Follow standard Go conventions and idioms Use the functional options pattern for configuration Handle all errors explicitly (checked by errcheck linter) Add //nolint directives only with explanatory comments Keep the internal/api/ directory untouched (auto-generated) Submitting Changes \u00b6 Fork the repository Create a feature branch Make your changes Run tests and linter: go test ./... && golangci-lint run Submit a pull request Getting Help \u00b6 GitHub Issues Opik Documentation ogen Documentation","title":"Development"},{"location":"getting-started/development/#development-guide","text":"This guide covers development setup, code generation, and contribution guidelines for the Go Opik SDK.","title":"Development Guide"},{"location":"getting-started/development/#prerequisites","text":"Before contributing to the SDK, ensure you have the following installed: Go 1.21+ - Download Go golangci-lint - For linting: go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest ogen - For API client generation: go install github.com/ogen-go/ogen/cmd/ogen@latest","title":"Prerequisites"},{"location":"getting-started/development/#project-structure","text":"go-comet-ml-opik/ \u251c\u2500\u2500 internal/api/ # ogen-generated API client (DO NOT EDIT) \u251c\u2500\u2500 integrations/ # LLM provider integrations \u2502 \u251c\u2500\u2500 openai/ \u2502 \u251c\u2500\u2500 anthropic/ \u2502 \u2514\u2500\u2500 gollm/ \u251c\u2500\u2500 evaluation/ # Evaluation framework \u2502 \u251c\u2500\u2500 heuristic/ \u2502 \u2514\u2500\u2500 llm/ \u251c\u2500\u2500 middleware/ # HTTP tracing middleware \u251c\u2500\u2500 testutil/ # Test utilities \u251c\u2500\u2500 cmd/opik/ # CLI tool \u251c\u2500\u2500 examples/ # Usage examples \u251c\u2500\u2500 openapi/ # OpenAPI specification \u2502 \u2514\u2500\u2500 openapi.yaml \u2514\u2500\u2500 docsrc/ # Documentation source","title":"Project Structure"},{"location":"getting-started/development/#regenerating-the-api-client","text":"The SDK uses ogen to generate a type-safe API client from the Opik OpenAPI specification. When the upstream API changes, you'll need to regenerate the client code.","title":"Regenerating the API Client"},{"location":"getting-started/development/#step-1-update-the-openapi-specification","text":"The OpenAPI spec is located at openapi/openapi.yaml . To update it: # Option A: Copy from your local Opik clone cp /path/to/opik/sdks/code_generation/fern/openapi/openapi.yaml openapi/ # Option B: Download from the Opik repository curl -o openapi/openapi.yaml \\ https://raw.githubusercontent.com/comet-ml/opik/main/sdks/code_generation/fern/openapi/openapi.yaml","title":"Step 1: Update the OpenAPI Specification"},{"location":"getting-started/development/#step-2-install-ogen-if-not-already-installed","text":"go install github.com/ogen-go/ogen/cmd/ogen@latest Verify installation: ogen --version","title":"Step 2: Install ogen (if not already installed)"},{"location":"getting-started/development/#step-3-run-the-generation-script","text":"./generate.sh This script: Runs ogen to generate Go code from the OpenAPI spec Applies fixes for known issues (e.g., jx.Raw comparison) Runs go mod tidy to update dependencies Verifies the build compiles","title":"Step 3: Run the Generation Script"},{"location":"getting-started/development/#manual-generation","text":"If you need to run the generation manually: # Generate the API client ogen --package api --target internal/api --clean openapi/openapi.yaml # Update dependencies go mod tidy # Verify build go build ./...","title":"Manual Generation"},{"location":"getting-started/development/#known-issues-and-fixes","text":"The generated code may have issues that require post-processing:","title":"Known Issues and Fixes"},{"location":"getting-started/development/#jxraw-comparison","text":"The ogen generator creates equality functions that compare jx.Raw values directly, which doesn't work in Go. The generate.sh script automatically fixes this by replacing direct comparisons with bytes.Equal() . Before (generated): if a.Input != b.Input { return false } After (fixed): if !bytes.Equal([]byte(a.Input), []byte(b.Input)) { return false }","title":"jx.Raw Comparison"},{"location":"getting-started/development/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/development/#ogen-command-not-found","text":"Ensure $GOPATH/bin is in your PATH: export PATH=$PATH:$(go env GOPATH)/bin","title":"\"ogen: command not found\""},{"location":"getting-started/development/#build-errors-after-generation","text":"If you encounter build errors after regenerating: Check that the OpenAPI spec is valid Ensure you're using a compatible ogen version Review the generated code for any new fields that need handling Run go mod tidy to resolve dependency issues","title":"Build Errors After Generation"},{"location":"getting-started/development/#api-changes-breaking-existing-code","text":"When the API changes, you may need to update the SDK wrapper code (files outside internal/api/ ): Check for new required fields in request/response types Update any OptXxx type handling for new optional fields Add support for new endpoints in the appropriate files","title":"API Changes Breaking Existing Code"},{"location":"getting-started/development/#running-tests","text":"# Run all tests go test ./... # Run tests with race detection go test -race ./... # Run tests with coverage go test -coverprofile=coverage.out ./... go tool cover -html=coverage.out","title":"Running Tests"},{"location":"getting-started/development/#running-linter","text":"# Run all linters golangci-lint run # Run with auto-fix where possible golangci-lint run --fix","title":"Running Linter"},{"location":"getting-started/development/#building-the-cli","text":"# Build for current platform make build-cli # Or manually go build -o bin/opik ./cmd/opik","title":"Building the CLI"},{"location":"getting-started/development/#building-documentation","text":"# Generate presentation HTML make presentation # Build MkDocs site make docs # Serve locally for development make docs-serve","title":"Building Documentation"},{"location":"getting-started/development/#code-style-guidelines","text":"Follow standard Go conventions and idioms Use the functional options pattern for configuration Handle all errors explicitly (checked by errcheck linter) Add //nolint directives only with explanatory comments Keep the internal/api/ directory untouched (auto-generated)","title":"Code Style Guidelines"},{"location":"getting-started/development/#submitting-changes","text":"Fork the repository Create a feature branch Make your changes Run tests and linter: go test ./... && golangci-lint run Submit a pull request","title":"Submitting Changes"},{"location":"getting-started/development/#getting-help","text":"GitHub Issues Opik Documentation ogen Documentation","title":"Getting Help"},{"location":"getting-started/installation/","text":"Installation \u00b6 Requirements \u00b6 Go 1.21 or later Install the SDK \u00b6 go get github.com/agentplexus/go-comet-ml-opik Install the CLI (Optional) \u00b6 go install github.com/agentplexus/go-comet-ml-opik/cmd/opik@latest Verify Installation \u00b6 package main import ( \"fmt\" opik \"github.com/agentplexus/go-comet-ml-opik\" ) func main() { client, err := opik.NewClient() if err != nil { fmt.Printf(\"Error: %v\\n\", err) return } fmt.Println(\"Opik client created successfully!\") _ = client } Opik Server Options \u00b6 Opik Cloud (Recommended) \u00b6 Use the hosted Opik Cloud service: Sign up at comet.com Get your API key and workspace name Set environment variables: export OPIK_API_KEY=\"your-api-key\" export OPIK_WORKSPACE=\"your-workspace\" Self-Hosted \u00b6 Run Opik locally using Docker: # Clone the Opik repository git clone https://github.com/comet-ml/opik.git cd opik # Start with Docker Compose docker-compose up -d The local server runs at http://localhost:5173 by default. export OPIK_URL_OVERRIDE=\"http://localhost:5173/api\" Next Steps \u00b6 Configuration - Configure the SDK for your environment Testing - Run the test suite (no API key required) Traces and Spans - Start tracing your LLM calls","title":"Installation"},{"location":"getting-started/installation/#installation","text":"","title":"Installation"},{"location":"getting-started/installation/#requirements","text":"Go 1.21 or later","title":"Requirements"},{"location":"getting-started/installation/#install-the-sdk","text":"go get github.com/agentplexus/go-comet-ml-opik","title":"Install the SDK"},{"location":"getting-started/installation/#install-the-cli-optional","text":"go install github.com/agentplexus/go-comet-ml-opik/cmd/opik@latest","title":"Install the CLI (Optional)"},{"location":"getting-started/installation/#verify-installation","text":"package main import ( \"fmt\" opik \"github.com/agentplexus/go-comet-ml-opik\" ) func main() { client, err := opik.NewClient() if err != nil { fmt.Printf(\"Error: %v\\n\", err) return } fmt.Println(\"Opik client created successfully!\") _ = client }","title":"Verify Installation"},{"location":"getting-started/installation/#opik-server-options","text":"","title":"Opik Server Options"},{"location":"getting-started/installation/#opik-cloud-recommended","text":"Use the hosted Opik Cloud service: Sign up at comet.com Get your API key and workspace name Set environment variables: export OPIK_API_KEY=\"your-api-key\" export OPIK_WORKSPACE=\"your-workspace\"","title":"Opik Cloud (Recommended)"},{"location":"getting-started/installation/#self-hosted","text":"Run Opik locally using Docker: # Clone the Opik repository git clone https://github.com/comet-ml/opik.git cd opik # Start with Docker Compose docker-compose up -d The local server runs at http://localhost:5173 by default. export OPIK_URL_OVERRIDE=\"http://localhost:5173/api\"","title":"Self-Hosted"},{"location":"getting-started/installation/#next-steps","text":"Configuration - Configure the SDK for your environment Testing - Run the test suite (no API key required) Traces and Spans - Start tracing your LLM calls","title":"Next Steps"},{"location":"getting-started/testing/","text":"Testing \u00b6 The Go Opik SDK includes a comprehensive test suite to ensure reliability and correctness. Running Tests \u00b6 Quick Start \u00b6 Run all tests without any API key or external dependencies: go test ./... Verbose Output \u00b6 See detailed test output: go test ./... -v Run Specific Test Packages \u00b6 # Core SDK tests (config, context) go test github.com/agentplexus/go-comet-ml-opik -v # Evaluation heuristic metrics go test github.com/agentplexus/go-comet-ml-opik/evaluation/heuristic -v API Key Requirements \u00b6 No API Key Required for Unit Tests All unit tests run locally without requiring an Opik API key or server connection. The test suite is designed to test the SDK's logic, data structures, and algorithms without making external API calls. What Tests Don't Require API Keys \u00b6 Test Category Description Config Tests Configuration loading from environment variables and files Context Tests Context propagation for traces and spans String Metrics Equals, Contains, StartsWith, EndsWith, etc. Similarity Metrics Levenshtein, Jaccard, Cosine, BLEU, ROUGE, etc. When You Need an API Key \u00b6 API keys are only required for: Integration tests (if added) that verify end-to-end functionality with an Opik server Running the CLI to interact with a live Opik instance Production usage of the SDK to send traces to Opik Cloud Test Coverage \u00b6 Core SDK ( config_test.go , context_test.go ) \u00b6 Tests for configuration management and context propagation: go test github.com/agentplexus/go-comet-ml-opik -v -run TestConfig go test github.com/agentplexus/go-comet-ml-opik -v -run TestContext Config tests cover: TestNewConfig - Default configuration values TestLoadConfigFromEnv - Environment variable loading TestLoadConfigTracingDisableVariants - Tracing disable flag variants TestLoadConfigDefaultURL - URL defaults based on API key presence TestConfigIsCloud - Cloud vs local detection TestConfigValidate - Configuration validation TestSaveConfig - Configuration file saving Context tests cover: TestContextWithTrace / TestTraceFromContext - Trace context storage TestContextWithSpan / TestSpanFromContext - Span context storage TestContextWithClient / TestClientFromContext - Client context storage TestContextChaining - Multiple values in context TestCurrentTraceID / TestCurrentSpanID - ID retrieval from context TestStartSpanNoActiveTrace - Error handling Heuristic Metrics ( evaluation/heuristic/ ) \u00b6 Tests for string and similarity metrics: go test github.com/agentplexus/go-comet-ml-opik/evaluation/heuristic -v String metric tests ( string_test.go ): TestEquals / TestEqualsIgnoreCase - Exact string matching TestContains / TestContainsIgnoreCase - Substring matching TestStartsWith / TestEndsWith - Prefix/suffix matching TestContainsAny / TestContainsAll - Multiple value matching TestNotEmpty - Empty string detection TestLengthBetween / TestWordCount - Length constraints TestNoOffensiveLanguage - Content filtering TestMetricName - Metric naming consistency Similarity metric tests ( similarity_test.go ): TestLevenshteinSimilarity - Edit distance similarity TestLevenshteinDistance - Raw edit distance calculation TestJaccardSimilarity - Set-based similarity TestCosineSimilarity - Vector-based similarity TestBLEU - Machine translation metric TestROUGE - Summarization metric TestFuzzyMatch - Combined fuzzy matching TestSemanticSimilarity - Word-based semantic similarity TestLCSLength - Longest common subsequence Writing New Tests \u00b6 Test File Naming \u00b6 Follow Go conventions: foo.go \u2192 foo_test.go Package remains the same (not _test suffix for internal tests) Test Structure Example \u00b6 package opik import ( \"context\" \"testing\" ) func TestMyFunction(t *testing.T) { ctx := context.Background() tests := []struct { name string input string expected string }{ {\"basic case\", \"input\", \"expected\"}, {\"edge case\", \"\", \"\"}, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { result := MyFunction(ctx, tt.input) if result != tt.expected { t.Errorf(\"MyFunction() = %v, want %v\", result, tt.expected) } }) } } Testing Metrics \u00b6 Use the evaluation.MetricInput type: func TestMyMetric(t *testing.T) { ctx := context.Background() metric := NewMyMetric() input := evaluation.NewMetricInput(\"\", \"output text\"). WithExpected(\"expected text\") result := metric.Score(ctx, input) if result.Value < 0.9 { t.Errorf(\"Score = %v, want >= 0.9\", result.Value) } } Continuous Integration \u00b6 GitHub Actions Example \u00b6 name: Tests on: [push, pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-go@v5 with: go-version: '1.21' - name: Run Tests run: go test ./... -v -race - name: Run Tests with Coverage run: go test ./... -coverprofile=coverage.out - name: Upload Coverage uses: codecov/codecov-action@v3 with: file: coverage.out Test Coverage Report \u00b6 Generate a coverage report: # Generate coverage profile go test ./... -coverprofile=coverage.out # View coverage summary go tool cover -func=coverage.out # Generate HTML report go tool cover -html=coverage.out -o coverage.html Troubleshooting \u00b6 Tests Fail Due to Missing Dependencies \u00b6 go mod download go mod tidy Timeout Issues \u00b6 Increase test timeout for slow machines: go test ./... -timeout 5m Race Condition Detection \u00b6 Run tests with race detector: go test ./... -race","title":"Testing"},{"location":"getting-started/testing/#testing","text":"The Go Opik SDK includes a comprehensive test suite to ensure reliability and correctness.","title":"Testing"},{"location":"getting-started/testing/#running-tests","text":"","title":"Running Tests"},{"location":"getting-started/testing/#quick-start","text":"Run all tests without any API key or external dependencies: go test ./...","title":"Quick Start"},{"location":"getting-started/testing/#verbose-output","text":"See detailed test output: go test ./... -v","title":"Verbose Output"},{"location":"getting-started/testing/#run-specific-test-packages","text":"# Core SDK tests (config, context) go test github.com/agentplexus/go-comet-ml-opik -v # Evaluation heuristic metrics go test github.com/agentplexus/go-comet-ml-opik/evaluation/heuristic -v","title":"Run Specific Test Packages"},{"location":"getting-started/testing/#api-key-requirements","text":"No API Key Required for Unit Tests All unit tests run locally without requiring an Opik API key or server connection. The test suite is designed to test the SDK's logic, data structures, and algorithms without making external API calls.","title":"API Key Requirements"},{"location":"getting-started/testing/#what-tests-dont-require-api-keys","text":"Test Category Description Config Tests Configuration loading from environment variables and files Context Tests Context propagation for traces and spans String Metrics Equals, Contains, StartsWith, EndsWith, etc. Similarity Metrics Levenshtein, Jaccard, Cosine, BLEU, ROUGE, etc.","title":"What Tests Don't Require API Keys"},{"location":"getting-started/testing/#when-you-need-an-api-key","text":"API keys are only required for: Integration tests (if added) that verify end-to-end functionality with an Opik server Running the CLI to interact with a live Opik instance Production usage of the SDK to send traces to Opik Cloud","title":"When You Need an API Key"},{"location":"getting-started/testing/#test-coverage","text":"","title":"Test Coverage"},{"location":"getting-started/testing/#core-sdk-config_testgo-context_testgo","text":"Tests for configuration management and context propagation: go test github.com/agentplexus/go-comet-ml-opik -v -run TestConfig go test github.com/agentplexus/go-comet-ml-opik -v -run TestContext Config tests cover: TestNewConfig - Default configuration values TestLoadConfigFromEnv - Environment variable loading TestLoadConfigTracingDisableVariants - Tracing disable flag variants TestLoadConfigDefaultURL - URL defaults based on API key presence TestConfigIsCloud - Cloud vs local detection TestConfigValidate - Configuration validation TestSaveConfig - Configuration file saving Context tests cover: TestContextWithTrace / TestTraceFromContext - Trace context storage TestContextWithSpan / TestSpanFromContext - Span context storage TestContextWithClient / TestClientFromContext - Client context storage TestContextChaining - Multiple values in context TestCurrentTraceID / TestCurrentSpanID - ID retrieval from context TestStartSpanNoActiveTrace - Error handling","title":"Core SDK (config_test.go, context_test.go)"},{"location":"getting-started/testing/#heuristic-metrics-evaluationheuristic","text":"Tests for string and similarity metrics: go test github.com/agentplexus/go-comet-ml-opik/evaluation/heuristic -v String metric tests ( string_test.go ): TestEquals / TestEqualsIgnoreCase - Exact string matching TestContains / TestContainsIgnoreCase - Substring matching TestStartsWith / TestEndsWith - Prefix/suffix matching TestContainsAny / TestContainsAll - Multiple value matching TestNotEmpty - Empty string detection TestLengthBetween / TestWordCount - Length constraints TestNoOffensiveLanguage - Content filtering TestMetricName - Metric naming consistency Similarity metric tests ( similarity_test.go ): TestLevenshteinSimilarity - Edit distance similarity TestLevenshteinDistance - Raw edit distance calculation TestJaccardSimilarity - Set-based similarity TestCosineSimilarity - Vector-based similarity TestBLEU - Machine translation metric TestROUGE - Summarization metric TestFuzzyMatch - Combined fuzzy matching TestSemanticSimilarity - Word-based semantic similarity TestLCSLength - Longest common subsequence","title":"Heuristic Metrics (evaluation/heuristic/)"},{"location":"getting-started/testing/#writing-new-tests","text":"","title":"Writing New Tests"},{"location":"getting-started/testing/#test-file-naming","text":"Follow Go conventions: foo.go \u2192 foo_test.go Package remains the same (not _test suffix for internal tests)","title":"Test File Naming"},{"location":"getting-started/testing/#test-structure-example","text":"package opik import ( \"context\" \"testing\" ) func TestMyFunction(t *testing.T) { ctx := context.Background() tests := []struct { name string input string expected string }{ {\"basic case\", \"input\", \"expected\"}, {\"edge case\", \"\", \"\"}, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { result := MyFunction(ctx, tt.input) if result != tt.expected { t.Errorf(\"MyFunction() = %v, want %v\", result, tt.expected) } }) } }","title":"Test Structure Example"},{"location":"getting-started/testing/#testing-metrics","text":"Use the evaluation.MetricInput type: func TestMyMetric(t *testing.T) { ctx := context.Background() metric := NewMyMetric() input := evaluation.NewMetricInput(\"\", \"output text\"). WithExpected(\"expected text\") result := metric.Score(ctx, input) if result.Value < 0.9 { t.Errorf(\"Score = %v, want >= 0.9\", result.Value) } }","title":"Testing Metrics"},{"location":"getting-started/testing/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"getting-started/testing/#github-actions-example","text":"name: Tests on: [push, pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-go@v5 with: go-version: '1.21' - name: Run Tests run: go test ./... -v -race - name: Run Tests with Coverage run: go test ./... -coverprofile=coverage.out - name: Upload Coverage uses: codecov/codecov-action@v3 with: file: coverage.out","title":"GitHub Actions Example"},{"location":"getting-started/testing/#test-coverage-report","text":"Generate a coverage report: # Generate coverage profile go test ./... -coverprofile=coverage.out # View coverage summary go tool cover -func=coverage.out # Generate HTML report go tool cover -html=coverage.out -o coverage.html","title":"Test Coverage Report"},{"location":"getting-started/testing/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/testing/#tests-fail-due-to-missing-dependencies","text":"go mod download go mod tidy","title":"Tests Fail Due to Missing Dependencies"},{"location":"getting-started/testing/#timeout-issues","text":"Increase test timeout for slow machines: go test ./... -timeout 5m","title":"Timeout Issues"},{"location":"getting-started/testing/#race-condition-detection","text":"Run tests with race detector: go test ./... -race","title":"Race Condition Detection"},{"location":"integrations/anthropic/","text":"Anthropic Integration \u00b6 Auto-trace Anthropic API calls and use Claude as an evaluation judge. import \"github.com/agentplexus/go-comet-ml-opik/integrations/anthropic\" Auto-Tracing \u00b6 Tracing HTTP Client \u00b6 Wrap HTTP calls to automatically create spans: opikClient, _ := opik.NewClient() // Create tracing HTTP client httpClient := anthropic.TracingHTTPClient(opikClient) // Use with your Anthropic client Tracing Provider \u00b6 Create a complete tracing provider: tracingProvider := anthropic.TracingProvider(opikClient, anthropic.WithModel(\"claude-sonnet-4-20250514\"), ) Wrap Existing Client \u00b6 Add tracing to an existing HTTP client: existingClient := &http.Client{Timeout: 60 * time.Second} tracingClient := anthropic.Wrap(existingClient, opikClient) What Gets Traced \u00b6 Each API call creates a span with: Field Description Type LLM Provider anthropic Model Model name from request Input Request body (messages, system prompt) Output Response body (content, stop reason) Metadata Token usage (input_tokens, output_tokens), duration Evaluation Provider \u00b6 Use Claude as an LLM judge: provider := anthropic.NewProvider( anthropic.WithAPIKey(\"your-api-key\"), anthropic.WithModel(\"claude-sonnet-4-20250514\"), anthropic.WithTemperature(0.0), ) // Use with evaluation metrics relevance := llm.NewAnswerRelevance(provider) coherence := llm.NewCoherence(provider) Provider Options \u00b6 Option Description WithAPIKey(key) Set API key (default: ANTHROPIC_API_KEY env) WithModel(model) Set model (default: claude-sonnet-4-20250514 ) WithBaseURL(url) Custom endpoint WithHTTPClient(client) Custom HTTP client WithTemperature(temp) Generation temperature WithMaxTokens(max) Maximum tokens Complete Example \u00b6 func main() { ctx := context.Background() // Create Opik client opikClient, _ := opik.NewClient() // Create tracing HTTP client httpClient := anthropic.TracingHTTPClient(opikClient) // Start a trace ctx, trace, _ := opik.StartTrace(ctx, opikClient, \"claude-request\") defer trace.End(ctx) // Make Anthropic API call req, _ := http.NewRequestWithContext(ctx, \"POST\", \"https://api.anthropic.com/v1/messages\", bytes.NewReader(requestBody)) req.Header.Set(\"x-api-key\", os.Getenv(\"ANTHROPIC_API_KEY\")) req.Header.Set(\"anthropic-version\", \"2023-06-01\") req.Header.Set(\"content-type\", \"application/json\") // Call is automatically traced! resp, err := httpClient.Do(req) if err != nil { log.Fatal(err) } defer resp.Body.Close() // Process response... } Supported Operations \u00b6 The integration traces these Anthropic API endpoints: Endpoint Span Name /v1/messages anthropic.messages /v1/complete anthropic.complete Other anthropic.api","title":"Anthropic"},{"location":"integrations/anthropic/#anthropic-integration","text":"Auto-trace Anthropic API calls and use Claude as an evaluation judge. import \"github.com/agentplexus/go-comet-ml-opik/integrations/anthropic\"","title":"Anthropic Integration"},{"location":"integrations/anthropic/#auto-tracing","text":"","title":"Auto-Tracing"},{"location":"integrations/anthropic/#tracing-http-client","text":"Wrap HTTP calls to automatically create spans: opikClient, _ := opik.NewClient() // Create tracing HTTP client httpClient := anthropic.TracingHTTPClient(opikClient) // Use with your Anthropic client","title":"Tracing HTTP Client"},{"location":"integrations/anthropic/#tracing-provider","text":"Create a complete tracing provider: tracingProvider := anthropic.TracingProvider(opikClient, anthropic.WithModel(\"claude-sonnet-4-20250514\"), )","title":"Tracing Provider"},{"location":"integrations/anthropic/#wrap-existing-client","text":"Add tracing to an existing HTTP client: existingClient := &http.Client{Timeout: 60 * time.Second} tracingClient := anthropic.Wrap(existingClient, opikClient)","title":"Wrap Existing Client"},{"location":"integrations/anthropic/#what-gets-traced","text":"Each API call creates a span with: Field Description Type LLM Provider anthropic Model Model name from request Input Request body (messages, system prompt) Output Response body (content, stop reason) Metadata Token usage (input_tokens, output_tokens), duration","title":"What Gets Traced"},{"location":"integrations/anthropic/#evaluation-provider","text":"Use Claude as an LLM judge: provider := anthropic.NewProvider( anthropic.WithAPIKey(\"your-api-key\"), anthropic.WithModel(\"claude-sonnet-4-20250514\"), anthropic.WithTemperature(0.0), ) // Use with evaluation metrics relevance := llm.NewAnswerRelevance(provider) coherence := llm.NewCoherence(provider)","title":"Evaluation Provider"},{"location":"integrations/anthropic/#provider-options","text":"Option Description WithAPIKey(key) Set API key (default: ANTHROPIC_API_KEY env) WithModel(model) Set model (default: claude-sonnet-4-20250514 ) WithBaseURL(url) Custom endpoint WithHTTPClient(client) Custom HTTP client WithTemperature(temp) Generation temperature WithMaxTokens(max) Maximum tokens","title":"Provider Options"},{"location":"integrations/anthropic/#complete-example","text":"func main() { ctx := context.Background() // Create Opik client opikClient, _ := opik.NewClient() // Create tracing HTTP client httpClient := anthropic.TracingHTTPClient(opikClient) // Start a trace ctx, trace, _ := opik.StartTrace(ctx, opikClient, \"claude-request\") defer trace.End(ctx) // Make Anthropic API call req, _ := http.NewRequestWithContext(ctx, \"POST\", \"https://api.anthropic.com/v1/messages\", bytes.NewReader(requestBody)) req.Header.Set(\"x-api-key\", os.Getenv(\"ANTHROPIC_API_KEY\")) req.Header.Set(\"anthropic-version\", \"2023-06-01\") req.Header.Set(\"content-type\", \"application/json\") // Call is automatically traced! resp, err := httpClient.Do(req) if err != nil { log.Fatal(err) } defer resp.Body.Close() // Process response... }","title":"Complete Example"},{"location":"integrations/anthropic/#supported-operations","text":"The integration traces these Anthropic API endpoints: Endpoint Span Name /v1/messages anthropic.messages /v1/complete anthropic.complete Other anthropic.api","title":"Supported Operations"},{"location":"integrations/http-middleware/","text":"HTTP Middleware \u00b6 Add tracing to HTTP handlers and clients. import \"github.com/agentplexus/go-comet-ml-opik/middleware\" Server Middleware \u00b6 Wrap HTTP Handlers \u00b6 Automatically create traces for incoming requests: opikClient, _ := opik.NewClient() // Wrap a handler handler := middleware.TracingMiddleware(opikClient, \"api-request\")(yourHandler) // Use with http.ServeMux mux := http.NewServeMux() mux.Handle(\"/api/\", middleware.TracingMiddleware(opikClient, \"api\")(apiHandler)) What Gets Traced \u00b6 Each request creates a trace with: Field Description Name Configured name (e.g., \"api-request\") Input Method, URL, headers Output Status code, response size Metadata Duration, client IP Example Handler \u00b6 func main() { opikClient, _ := opik.NewClient() handler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { // Access trace from context trace := opik.TraceFromContext(r.Context()) // Create spans for sub-operations ctx, span, _ := opik.StartSpan(r.Context(), \"process-request\") // Do work... span.End(ctx) w.Write([]byte(\"OK\")) }) // Wrap with tracing tracedHandler := middleware.TracingMiddleware(opikClient, \"my-api\")(handler) http.ListenAndServe(\":8080\", tracedHandler) } Client Middleware \u00b6 Tracing HTTP Client \u00b6 Create a client that traces all outgoing requests: httpClient := middleware.TracingHTTPClient(\"external-api\") resp, _ := httpClient.Get(\"https://api.example.com/data\") Tracing Round Tripper \u00b6 Wrap an existing transport: transport := middleware.NewTracingRoundTripper(http.DefaultTransport, \"api-call\") httpClient := &http.Client{ Transport: transport, Timeout: 30 * time.Second, } What Gets Traced \u00b6 Each outgoing request creates a span with: Field Description Name Configured name Type general Input Method, URL Output Status code Metadata Duration Combining Server and Client \u00b6 func handler(w http.ResponseWriter, r *http.Request) { ctx := r.Context() // Trace from middleware is in context trace := opik.TraceFromContext(ctx) // Create span for external call ctx, span, _ := opik.StartSpan(ctx, \"fetch-data\") // Use tracing client - span becomes parent client := middleware.TracingHTTPClient(\"external-api\") req, _ := http.NewRequestWithContext(ctx, \"GET\", \"https://api.example.com\", nil) resp, err := client.Do(req) span.End(ctx) // Process response... } Framework Integration \u00b6 Chi \u00b6 import \"github.com/go-chi/chi/v5\" r := chi.NewRouter() r.Use(func(next http.Handler) http.Handler { return middleware.TracingMiddleware(opikClient, \"api\")(next) }) Gin \u00b6 import \"github.com/gin-gonic/gin\" r := gin.Default() r.Use(func(c *gin.Context) { handler := middleware.TracingMiddleware(opikClient, \"api\")( http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { c.Request = r.WithContext(r.Context()) c.Next() }), ) handler.ServeHTTP(c.Writer, c.Request) }) Echo \u00b6 import \"github.com/labstack/echo/v4\" e := echo.New() e.Use(echo.WrapMiddleware(func(next http.Handler) http.Handler { return middleware.TracingMiddleware(opikClient, \"api\")(next) })) Best Practices \u00b6 Name traces descriptively : Use service/endpoint names Add to all entry points : Trace all HTTP handlers Propagate context : Pass request context to downstream calls Use with distributed tracing : Combine with header propagation","title":"HTTP Middleware"},{"location":"integrations/http-middleware/#http-middleware","text":"Add tracing to HTTP handlers and clients. import \"github.com/agentplexus/go-comet-ml-opik/middleware\"","title":"HTTP Middleware"},{"location":"integrations/http-middleware/#server-middleware","text":"","title":"Server Middleware"},{"location":"integrations/http-middleware/#wrap-http-handlers","text":"Automatically create traces for incoming requests: opikClient, _ := opik.NewClient() // Wrap a handler handler := middleware.TracingMiddleware(opikClient, \"api-request\")(yourHandler) // Use with http.ServeMux mux := http.NewServeMux() mux.Handle(\"/api/\", middleware.TracingMiddleware(opikClient, \"api\")(apiHandler))","title":"Wrap HTTP Handlers"},{"location":"integrations/http-middleware/#what-gets-traced","text":"Each request creates a trace with: Field Description Name Configured name (e.g., \"api-request\") Input Method, URL, headers Output Status code, response size Metadata Duration, client IP","title":"What Gets Traced"},{"location":"integrations/http-middleware/#example-handler","text":"func main() { opikClient, _ := opik.NewClient() handler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { // Access trace from context trace := opik.TraceFromContext(r.Context()) // Create spans for sub-operations ctx, span, _ := opik.StartSpan(r.Context(), \"process-request\") // Do work... span.End(ctx) w.Write([]byte(\"OK\")) }) // Wrap with tracing tracedHandler := middleware.TracingMiddleware(opikClient, \"my-api\")(handler) http.ListenAndServe(\":8080\", tracedHandler) }","title":"Example Handler"},{"location":"integrations/http-middleware/#client-middleware","text":"","title":"Client Middleware"},{"location":"integrations/http-middleware/#tracing-http-client","text":"Create a client that traces all outgoing requests: httpClient := middleware.TracingHTTPClient(\"external-api\") resp, _ := httpClient.Get(\"https://api.example.com/data\")","title":"Tracing HTTP Client"},{"location":"integrations/http-middleware/#tracing-round-tripper","text":"Wrap an existing transport: transport := middleware.NewTracingRoundTripper(http.DefaultTransport, \"api-call\") httpClient := &http.Client{ Transport: transport, Timeout: 30 * time.Second, }","title":"Tracing Round Tripper"},{"location":"integrations/http-middleware/#what-gets-traced_1","text":"Each outgoing request creates a span with: Field Description Name Configured name Type general Input Method, URL Output Status code Metadata Duration","title":"What Gets Traced"},{"location":"integrations/http-middleware/#combining-server-and-client","text":"func handler(w http.ResponseWriter, r *http.Request) { ctx := r.Context() // Trace from middleware is in context trace := opik.TraceFromContext(ctx) // Create span for external call ctx, span, _ := opik.StartSpan(ctx, \"fetch-data\") // Use tracing client - span becomes parent client := middleware.TracingHTTPClient(\"external-api\") req, _ := http.NewRequestWithContext(ctx, \"GET\", \"https://api.example.com\", nil) resp, err := client.Do(req) span.End(ctx) // Process response... }","title":"Combining Server and Client"},{"location":"integrations/http-middleware/#framework-integration","text":"","title":"Framework Integration"},{"location":"integrations/http-middleware/#chi","text":"import \"github.com/go-chi/chi/v5\" r := chi.NewRouter() r.Use(func(next http.Handler) http.Handler { return middleware.TracingMiddleware(opikClient, \"api\")(next) })","title":"Chi"},{"location":"integrations/http-middleware/#gin","text":"import \"github.com/gin-gonic/gin\" r := gin.Default() r.Use(func(c *gin.Context) { handler := middleware.TracingMiddleware(opikClient, \"api\")( http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { c.Request = r.WithContext(r.Context()) c.Next() }), ) handler.ServeHTTP(c.Writer, c.Request) })","title":"Gin"},{"location":"integrations/http-middleware/#echo","text":"import \"github.com/labstack/echo/v4\" e := echo.New() e.Use(echo.WrapMiddleware(func(next http.Handler) http.Handler { return middleware.TracingMiddleware(opikClient, \"api\")(next) }))","title":"Echo"},{"location":"integrations/http-middleware/#best-practices","text":"Name traces descriptively : Use service/endpoint names Add to all entry points : Trace all HTTP handlers Propagate context : Pass request context to downstream calls Use with distributed tracing : Combine with header propagation","title":"Best Practices"},{"location":"integrations/omnillm/","text":"omnillm Integration \u00b6 Integrate with omnillm , a unified LLM wrapper library that supports multiple providers (OpenAI, Anthropic, Bedrock, Gemini, Ollama, xAI). import ( \"github.com/agentplexus/omnillm\" opikomnillm \"github.com/agentplexus/go-comet-ml-opik/integrations/omnillm\" ) Three Integration Options \u00b6 Choose the approach that best fits your use case: Option Use Case Complexity Manual Span Wrapping Fine-grained control Low Tracing Wrapper Automatic tracing Low Evaluation Provider LLM-as-judge Low Option 1: Manual Span Wrapping \u00b6 Wrap individual LLM calls with spans for maximum control. import ( opik \"github.com/agentplexus/go-comet-ml-opik\" \"github.com/agentplexus/omnillm\" ) func callLLM(ctx context.Context, client *omnillm.ChatClient, req *omnillm.ChatCompletionRequest) (*omnillm.ChatCompletionResponse, error) { // Get current span/trace from context var span *opik.Span var err error if parentSpan := opik.SpanFromContext(ctx); parentSpan != nil { span, err = parentSpan.Span(ctx, \"llm.chat\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(req.Model), opik.WithSpanInput(map[string]any{ \"messages\": req.Messages, \"model\": req.Model, }), ) } else if trace := opik.TraceFromContext(ctx); trace != nil { span, err = trace.Span(ctx, \"llm.chat\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(req.Model), opik.WithSpanInput(map[string]any{ \"messages\": req.Messages, \"model\": req.Model, }), ) } // Make the call startTime := time.Now() resp, respErr := client.CreateChatCompletion(ctx, req) duration := time.Since(startTime) // End span with output if span != nil && err == nil { endOpts := []opik.SpanOption{} if resp != nil { endOpts = append(endOpts, opik.WithSpanOutput(map[string]any{ \"content\": resp.Choices[0].Message.Content, \"model\": resp.Model, })) endOpts = append(endOpts, opik.WithSpanMetadata(map[string]any{ \"duration_ms\": duration.Milliseconds(), \"prompt_tokens\": resp.Usage.PromptTokens, \"completion_tokens\": resp.Usage.CompletionTokens, \"total_tokens\": resp.Usage.TotalTokens, })) } if respErr != nil { endOpts = append(endOpts, opik.WithSpanMetadata(map[string]any{ \"error\": respErr.Error(), })) } span.End(ctx, endOpts...) } return resp, respErr } When to Use \u00b6 You need custom span names or metadata You want to trace only specific calls You're integrating into existing code gradually Option 2: Tracing Wrapper \u00b6 Use the built-in TracingClient wrapper for automatic tracing of all calls. import ( opik \"github.com/agentplexus/go-comet-ml-opik\" opikomnillm \"github.com/agentplexus/go-comet-ml-opik/integrations/omnillm\" \"github.com/agentplexus/omnillm\" ) func main() { // Create omnillm client client, _ := omnillm.NewClient(omnillm.ClientConfig{ Provider: omnillm.ProviderNameOpenAI, APIKey: os.Getenv(\"OPENAI_API_KEY\"), }) // Create Opik client opikClient, _ := opik.NewClient() // Wrap with tracing tracingClient := opikomnillm.NewTracingClient(client, opikClient) // Start a trace ctx, trace, _ := opik.StartTrace(ctx, opikClient, \"my-task\") defer trace.End(ctx) // All calls are automatically traced! resp, _ := tracingClient.CreateChatCompletion(ctx, &omnillm.ChatCompletionRequest{ Model: \"gpt-4o\", Messages: []omnillm.Message{ {Role: omnillm.RoleUser, Content: \"Hello!\"}, }, }) fmt.Println(resp.Choices[0].Message.Content) } TracingClient Methods \u00b6 Method Description CreateChatCompletion Traced chat completion CreateChatCompletionStream Traced streaming completion CreateChatCompletionWithMemory Traced completion with memory Close Close underlying client Client Access underlying omnillm client Streaming Support \u00b6 stream, _ := tracingClient.CreateChatCompletionStream(ctx, req) defer stream.Close() for { chunk, err := stream.Recv() if err == io.EOF { break } fmt.Print(chunk.Choices[0].Delta.Content) } // Span automatically ended with accumulated content Memory Support \u00b6 // Traced conversation with memory resp, _ := tracingClient.CreateChatCompletionWithMemory(ctx, \"session-123\", req) // Span includes session_id in metadata When to Use \u00b6 You want automatic tracing for all LLM calls You're building a new application You want consistent span formatting Option 3: Evaluation Provider \u00b6 Use omnillm as an LLM provider for evaluation judges. import ( opikomnillm \"github.com/agentplexus/go-comet-ml-opik/integrations/omnillm\" \"github.com/agentplexus/go-comet-ml-opik/evaluation/llm\" \"github.com/agentplexus/omnillm\" ) func main() { // Create omnillm client with any provider client, _ := omnillm.NewClient(omnillm.ClientConfig{ Provider: omnillm.ProviderNameAnthropic, APIKey: os.Getenv(\"ANTHROPIC_API_KEY\"), }) // Create evaluation provider provider := opikomnillm.NewProvider(client, opikomnillm.WithModel(\"claude-sonnet-4-20250514\"), opikomnillm.WithTemperature(0.0), ) // Use with evaluation metrics relevance := llm.NewAnswerRelevance(provider) hallucination := llm.NewHallucination(provider) coherence := llm.NewCoherence(provider) // Create evaluation engine engine := evaluation.NewEngine([]evaluation.Metric{ relevance, hallucination, coherence, }) // Evaluate input := evaluation.NewMetricInput(question, answer). WithExpected(expectedAnswer). WithContext(documents) result := engine.EvaluateOne(ctx, input) } Provider Options \u00b6 Option Description WithModel(model) Set model name WithTemperature(temp) Set temperature WithMaxTokens(max) Set max tokens When to Use \u00b6 Running evaluation experiments LLM-as-judge workflows Comparing outputs across models Supported Providers \u00b6 omnillm supports these providers, all work with the Opik integration: Provider Config OpenAI ProviderNameOpenAI Anthropic ProviderNameAnthropic AWS Bedrock ProviderNameBedrock Google Gemini ProviderNameGemini Ollama ProviderNameOllama xAI (Grok) ProviderNameXAI Complete Example: All Three Options \u00b6 package main import ( \"context\" \"fmt\" opik \"github.com/agentplexus/go-comet-ml-opik\" \"github.com/agentplexus/go-comet-ml-opik/evaluation\" \"github.com/agentplexus/go-comet-ml-opik/evaluation/llm\" opikomnillm \"github.com/agentplexus/go-comet-ml-opik/integrations/omnillm\" \"github.com/agentplexus/omnillm\" ) func main() { ctx := context.Background() // Create omnillm client client, _ := omnillm.NewClient(omnillm.ClientConfig{ Provider: omnillm.ProviderNameOpenAI, APIKey: os.Getenv(\"OPENAI_API_KEY\"), }) // Create Opik client opikClient, _ := opik.NewClient() // OPTION 2: Tracing wrapper for automatic tracing tracingClient := opikomnillm.NewTracingClient(client, opikClient) // OPTION 3: Evaluation provider for LLM judges evalProvider := opikomnillm.NewProvider(client, opikomnillm.WithModel(\"gpt-4o\"), ) // Start trace ctx, trace, _ := opik.StartTrace(ctx, opikClient, \"demo\") defer trace.End(ctx) // Generate response (automatically traced) resp, _ := tracingClient.CreateChatCompletion(ctx, &omnillm.ChatCompletionRequest{ Model: \"gpt-4o\", Messages: []omnillm.Message{{Role: omnillm.RoleUser, Content: \"What is 2+2?\"}}, }) answer := resp.Choices[0].Message.Content // Evaluate response metrics := []evaluation.Metric{ llm.NewAnswerRelevance(evalProvider), llm.NewCoherence(evalProvider), } engine := evaluation.NewEngine(metrics) input := evaluation.NewMetricInput(\"What is 2+2?\", answer).WithExpected(\"4\") result := engine.EvaluateOne(ctx, input) fmt.Printf(\"Response: %s\\n\", answer) fmt.Printf(\"Average score: %.2f\\n\", result.AverageScore()) }","title":"omnillm"},{"location":"integrations/omnillm/#omnillm-integration","text":"Integrate with omnillm , a unified LLM wrapper library that supports multiple providers (OpenAI, Anthropic, Bedrock, Gemini, Ollama, xAI). import ( \"github.com/agentplexus/omnillm\" opikomnillm \"github.com/agentplexus/go-comet-ml-opik/integrations/omnillm\" )","title":"omnillm Integration"},{"location":"integrations/omnillm/#three-integration-options","text":"Choose the approach that best fits your use case: Option Use Case Complexity Manual Span Wrapping Fine-grained control Low Tracing Wrapper Automatic tracing Low Evaluation Provider LLM-as-judge Low","title":"Three Integration Options"},{"location":"integrations/omnillm/#option-1-manual-span-wrapping","text":"Wrap individual LLM calls with spans for maximum control. import ( opik \"github.com/agentplexus/go-comet-ml-opik\" \"github.com/agentplexus/omnillm\" ) func callLLM(ctx context.Context, client *omnillm.ChatClient, req *omnillm.ChatCompletionRequest) (*omnillm.ChatCompletionResponse, error) { // Get current span/trace from context var span *opik.Span var err error if parentSpan := opik.SpanFromContext(ctx); parentSpan != nil { span, err = parentSpan.Span(ctx, \"llm.chat\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(req.Model), opik.WithSpanInput(map[string]any{ \"messages\": req.Messages, \"model\": req.Model, }), ) } else if trace := opik.TraceFromContext(ctx); trace != nil { span, err = trace.Span(ctx, \"llm.chat\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(req.Model), opik.WithSpanInput(map[string]any{ \"messages\": req.Messages, \"model\": req.Model, }), ) } // Make the call startTime := time.Now() resp, respErr := client.CreateChatCompletion(ctx, req) duration := time.Since(startTime) // End span with output if span != nil && err == nil { endOpts := []opik.SpanOption{} if resp != nil { endOpts = append(endOpts, opik.WithSpanOutput(map[string]any{ \"content\": resp.Choices[0].Message.Content, \"model\": resp.Model, })) endOpts = append(endOpts, opik.WithSpanMetadata(map[string]any{ \"duration_ms\": duration.Milliseconds(), \"prompt_tokens\": resp.Usage.PromptTokens, \"completion_tokens\": resp.Usage.CompletionTokens, \"total_tokens\": resp.Usage.TotalTokens, })) } if respErr != nil { endOpts = append(endOpts, opik.WithSpanMetadata(map[string]any{ \"error\": respErr.Error(), })) } span.End(ctx, endOpts...) } return resp, respErr }","title":"Option 1: Manual Span Wrapping"},{"location":"integrations/omnillm/#when-to-use","text":"You need custom span names or metadata You want to trace only specific calls You're integrating into existing code gradually","title":"When to Use"},{"location":"integrations/omnillm/#option-2-tracing-wrapper","text":"Use the built-in TracingClient wrapper for automatic tracing of all calls. import ( opik \"github.com/agentplexus/go-comet-ml-opik\" opikomnillm \"github.com/agentplexus/go-comet-ml-opik/integrations/omnillm\" \"github.com/agentplexus/omnillm\" ) func main() { // Create omnillm client client, _ := omnillm.NewClient(omnillm.ClientConfig{ Provider: omnillm.ProviderNameOpenAI, APIKey: os.Getenv(\"OPENAI_API_KEY\"), }) // Create Opik client opikClient, _ := opik.NewClient() // Wrap with tracing tracingClient := opikomnillm.NewTracingClient(client, opikClient) // Start a trace ctx, trace, _ := opik.StartTrace(ctx, opikClient, \"my-task\") defer trace.End(ctx) // All calls are automatically traced! resp, _ := tracingClient.CreateChatCompletion(ctx, &omnillm.ChatCompletionRequest{ Model: \"gpt-4o\", Messages: []omnillm.Message{ {Role: omnillm.RoleUser, Content: \"Hello!\"}, }, }) fmt.Println(resp.Choices[0].Message.Content) }","title":"Option 2: Tracing Wrapper"},{"location":"integrations/omnillm/#tracingclient-methods","text":"Method Description CreateChatCompletion Traced chat completion CreateChatCompletionStream Traced streaming completion CreateChatCompletionWithMemory Traced completion with memory Close Close underlying client Client Access underlying omnillm client","title":"TracingClient Methods"},{"location":"integrations/omnillm/#streaming-support","text":"stream, _ := tracingClient.CreateChatCompletionStream(ctx, req) defer stream.Close() for { chunk, err := stream.Recv() if err == io.EOF { break } fmt.Print(chunk.Choices[0].Delta.Content) } // Span automatically ended with accumulated content","title":"Streaming Support"},{"location":"integrations/omnillm/#memory-support","text":"// Traced conversation with memory resp, _ := tracingClient.CreateChatCompletionWithMemory(ctx, \"session-123\", req) // Span includes session_id in metadata","title":"Memory Support"},{"location":"integrations/omnillm/#when-to-use_1","text":"You want automatic tracing for all LLM calls You're building a new application You want consistent span formatting","title":"When to Use"},{"location":"integrations/omnillm/#option-3-evaluation-provider","text":"Use omnillm as an LLM provider for evaluation judges. import ( opikomnillm \"github.com/agentplexus/go-comet-ml-opik/integrations/omnillm\" \"github.com/agentplexus/go-comet-ml-opik/evaluation/llm\" \"github.com/agentplexus/omnillm\" ) func main() { // Create omnillm client with any provider client, _ := omnillm.NewClient(omnillm.ClientConfig{ Provider: omnillm.ProviderNameAnthropic, APIKey: os.Getenv(\"ANTHROPIC_API_KEY\"), }) // Create evaluation provider provider := opikomnillm.NewProvider(client, opikomnillm.WithModel(\"claude-sonnet-4-20250514\"), opikomnillm.WithTemperature(0.0), ) // Use with evaluation metrics relevance := llm.NewAnswerRelevance(provider) hallucination := llm.NewHallucination(provider) coherence := llm.NewCoherence(provider) // Create evaluation engine engine := evaluation.NewEngine([]evaluation.Metric{ relevance, hallucination, coherence, }) // Evaluate input := evaluation.NewMetricInput(question, answer). WithExpected(expectedAnswer). WithContext(documents) result := engine.EvaluateOne(ctx, input) }","title":"Option 3: Evaluation Provider"},{"location":"integrations/omnillm/#provider-options","text":"Option Description WithModel(model) Set model name WithTemperature(temp) Set temperature WithMaxTokens(max) Set max tokens","title":"Provider Options"},{"location":"integrations/omnillm/#when-to-use_2","text":"Running evaluation experiments LLM-as-judge workflows Comparing outputs across models","title":"When to Use"},{"location":"integrations/omnillm/#supported-providers","text":"omnillm supports these providers, all work with the Opik integration: Provider Config OpenAI ProviderNameOpenAI Anthropic ProviderNameAnthropic AWS Bedrock ProviderNameBedrock Google Gemini ProviderNameGemini Ollama ProviderNameOllama xAI (Grok) ProviderNameXAI","title":"Supported Providers"},{"location":"integrations/omnillm/#complete-example-all-three-options","text":"package main import ( \"context\" \"fmt\" opik \"github.com/agentplexus/go-comet-ml-opik\" \"github.com/agentplexus/go-comet-ml-opik/evaluation\" \"github.com/agentplexus/go-comet-ml-opik/evaluation/llm\" opikomnillm \"github.com/agentplexus/go-comet-ml-opik/integrations/omnillm\" \"github.com/agentplexus/omnillm\" ) func main() { ctx := context.Background() // Create omnillm client client, _ := omnillm.NewClient(omnillm.ClientConfig{ Provider: omnillm.ProviderNameOpenAI, APIKey: os.Getenv(\"OPENAI_API_KEY\"), }) // Create Opik client opikClient, _ := opik.NewClient() // OPTION 2: Tracing wrapper for automatic tracing tracingClient := opikomnillm.NewTracingClient(client, opikClient) // OPTION 3: Evaluation provider for LLM judges evalProvider := opikomnillm.NewProvider(client, opikomnillm.WithModel(\"gpt-4o\"), ) // Start trace ctx, trace, _ := opik.StartTrace(ctx, opikClient, \"demo\") defer trace.End(ctx) // Generate response (automatically traced) resp, _ := tracingClient.CreateChatCompletion(ctx, &omnillm.ChatCompletionRequest{ Model: \"gpt-4o\", Messages: []omnillm.Message{{Role: omnillm.RoleUser, Content: \"What is 2+2?\"}}, }) answer := resp.Choices[0].Message.Content // Evaluate response metrics := []evaluation.Metric{ llm.NewAnswerRelevance(evalProvider), llm.NewCoherence(evalProvider), } engine := evaluation.NewEngine(metrics) input := evaluation.NewMetricInput(\"What is 2+2?\", answer).WithExpected(\"4\") result := engine.EvaluateOne(ctx, input) fmt.Printf(\"Response: %s\\n\", answer) fmt.Printf(\"Average score: %.2f\\n\", result.AverageScore()) }","title":"Complete Example: All Three Options"},{"location":"integrations/openai/","text":"OpenAI Integration \u00b6 Auto-trace OpenAI API calls and use OpenAI as an evaluation judge. import \"github.com/agentplexus/go-comet-ml-opik/integrations/openai\" Auto-Tracing \u00b6 Tracing HTTP Client \u00b6 Wrap HTTP calls to automatically create spans: opikClient, _ := opik.NewClient() // Create tracing HTTP client httpClient := openai.TracingHTTPClient(opikClient) // Use with OpenAI SDK config := openai.DefaultConfig(\"your-api-key\") config.HTTPClient = httpClient client := openai.NewClientWithConfig(config) Tracing Provider \u00b6 Create a complete tracing provider: tracingProvider := openai.TracingProvider(opikClient, openai.WithModel(\"gpt-4o\"), ) Wrap Existing Client \u00b6 Add tracing to an existing HTTP client: existingClient := &http.Client{Timeout: 30 * time.Second} tracingClient := openai.Wrap(existingClient, opikClient) What Gets Traced \u00b6 Each API call creates a span with: Field Description Type LLM Provider openai Model Model name from request Input Request body (messages, parameters) Output Response body (completions, choices) Metadata Token usage, duration Evaluation Provider \u00b6 Use OpenAI as an LLM judge: provider := openai.NewProvider( openai.WithAPIKey(\"your-api-key\"), openai.WithModel(\"gpt-4o\"), openai.WithTemperature(0.0), // Deterministic for evaluation ) // Use with evaluation metrics relevance := llm.NewAnswerRelevance(provider) hallucination := llm.NewHallucination(provider) Provider Options \u00b6 Option Description WithAPIKey(key) Set API key (default: OPENAI_API_KEY env) WithModel(model) Set model (default: gpt-4o ) WithBaseURL(url) Custom endpoint (for Azure, proxies) WithHTTPClient(client) Custom HTTP client WithTemperature(temp) Generation temperature WithMaxTokens(max) Maximum tokens Complete Example \u00b6 func main() { ctx := context.Background() // Create Opik client opikClient, _ := opik.NewClient() // Create tracing HTTP client httpClient := openai.TracingHTTPClient(opikClient) // Configure OpenAI with tracing config := openai.DefaultConfig(os.Getenv(\"OPENAI_API_KEY\")) config.HTTPClient = httpClient oaiClient := openai.NewClientWithConfig(config) // Start a trace ctx, trace, _ := opik.StartTrace(ctx, opikClient, \"chat-request\") defer trace.End(ctx) // Make OpenAI call - automatically traced! resp, err := oaiClient.CreateChatCompletion(ctx, openai.ChatCompletionRequest{ Model: \"gpt-4o\", Messages: []openai.ChatCompletionMessage{ {Role: \"user\", Content: \"Hello!\"}, }, }) if err != nil { log.Fatal(err) } fmt.Println(resp.Choices[0].Message.Content) } Azure OpenAI \u00b6 Use with Azure OpenAI Service: provider := openai.NewProvider( openai.WithAPIKey(\"your-azure-key\"), openai.WithBaseURL(\"https://your-resource.openai.azure.com/openai/deployments/your-deployment\"), )","title":"OpenAI"},{"location":"integrations/openai/#openai-integration","text":"Auto-trace OpenAI API calls and use OpenAI as an evaluation judge. import \"github.com/agentplexus/go-comet-ml-opik/integrations/openai\"","title":"OpenAI Integration"},{"location":"integrations/openai/#auto-tracing","text":"","title":"Auto-Tracing"},{"location":"integrations/openai/#tracing-http-client","text":"Wrap HTTP calls to automatically create spans: opikClient, _ := opik.NewClient() // Create tracing HTTP client httpClient := openai.TracingHTTPClient(opikClient) // Use with OpenAI SDK config := openai.DefaultConfig(\"your-api-key\") config.HTTPClient = httpClient client := openai.NewClientWithConfig(config)","title":"Tracing HTTP Client"},{"location":"integrations/openai/#tracing-provider","text":"Create a complete tracing provider: tracingProvider := openai.TracingProvider(opikClient, openai.WithModel(\"gpt-4o\"), )","title":"Tracing Provider"},{"location":"integrations/openai/#wrap-existing-client","text":"Add tracing to an existing HTTP client: existingClient := &http.Client{Timeout: 30 * time.Second} tracingClient := openai.Wrap(existingClient, opikClient)","title":"Wrap Existing Client"},{"location":"integrations/openai/#what-gets-traced","text":"Each API call creates a span with: Field Description Type LLM Provider openai Model Model name from request Input Request body (messages, parameters) Output Response body (completions, choices) Metadata Token usage, duration","title":"What Gets Traced"},{"location":"integrations/openai/#evaluation-provider","text":"Use OpenAI as an LLM judge: provider := openai.NewProvider( openai.WithAPIKey(\"your-api-key\"), openai.WithModel(\"gpt-4o\"), openai.WithTemperature(0.0), // Deterministic for evaluation ) // Use with evaluation metrics relevance := llm.NewAnswerRelevance(provider) hallucination := llm.NewHallucination(provider)","title":"Evaluation Provider"},{"location":"integrations/openai/#provider-options","text":"Option Description WithAPIKey(key) Set API key (default: OPENAI_API_KEY env) WithModel(model) Set model (default: gpt-4o ) WithBaseURL(url) Custom endpoint (for Azure, proxies) WithHTTPClient(client) Custom HTTP client WithTemperature(temp) Generation temperature WithMaxTokens(max) Maximum tokens","title":"Provider Options"},{"location":"integrations/openai/#complete-example","text":"func main() { ctx := context.Background() // Create Opik client opikClient, _ := opik.NewClient() // Create tracing HTTP client httpClient := openai.TracingHTTPClient(opikClient) // Configure OpenAI with tracing config := openai.DefaultConfig(os.Getenv(\"OPENAI_API_KEY\")) config.HTTPClient = httpClient oaiClient := openai.NewClientWithConfig(config) // Start a trace ctx, trace, _ := opik.StartTrace(ctx, opikClient, \"chat-request\") defer trace.End(ctx) // Make OpenAI call - automatically traced! resp, err := oaiClient.CreateChatCompletion(ctx, openai.ChatCompletionRequest{ Model: \"gpt-4o\", Messages: []openai.ChatCompletionMessage{ {Role: \"user\", Content: \"Hello!\"}, }, }) if err != nil { log.Fatal(err) } fmt.Println(resp.Choices[0].Message.Content) }","title":"Complete Example"},{"location":"integrations/openai/#azure-openai","text":"Use with Azure OpenAI Service: provider := openai.NewProvider( openai.WithAPIKey(\"your-azure-key\"), openai.WithBaseURL(\"https://your-resource.openai.azure.com/openai/deployments/your-deployment\"), )","title":"Azure OpenAI"},{"location":"tutorials/agentic-observability/","text":"Agentic Observability: Integrating Opik with Google ADK and Eino \u00b6 This tutorial demonstrates how to add LLM observability to agentic Go applications using the go-comet-ml-opik SDK. We'll use a real-world case study based on the stats-agent-team project, which implements a multi-agent system for statistics research and verification. Overview \u00b6 Modern AI applications increasingly use agentic architectures where multiple specialized agents collaborate to complete complex tasks. Observability is critical for: Debugging : Understanding why an agent made a particular decision Performance : Identifying bottlenecks in multi-agent workflows Cost tracking : Monitoring LLM token usage across agents Quality assurance : Evaluating agent outputs over time This tutorial covers integration with two popular Go agent frameworks: Google Agent Development Kit (ADK) - A framework for building LLM-powered agents with tools Eino - CloudWeGo's framework for building deterministic agent workflows as graphs Case Study: Stats Agent Team \u00b6 The stats-agent-team project implements a 4-agent system for researching and verifying statistics: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Research Agent \u2502 \u2500\u2500\u25b6 \u2502 Synthesis Agent \u2502 \u2500\u2500\u25b6 \u2502Verification Agent\u2502 \u2500\u2500\u25b6 \u2502 Orchestrator \u2502 \u2502 (Web Search) \u2502 \u2502 (LLM + ADK) \u2502 \u2502 (LLM + ADK) \u2502 \u2502 (Eino Graph) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Research Agent : Finds relevant sources using web search APIs Synthesis Agent : Extracts statistics from web pages using LLM (Google ADK) Verification Agent : Verifies extracted statistics against sources (Google ADK) Orchestrator : Coordinates the workflow using Eino's graph-based approach Part 1: Integrating with Google ADK Agents \u00b6 Google ADK provides a structured way to build agents with tools. Here's how to add Opik observability. 1.1 Basic ADK Agent Structure \u00b6 The Synthesis Agent uses ADK's llmagent and functiontool packages: import ( \"google.golang.org/adk/agent\" \"google.golang.org/adk/agent/llmagent\" \"google.golang.org/adk/model\" \"google.golang.org/adk/tool\" \"google.golang.org/adk/tool/functiontool\" ) type SynthesisAgent struct { model model.LLM adkAgent agent.Agent } func NewSynthesisAgent(llmModel model.LLM) (*SynthesisAgent, error) { sa := &SynthesisAgent{model: llmModel} // Create a tool for statistics extraction synthesisTool, err := functiontool.New(functiontool.Config{ Name: \"synthesize_statistics\", Description: \"Extracts numerical statistics from web page content\", }, sa.synthesisToolHandler) if err != nil { return nil, err } // Create the ADK agent adkAgent, err := llmagent.New(llmagent.Config{ Name: \"statistics_synthesis_agent\", Model: llmModel, Description: \"Extracts statistics from web content\", Instruction: \"You are a statistics extraction expert...\", Tools: []tool.Tool{synthesisTool}, }) if err != nil { return nil, err } sa.adkAgent = adkAgent return sa, nil } 1.2 Adding Opik Tracing to ADK Agents \u00b6 Wrap your ADK agent's LLM calls with Opik traces: import ( opik \"github.com/agentplexus/go-comet-ml-opik\" ) type TracedSynthesisAgent struct { *SynthesisAgent opikClient *opik.Client } func NewTracedSynthesisAgent(llmModel model.LLM) (*TracedSynthesisAgent, error) { // Create base agent base, err := NewSynthesisAgent(llmModel) if err != nil { return nil, err } // Create Opik client opikClient, err := opik.NewClient( opik.WithProjectName(\"stats-agent-team\"), ) if err != nil { return nil, err } return &TracedSynthesisAgent{ SynthesisAgent: base, opikClient: opikClient, }, nil } func (tsa *TracedSynthesisAgent) ExtractStatistics(ctx context.Context, topic string, content string) ([]Statistic, error) { // Create a trace for this extraction operation trace, err := tsa.opikClient.Trace(ctx, \"extract-statistics\", opik.WithTraceInput(map[string]any{ \"topic\": topic, \"content_length\": len(content), }), opik.WithTraceTags(\"agent:synthesis\", \"operation:extraction\"), ) if err != nil { return nil, err } defer trace.End(ctx) // Create a span for the LLM call span, err := trace.Span(ctx, \"llm-extraction\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gemini-2.0-flash-exp\"), opik.WithSpanProvider(\"google\"), opik.WithSpanInput(map[string]any{ \"prompt_template\": \"statistics_extraction\", \"topic\": topic, }), ) if err != nil { return nil, err } // Make the actual LLM call stats, err := tsa.doExtraction(ctx, topic, content) // End span with output and token usage span.End(ctx, opik.WithSpanOutput(map[string]any{ \"statistics_count\": len(stats), \"statistics\": stats, }), opik.WithSpanUsage(opik.Usage{ PromptTokens: estimateTokens(content), CompletionTokens: estimateTokens(formatStats(stats)), }), ) if err != nil { // Add error feedback span.AddFeedbackScore(ctx, \"error\", 0, err.Error()) return nil, err } // Add quality feedback span.AddFeedbackScore(ctx, \"extraction_quality\", float64(len(stats))/10.0, // Normalize to 0-1 fmt.Sprintf(\"Extracted %d statistics\", len(stats)), ) // Update trace output trace.End(ctx, opik.WithTraceOutput(map[string]any{ \"total_statistics\": len(stats), \"success\": true, })) return stats, nil } 1.3 Tracing Tool Invocations \u00b6 When ADK agents invoke tools, trace each tool call as a separate span: func (tsa *TracedSynthesisAgent) synthesisToolHandler(ctx tool.Context, input SynthesisInput) (SynthesisOutput, error) { // Get parent trace from context (set by orchestrator) parentTrace := opik.TraceFromContext(ctx) // Create a span for this tool invocation span, err := parentTrace.Span(ctx, \"tool:synthesize_statistics\", opik.WithSpanType(opik.SpanTypeTool), opik.WithSpanInput(map[string]any{ \"topic\": input.Topic, \"url_count\": len(input.SearchResults), \"min_stats\": input.MinStatistics, \"max_stats\": input.MaxStatistics, }), ) if err != nil { return SynthesisOutput{}, err } defer span.End(ctx) // Process each URL with its own span var candidates []CandidateStatistic for i, result := range input.SearchResults { urlSpan, _ := span.Span(ctx, fmt.Sprintf(\"process-url-%d\", i), opik.WithSpanType(opik.SpanTypeGeneral), opik.WithSpanInput(map[string]any{ \"url\": result.URL, \"domain\": result.Domain, }), ) stats, err := tsa.processURL(ctx, input.Topic, result) urlSpan.End(ctx, opik.WithSpanOutput(map[string]any{ \"stats_extracted\": len(stats), \"error\": err != nil, })) if err == nil { candidates = append(candidates, stats...) } } span.End(ctx, opik.WithSpanOutput(map[string]any{ \"total_candidates\": len(candidates), })) return SynthesisOutput{Candidates: candidates}, nil } Part 2: Integrating with Eino Workflow Graphs \u00b6 Eino provides a graph-based approach to building agent workflows. Each node in the graph can be traced. 2.1 Basic Eino Graph Structure \u00b6 The Orchestrator uses Eino's compose package to build a workflow: import ( \"github.com/cloudwego/eino/compose\" ) type EinoOrchestrator struct { graph *compose.Graph[*OrchestrationRequest, *OrchestrationResponse] } func NewEinoOrchestrator() *EinoOrchestrator { eo := &EinoOrchestrator{} eo.graph = eo.buildWorkflowGraph() return eo } func (eo *EinoOrchestrator) buildWorkflowGraph() *compose.Graph[*OrchestrationRequest, *OrchestrationResponse] { g := compose.NewGraph[*OrchestrationRequest, *OrchestrationResponse]() // Define nodes const ( nodeValidate = \"validate\" nodeResearch = \"research\" nodeSynthesis = \"synthesis\" nodeVerification = \"verification\" nodeFormat = \"format\" ) // Add lambda nodes g.AddLambdaNode(nodeValidate, compose.InvokableLambda(eo.validateInput)) g.AddLambdaNode(nodeResearch, compose.InvokableLambda(eo.callResearch)) g.AddLambdaNode(nodeSynthesis, compose.InvokableLambda(eo.callSynthesis)) g.AddLambdaNode(nodeVerification, compose.InvokableLambda(eo.callVerification)) g.AddLambdaNode(nodeFormat, compose.InvokableLambda(eo.formatResponse)) // Define edges g.AddEdge(compose.START, nodeValidate) g.AddEdge(nodeValidate, nodeResearch) g.AddEdge(nodeResearch, nodeSynthesis) g.AddEdge(nodeSynthesis, nodeVerification) g.AddEdge(nodeVerification, nodeFormat) g.AddEdge(nodeFormat, compose.END) return g } 2.2 Adding Opik Tracing to Eino Workflows \u00b6 Trace the entire workflow and each node: type TracedEinoOrchestrator struct { *EinoOrchestrator opikClient *opik.Client } func NewTracedEinoOrchestrator() (*TracedEinoOrchestrator, error) { opikClient, err := opik.NewClient( opik.WithProjectName(\"stats-agent-team\"), ) if err != nil { return nil, err } return &TracedEinoOrchestrator{ EinoOrchestrator: NewEinoOrchestrator(), opikClient: opikClient, }, nil } func (teo *TracedEinoOrchestrator) Orchestrate(ctx context.Context, req *OrchestrationRequest) (*OrchestrationResponse, error) { // Create a trace for the entire workflow trace, err := teo.opikClient.Trace(ctx, \"eino-orchestration\", opik.WithTraceInput(map[string]any{ \"topic\": req.Topic, \"min_verified_stats\": req.MinVerifiedStats, \"max_candidates\": req.MaxCandidates, }), opik.WithTraceTags(\"framework:eino\", \"workflow:orchestration\"), ) if err != nil { return nil, err } // Store trace in context for child spans ctx = opik.ContextWithTrace(ctx, trace) // Compile and execute the graph compiled, err := teo.graph.Compile(ctx) if err != nil { trace.End(ctx, opik.WithTraceOutput(map[string]any{ \"error\": err.Error(), })) return nil, err } result, err := compiled.Invoke(ctx, req) // End trace with final results if err != nil { trace.End(ctx, opik.WithTraceOutput(map[string]any{ \"error\": err.Error(), \"success\": false, })) return nil, err } trace.End(ctx, opik.WithTraceOutput(map[string]any{ \"success\": true, \"verified_count\": result.VerifiedCount, \"total_candidates\": result.TotalCandidates, \"failed_count\": result.FailedCount, })) // Add workflow quality score qualityScore := float64(result.VerifiedCount) / float64(req.MinVerifiedStats) if qualityScore > 1.0 { qualityScore = 1.0 } trace.AddFeedbackScore(ctx, \"workflow_quality\", qualityScore, fmt.Sprintf(\"Verified %d/%d statistics\", result.VerifiedCount, req.MinVerifiedStats)) return result, nil } 2.3 Tracing Individual Eino Nodes \u00b6 Create spans for each node in the workflow: func (teo *TracedEinoOrchestrator) callResearch(ctx context.Context, req *OrchestrationRequest) (*ResearchState, error) { // Get trace from context trace := opik.TraceFromContext(ctx) // Create span for this node span, _ := trace.Span(ctx, \"eino-node:research\", opik.WithSpanType(opik.SpanTypeGeneral), opik.WithSpanInput(map[string]any{ \"topic\": req.Topic, \"max_results\": req.MaxCandidates, }), opik.WithSpanMetadata(map[string]any{ \"eino_node\": \"research\", \"agent_url\": teo.researchAgentURL, }), ) startTime := time.Now() // Call the research agent resp, err := teo.callResearchAgent(ctx, &ResearchRequest{ Topic: req.Topic, MaxStatistics: req.MaxCandidates, }) duration := time.Since(startTime) if err != nil { span.End(ctx, opik.WithSpanOutput(map[string]any{ \"error\": err.Error(), \"duration\": duration.String(), })) return nil, err } span.End(ctx, opik.WithSpanOutput(map[string]any{ \"sources_found\": len(resp.Candidates), \"duration\": duration.String(), })) return &ResearchState{ Request: req, SearchResults: convertToSearchResults(resp.Candidates), }, nil } func (teo *TracedEinoOrchestrator) callSynthesis(ctx context.Context, state *ResearchState) (*SynthesisState, error) { trace := opik.TraceFromContext(ctx) span, _ := trace.Span(ctx, \"eino-node:synthesis\", opik.WithSpanType(opik.SpanTypeLLM), // LLM-heavy operation opik.WithSpanInput(map[string]any{ \"sources_count\": len(state.SearchResults), \"topic\": state.Request.Topic, }), ) resp, err := teo.callSynthesisAgent(ctx, &SynthesisRequest{ Topic: state.Request.Topic, SearchResults: state.SearchResults, }) if err != nil { span.End(ctx, opik.WithSpanOutput(map[string]any{\"error\": err.Error()})) return nil, err } span.End(ctx, opik.WithSpanOutput(map[string]any{ \"candidates_extracted\": len(resp.Candidates), })) return &SynthesisState{ Request: state.Request, Candidates: resp.Candidates, }, nil } Part 3: Complete Integration Example \u00b6 Here's a complete example combining ADK agents with Eino orchestration and full Opik tracing: package main import ( \"context\" \"log\" opik \"github.com/agentplexus/go-comet-ml-opik\" ) func main() { // Initialize Opik client opikClient, err := opik.NewClient( opik.WithProjectName(\"stats-agent-team\"), opik.WithAPIKey(os.Getenv(\"OPIK_API_KEY\")), opik.WithWorkspace(os.Getenv(\"OPIK_WORKSPACE\")), ) if err != nil { log.Fatalf(\"Failed to create Opik client: %v\", err) } // Create the orchestrator with tracing orchestrator := NewTracedOrchestrator(opikClient) // Execute a research request ctx := context.Background() result, err := orchestrator.Research(ctx, &OrchestrationRequest{ Topic: \"climate change statistics 2024\", MinVerifiedStats: 10, MaxCandidates: 30, }) if err != nil { log.Fatalf(\"Research failed: %v\", err) } log.Printf(\"Research complete: %d verified statistics\", result.VerifiedCount) } type TracedOrchestrator struct { opikClient *opik.Client eino *EinoOrchestrator synthesis *SynthesisAgent verification *VerificationAgent } func (to *TracedOrchestrator) Research(ctx context.Context, req *OrchestrationRequest) (*OrchestrationResponse, error) { // Create main trace trace, _ := to.opikClient.Trace(ctx, \"full-research-workflow\", opik.WithTraceInput(map[string]any{ \"topic\": req.Topic, \"min_stats\": req.MinVerifiedStats, }), opik.WithTraceTags(\"workflow:research\", \"version:v1\"), ) defer trace.End(ctx) // Store in context ctx = opik.ContextWithTrace(ctx, trace) // Execute Eino workflow (which creates child spans) result, err := to.eino.Orchestrate(ctx, req) // Add evaluation scores if err == nil { // Calculate quality metrics accuracyScore := float64(result.VerifiedCount) / float64(result.TotalCandidates) coverageScore := float64(result.VerifiedCount) / float64(req.MinVerifiedStats) if coverageScore > 1.0 { coverageScore = 1.0 } trace.AddFeedbackScore(ctx, \"accuracy\", accuracyScore, \"Verification accuracy\") trace.AddFeedbackScore(ctx, \"coverage\", coverageScore, \"Target coverage\") } return result, err } Part 4: Viewing Traces in Opik Dashboard \u00b6 After running your agents with tracing enabled, you can view the traces in the Opik dashboard: Trace Hierarchy \u00b6 full-research-workflow (trace) \u251c\u2500\u2500 eino-node:validate (span) \u251c\u2500\u2500 eino-node:research (span) \u2502 \u2514\u2500\u2500 http-call:research-agent (span) \u251c\u2500\u2500 eino-node:synthesis (span) \u2502 \u251c\u2500\u2500 llm-extraction (span, type=llm) \u2502 \u251c\u2500\u2500 process-url-0 (span) \u2502 \u2502 \u2514\u2500\u2500 llm-call (span, type=llm) \u2502 \u251c\u2500\u2500 process-url-1 (span) \u2502 \u2502 \u2514\u2500\u2500 llm-call (span, type=llm) \u2502 \u2514\u2500\u2500 ... more URLs \u251c\u2500\u2500 eino-node:verification (span) \u2502 \u251c\u2500\u2500 verify-stat-0 (span, type=llm) \u2502 \u251c\u2500\u2500 verify-stat-1 (span, type=llm) \u2502 \u2514\u2500\u2500 ... more verifications \u2514\u2500\u2500 eino-node:format (span) Key Metrics to Monitor \u00b6 Latency : Total workflow time and per-node breakdown Token Usage : LLM tokens per agent and total Success Rate : Verification success rate over time Quality Scores : Accuracy and coverage metrics Best Practices \u00b6 1. Use Meaningful Span Names \u00b6 // Good: Descriptive and hierarchical span, _ := trace.Span(ctx, \"synthesis:extract-from-url\", opik.WithSpanMetadata(map[string]any{ \"url_index\": i, \"domain\": result.Domain, }), ) // Avoid: Generic names span, _ := trace.Span(ctx, \"process\") // Too vague 2. Capture Relevant Inputs/Outputs \u00b6 // Capture enough context for debugging span, _ := trace.Span(ctx, \"llm-extraction\", opik.WithSpanInput(map[string]any{ \"topic\": topic, \"content_length\": len(content), \"content_hash\": hashContent(content), // For deduplication }), ) span.End(ctx, opik.WithSpanOutput(map[string]any{ \"stats_count\": len(stats), \"stats\": stats, // Full output for analysis })) 3. Add Structured Feedback Scores \u00b6 // Quantitative metrics span.AddFeedbackScore(ctx, \"extraction_count\", float64(len(stats)), \"\") span.AddFeedbackScore(ctx, \"accuracy\", verifiedCount/totalCount, \"\") // Categorical with reason span.AddFeedbackScore(ctx, \"quality\", 0.8, \"Good extraction but missing units\") 4. Use Tags for Filtering \u00b6 trace, _ := opikClient.Trace(ctx, \"workflow\", opik.WithTraceTags( \"env:production\", \"agent:synthesis\", \"llm:gemini-2.0-flash\", \"topic:climate\", ), ) Conclusion \u00b6 Integrating Opik with Google ADK and Eino provides comprehensive observability for agentic applications: ADK Integration : Trace tool invocations and LLM calls within agents Eino Integration : Trace workflow nodes and graph execution Combined : Full visibility into multi-agent orchestration This enables you to debug issues, optimize performance, track costs, and improve agent quality over time. Next Steps \u00b6 Evaluation Metrics - Use Opik's evaluation system to automatically score agent outputs LLM Provider Integrations - Direct integration with OpenAI, Anthropic, and other providers Datasets and Experiments - Run experiments to compare agent configurations","title":"Agentic Observability"},{"location":"tutorials/agentic-observability/#agentic-observability-integrating-opik-with-google-adk-and-eino","text":"This tutorial demonstrates how to add LLM observability to agentic Go applications using the go-comet-ml-opik SDK. We'll use a real-world case study based on the stats-agent-team project, which implements a multi-agent system for statistics research and verification.","title":"Agentic Observability: Integrating Opik with Google ADK and Eino"},{"location":"tutorials/agentic-observability/#overview","text":"Modern AI applications increasingly use agentic architectures where multiple specialized agents collaborate to complete complex tasks. Observability is critical for: Debugging : Understanding why an agent made a particular decision Performance : Identifying bottlenecks in multi-agent workflows Cost tracking : Monitoring LLM token usage across agents Quality assurance : Evaluating agent outputs over time This tutorial covers integration with two popular Go agent frameworks: Google Agent Development Kit (ADK) - A framework for building LLM-powered agents with tools Eino - CloudWeGo's framework for building deterministic agent workflows as graphs","title":"Overview"},{"location":"tutorials/agentic-observability/#case-study-stats-agent-team","text":"The stats-agent-team project implements a 4-agent system for researching and verifying statistics: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Research Agent \u2502 \u2500\u2500\u25b6 \u2502 Synthesis Agent \u2502 \u2500\u2500\u25b6 \u2502Verification Agent\u2502 \u2500\u2500\u25b6 \u2502 Orchestrator \u2502 \u2502 (Web Search) \u2502 \u2502 (LLM + ADK) \u2502 \u2502 (LLM + ADK) \u2502 \u2502 (Eino Graph) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Research Agent : Finds relevant sources using web search APIs Synthesis Agent : Extracts statistics from web pages using LLM (Google ADK) Verification Agent : Verifies extracted statistics against sources (Google ADK) Orchestrator : Coordinates the workflow using Eino's graph-based approach","title":"Case Study: Stats Agent Team"},{"location":"tutorials/agentic-observability/#part-1-integrating-with-google-adk-agents","text":"Google ADK provides a structured way to build agents with tools. Here's how to add Opik observability.","title":"Part 1: Integrating with Google ADK Agents"},{"location":"tutorials/agentic-observability/#11-basic-adk-agent-structure","text":"The Synthesis Agent uses ADK's llmagent and functiontool packages: import ( \"google.golang.org/adk/agent\" \"google.golang.org/adk/agent/llmagent\" \"google.golang.org/adk/model\" \"google.golang.org/adk/tool\" \"google.golang.org/adk/tool/functiontool\" ) type SynthesisAgent struct { model model.LLM adkAgent agent.Agent } func NewSynthesisAgent(llmModel model.LLM) (*SynthesisAgent, error) { sa := &SynthesisAgent{model: llmModel} // Create a tool for statistics extraction synthesisTool, err := functiontool.New(functiontool.Config{ Name: \"synthesize_statistics\", Description: \"Extracts numerical statistics from web page content\", }, sa.synthesisToolHandler) if err != nil { return nil, err } // Create the ADK agent adkAgent, err := llmagent.New(llmagent.Config{ Name: \"statistics_synthesis_agent\", Model: llmModel, Description: \"Extracts statistics from web content\", Instruction: \"You are a statistics extraction expert...\", Tools: []tool.Tool{synthesisTool}, }) if err != nil { return nil, err } sa.adkAgent = adkAgent return sa, nil }","title":"1.1 Basic ADK Agent Structure"},{"location":"tutorials/agentic-observability/#12-adding-opik-tracing-to-adk-agents","text":"Wrap your ADK agent's LLM calls with Opik traces: import ( opik \"github.com/agentplexus/go-comet-ml-opik\" ) type TracedSynthesisAgent struct { *SynthesisAgent opikClient *opik.Client } func NewTracedSynthesisAgent(llmModel model.LLM) (*TracedSynthesisAgent, error) { // Create base agent base, err := NewSynthesisAgent(llmModel) if err != nil { return nil, err } // Create Opik client opikClient, err := opik.NewClient( opik.WithProjectName(\"stats-agent-team\"), ) if err != nil { return nil, err } return &TracedSynthesisAgent{ SynthesisAgent: base, opikClient: opikClient, }, nil } func (tsa *TracedSynthesisAgent) ExtractStatistics(ctx context.Context, topic string, content string) ([]Statistic, error) { // Create a trace for this extraction operation trace, err := tsa.opikClient.Trace(ctx, \"extract-statistics\", opik.WithTraceInput(map[string]any{ \"topic\": topic, \"content_length\": len(content), }), opik.WithTraceTags(\"agent:synthesis\", \"operation:extraction\"), ) if err != nil { return nil, err } defer trace.End(ctx) // Create a span for the LLM call span, err := trace.Span(ctx, \"llm-extraction\", opik.WithSpanType(opik.SpanTypeLLM), opik.WithSpanModel(\"gemini-2.0-flash-exp\"), opik.WithSpanProvider(\"google\"), opik.WithSpanInput(map[string]any{ \"prompt_template\": \"statistics_extraction\", \"topic\": topic, }), ) if err != nil { return nil, err } // Make the actual LLM call stats, err := tsa.doExtraction(ctx, topic, content) // End span with output and token usage span.End(ctx, opik.WithSpanOutput(map[string]any{ \"statistics_count\": len(stats), \"statistics\": stats, }), opik.WithSpanUsage(opik.Usage{ PromptTokens: estimateTokens(content), CompletionTokens: estimateTokens(formatStats(stats)), }), ) if err != nil { // Add error feedback span.AddFeedbackScore(ctx, \"error\", 0, err.Error()) return nil, err } // Add quality feedback span.AddFeedbackScore(ctx, \"extraction_quality\", float64(len(stats))/10.0, // Normalize to 0-1 fmt.Sprintf(\"Extracted %d statistics\", len(stats)), ) // Update trace output trace.End(ctx, opik.WithTraceOutput(map[string]any{ \"total_statistics\": len(stats), \"success\": true, })) return stats, nil }","title":"1.2 Adding Opik Tracing to ADK Agents"},{"location":"tutorials/agentic-observability/#13-tracing-tool-invocations","text":"When ADK agents invoke tools, trace each tool call as a separate span: func (tsa *TracedSynthesisAgent) synthesisToolHandler(ctx tool.Context, input SynthesisInput) (SynthesisOutput, error) { // Get parent trace from context (set by orchestrator) parentTrace := opik.TraceFromContext(ctx) // Create a span for this tool invocation span, err := parentTrace.Span(ctx, \"tool:synthesize_statistics\", opik.WithSpanType(opik.SpanTypeTool), opik.WithSpanInput(map[string]any{ \"topic\": input.Topic, \"url_count\": len(input.SearchResults), \"min_stats\": input.MinStatistics, \"max_stats\": input.MaxStatistics, }), ) if err != nil { return SynthesisOutput{}, err } defer span.End(ctx) // Process each URL with its own span var candidates []CandidateStatistic for i, result := range input.SearchResults { urlSpan, _ := span.Span(ctx, fmt.Sprintf(\"process-url-%d\", i), opik.WithSpanType(opik.SpanTypeGeneral), opik.WithSpanInput(map[string]any{ \"url\": result.URL, \"domain\": result.Domain, }), ) stats, err := tsa.processURL(ctx, input.Topic, result) urlSpan.End(ctx, opik.WithSpanOutput(map[string]any{ \"stats_extracted\": len(stats), \"error\": err != nil, })) if err == nil { candidates = append(candidates, stats...) } } span.End(ctx, opik.WithSpanOutput(map[string]any{ \"total_candidates\": len(candidates), })) return SynthesisOutput{Candidates: candidates}, nil }","title":"1.3 Tracing Tool Invocations"},{"location":"tutorials/agentic-observability/#part-2-integrating-with-eino-workflow-graphs","text":"Eino provides a graph-based approach to building agent workflows. Each node in the graph can be traced.","title":"Part 2: Integrating with Eino Workflow Graphs"},{"location":"tutorials/agentic-observability/#21-basic-eino-graph-structure","text":"The Orchestrator uses Eino's compose package to build a workflow: import ( \"github.com/cloudwego/eino/compose\" ) type EinoOrchestrator struct { graph *compose.Graph[*OrchestrationRequest, *OrchestrationResponse] } func NewEinoOrchestrator() *EinoOrchestrator { eo := &EinoOrchestrator{} eo.graph = eo.buildWorkflowGraph() return eo } func (eo *EinoOrchestrator) buildWorkflowGraph() *compose.Graph[*OrchestrationRequest, *OrchestrationResponse] { g := compose.NewGraph[*OrchestrationRequest, *OrchestrationResponse]() // Define nodes const ( nodeValidate = \"validate\" nodeResearch = \"research\" nodeSynthesis = \"synthesis\" nodeVerification = \"verification\" nodeFormat = \"format\" ) // Add lambda nodes g.AddLambdaNode(nodeValidate, compose.InvokableLambda(eo.validateInput)) g.AddLambdaNode(nodeResearch, compose.InvokableLambda(eo.callResearch)) g.AddLambdaNode(nodeSynthesis, compose.InvokableLambda(eo.callSynthesis)) g.AddLambdaNode(nodeVerification, compose.InvokableLambda(eo.callVerification)) g.AddLambdaNode(nodeFormat, compose.InvokableLambda(eo.formatResponse)) // Define edges g.AddEdge(compose.START, nodeValidate) g.AddEdge(nodeValidate, nodeResearch) g.AddEdge(nodeResearch, nodeSynthesis) g.AddEdge(nodeSynthesis, nodeVerification) g.AddEdge(nodeVerification, nodeFormat) g.AddEdge(nodeFormat, compose.END) return g }","title":"2.1 Basic Eino Graph Structure"},{"location":"tutorials/agentic-observability/#22-adding-opik-tracing-to-eino-workflows","text":"Trace the entire workflow and each node: type TracedEinoOrchestrator struct { *EinoOrchestrator opikClient *opik.Client } func NewTracedEinoOrchestrator() (*TracedEinoOrchestrator, error) { opikClient, err := opik.NewClient( opik.WithProjectName(\"stats-agent-team\"), ) if err != nil { return nil, err } return &TracedEinoOrchestrator{ EinoOrchestrator: NewEinoOrchestrator(), opikClient: opikClient, }, nil } func (teo *TracedEinoOrchestrator) Orchestrate(ctx context.Context, req *OrchestrationRequest) (*OrchestrationResponse, error) { // Create a trace for the entire workflow trace, err := teo.opikClient.Trace(ctx, \"eino-orchestration\", opik.WithTraceInput(map[string]any{ \"topic\": req.Topic, \"min_verified_stats\": req.MinVerifiedStats, \"max_candidates\": req.MaxCandidates, }), opik.WithTraceTags(\"framework:eino\", \"workflow:orchestration\"), ) if err != nil { return nil, err } // Store trace in context for child spans ctx = opik.ContextWithTrace(ctx, trace) // Compile and execute the graph compiled, err := teo.graph.Compile(ctx) if err != nil { trace.End(ctx, opik.WithTraceOutput(map[string]any{ \"error\": err.Error(), })) return nil, err } result, err := compiled.Invoke(ctx, req) // End trace with final results if err != nil { trace.End(ctx, opik.WithTraceOutput(map[string]any{ \"error\": err.Error(), \"success\": false, })) return nil, err } trace.End(ctx, opik.WithTraceOutput(map[string]any{ \"success\": true, \"verified_count\": result.VerifiedCount, \"total_candidates\": result.TotalCandidates, \"failed_count\": result.FailedCount, })) // Add workflow quality score qualityScore := float64(result.VerifiedCount) / float64(req.MinVerifiedStats) if qualityScore > 1.0 { qualityScore = 1.0 } trace.AddFeedbackScore(ctx, \"workflow_quality\", qualityScore, fmt.Sprintf(\"Verified %d/%d statistics\", result.VerifiedCount, req.MinVerifiedStats)) return result, nil }","title":"2.2 Adding Opik Tracing to Eino Workflows"},{"location":"tutorials/agentic-observability/#23-tracing-individual-eino-nodes","text":"Create spans for each node in the workflow: func (teo *TracedEinoOrchestrator) callResearch(ctx context.Context, req *OrchestrationRequest) (*ResearchState, error) { // Get trace from context trace := opik.TraceFromContext(ctx) // Create span for this node span, _ := trace.Span(ctx, \"eino-node:research\", opik.WithSpanType(opik.SpanTypeGeneral), opik.WithSpanInput(map[string]any{ \"topic\": req.Topic, \"max_results\": req.MaxCandidates, }), opik.WithSpanMetadata(map[string]any{ \"eino_node\": \"research\", \"agent_url\": teo.researchAgentURL, }), ) startTime := time.Now() // Call the research agent resp, err := teo.callResearchAgent(ctx, &ResearchRequest{ Topic: req.Topic, MaxStatistics: req.MaxCandidates, }) duration := time.Since(startTime) if err != nil { span.End(ctx, opik.WithSpanOutput(map[string]any{ \"error\": err.Error(), \"duration\": duration.String(), })) return nil, err } span.End(ctx, opik.WithSpanOutput(map[string]any{ \"sources_found\": len(resp.Candidates), \"duration\": duration.String(), })) return &ResearchState{ Request: req, SearchResults: convertToSearchResults(resp.Candidates), }, nil } func (teo *TracedEinoOrchestrator) callSynthesis(ctx context.Context, state *ResearchState) (*SynthesisState, error) { trace := opik.TraceFromContext(ctx) span, _ := trace.Span(ctx, \"eino-node:synthesis\", opik.WithSpanType(opik.SpanTypeLLM), // LLM-heavy operation opik.WithSpanInput(map[string]any{ \"sources_count\": len(state.SearchResults), \"topic\": state.Request.Topic, }), ) resp, err := teo.callSynthesisAgent(ctx, &SynthesisRequest{ Topic: state.Request.Topic, SearchResults: state.SearchResults, }) if err != nil { span.End(ctx, opik.WithSpanOutput(map[string]any{\"error\": err.Error()})) return nil, err } span.End(ctx, opik.WithSpanOutput(map[string]any{ \"candidates_extracted\": len(resp.Candidates), })) return &SynthesisState{ Request: state.Request, Candidates: resp.Candidates, }, nil }","title":"2.3 Tracing Individual Eino Nodes"},{"location":"tutorials/agentic-observability/#part-3-complete-integration-example","text":"Here's a complete example combining ADK agents with Eino orchestration and full Opik tracing: package main import ( \"context\" \"log\" opik \"github.com/agentplexus/go-comet-ml-opik\" ) func main() { // Initialize Opik client opikClient, err := opik.NewClient( opik.WithProjectName(\"stats-agent-team\"), opik.WithAPIKey(os.Getenv(\"OPIK_API_KEY\")), opik.WithWorkspace(os.Getenv(\"OPIK_WORKSPACE\")), ) if err != nil { log.Fatalf(\"Failed to create Opik client: %v\", err) } // Create the orchestrator with tracing orchestrator := NewTracedOrchestrator(opikClient) // Execute a research request ctx := context.Background() result, err := orchestrator.Research(ctx, &OrchestrationRequest{ Topic: \"climate change statistics 2024\", MinVerifiedStats: 10, MaxCandidates: 30, }) if err != nil { log.Fatalf(\"Research failed: %v\", err) } log.Printf(\"Research complete: %d verified statistics\", result.VerifiedCount) } type TracedOrchestrator struct { opikClient *opik.Client eino *EinoOrchestrator synthesis *SynthesisAgent verification *VerificationAgent } func (to *TracedOrchestrator) Research(ctx context.Context, req *OrchestrationRequest) (*OrchestrationResponse, error) { // Create main trace trace, _ := to.opikClient.Trace(ctx, \"full-research-workflow\", opik.WithTraceInput(map[string]any{ \"topic\": req.Topic, \"min_stats\": req.MinVerifiedStats, }), opik.WithTraceTags(\"workflow:research\", \"version:v1\"), ) defer trace.End(ctx) // Store in context ctx = opik.ContextWithTrace(ctx, trace) // Execute Eino workflow (which creates child spans) result, err := to.eino.Orchestrate(ctx, req) // Add evaluation scores if err == nil { // Calculate quality metrics accuracyScore := float64(result.VerifiedCount) / float64(result.TotalCandidates) coverageScore := float64(result.VerifiedCount) / float64(req.MinVerifiedStats) if coverageScore > 1.0 { coverageScore = 1.0 } trace.AddFeedbackScore(ctx, \"accuracy\", accuracyScore, \"Verification accuracy\") trace.AddFeedbackScore(ctx, \"coverage\", coverageScore, \"Target coverage\") } return result, err }","title":"Part 3: Complete Integration Example"},{"location":"tutorials/agentic-observability/#part-4-viewing-traces-in-opik-dashboard","text":"After running your agents with tracing enabled, you can view the traces in the Opik dashboard:","title":"Part 4: Viewing Traces in Opik Dashboard"},{"location":"tutorials/agentic-observability/#trace-hierarchy","text":"full-research-workflow (trace) \u251c\u2500\u2500 eino-node:validate (span) \u251c\u2500\u2500 eino-node:research (span) \u2502 \u2514\u2500\u2500 http-call:research-agent (span) \u251c\u2500\u2500 eino-node:synthesis (span) \u2502 \u251c\u2500\u2500 llm-extraction (span, type=llm) \u2502 \u251c\u2500\u2500 process-url-0 (span) \u2502 \u2502 \u2514\u2500\u2500 llm-call (span, type=llm) \u2502 \u251c\u2500\u2500 process-url-1 (span) \u2502 \u2502 \u2514\u2500\u2500 llm-call (span, type=llm) \u2502 \u2514\u2500\u2500 ... more URLs \u251c\u2500\u2500 eino-node:verification (span) \u2502 \u251c\u2500\u2500 verify-stat-0 (span, type=llm) \u2502 \u251c\u2500\u2500 verify-stat-1 (span, type=llm) \u2502 \u2514\u2500\u2500 ... more verifications \u2514\u2500\u2500 eino-node:format (span)","title":"Trace Hierarchy"},{"location":"tutorials/agentic-observability/#key-metrics-to-monitor","text":"Latency : Total workflow time and per-node breakdown Token Usage : LLM tokens per agent and total Success Rate : Verification success rate over time Quality Scores : Accuracy and coverage metrics","title":"Key Metrics to Monitor"},{"location":"tutorials/agentic-observability/#best-practices","text":"","title":"Best Practices"},{"location":"tutorials/agentic-observability/#1-use-meaningful-span-names","text":"// Good: Descriptive and hierarchical span, _ := trace.Span(ctx, \"synthesis:extract-from-url\", opik.WithSpanMetadata(map[string]any{ \"url_index\": i, \"domain\": result.Domain, }), ) // Avoid: Generic names span, _ := trace.Span(ctx, \"process\") // Too vague","title":"1. Use Meaningful Span Names"},{"location":"tutorials/agentic-observability/#2-capture-relevant-inputsoutputs","text":"// Capture enough context for debugging span, _ := trace.Span(ctx, \"llm-extraction\", opik.WithSpanInput(map[string]any{ \"topic\": topic, \"content_length\": len(content), \"content_hash\": hashContent(content), // For deduplication }), ) span.End(ctx, opik.WithSpanOutput(map[string]any{ \"stats_count\": len(stats), \"stats\": stats, // Full output for analysis }))","title":"2. Capture Relevant Inputs/Outputs"},{"location":"tutorials/agentic-observability/#3-add-structured-feedback-scores","text":"// Quantitative metrics span.AddFeedbackScore(ctx, \"extraction_count\", float64(len(stats)), \"\") span.AddFeedbackScore(ctx, \"accuracy\", verifiedCount/totalCount, \"\") // Categorical with reason span.AddFeedbackScore(ctx, \"quality\", 0.8, \"Good extraction but missing units\")","title":"3. Add Structured Feedback Scores"},{"location":"tutorials/agentic-observability/#4-use-tags-for-filtering","text":"trace, _ := opikClient.Trace(ctx, \"workflow\", opik.WithTraceTags( \"env:production\", \"agent:synthesis\", \"llm:gemini-2.0-flash\", \"topic:climate\", ), )","title":"4. Use Tags for Filtering"},{"location":"tutorials/agentic-observability/#conclusion","text":"Integrating Opik with Google ADK and Eino provides comprehensive observability for agentic applications: ADK Integration : Trace tool invocations and LLM calls within agents Eino Integration : Trace workflow nodes and graph execution Combined : Full visibility into multi-agent orchestration This enables you to debug issues, optimize performance, track costs, and improve agent quality over time.","title":"Conclusion"},{"location":"tutorials/agentic-observability/#next-steps","text":"Evaluation Metrics - Use Opik's evaluation system to automatically score agent outputs LLM Provider Integrations - Direct integration with OpenAI, Anthropic, and other providers Datasets and Experiments - Run experiments to compare agent configurations","title":"Next Steps"}]}